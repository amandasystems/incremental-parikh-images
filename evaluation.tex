
We evaluate the performance of \Catra{} on~\NrBenchmarks{} instances of Parikh
automata intersection problems generated by the \OstrichPlus{} string constraint
solver when solving the PyEx benchmarks~\cite{pyex}. After generating an
initial~\InitialNrBenchmarks{},we remove~\NrTrivial{} instances solved in under
five seconds by baseline. We also attempted to benchmark instances generated by
\Ostrich{} solving the kaluza-len (\numprint{38227} instances) and pyex-len
(\numprint{791} instances) benchmark suites, but had to discard them since they
were all trivial. For instance, on a laptop, \Calculus{} could solve every
instance from kaluza-len in under five seconds, mean about 0.1s. The benchmarks
are run on commit~\texttt{\commit}.

The benchmarks are executed in parallel across a cluster of \Fudge{identical}
machines, \BenchmarkRig{}, but as the sole job for each machine.  We compiled
the code using Scala~\ScalaVersion{}, and executed the experiments
on~\JvmVersion{} with a maximum heap of~\MaxHeapSize{}. We used \Nuxmv{}
version~\NuxmvVersion{} invoked as a subprocess for each instance. Instances
were executed in batches of \BatchSize{}, each given a fresh JVM. Each JVM was
warmed up for \numprint{10}s on a random benchmark from the set before starting
to execute. We believe this represents a realistic use case where \Calculus{} is
used to support e.g a string solver. Experiments were executed in random order
for all backends. Each instance got a time budget of~\RuntimeTimeout.

All runtimes are measured in wall-clock time as observed by the JVM when
executing the instance, and exclude time spent parsing (usually far below
\numprint{0.1}s).

\subsection{Execution Time and Ability to Solve Instances}\label{sec:runtime}

In \cref{fig:solve-division} we show how many of the~\NrBenchmarks{} instances
the respective back-end could solve. A summary of their outcomes by instance
type is also available in \cref{tab:solve-status}. Note that many instances lack
a corum ground truth as they are solved by only one backend. We see that
\Calculus{} generally outperforms \Nuxmv{} on determining unsatisfiability,
while being similar at satisfiable instances. Both \Nuxmv{} and \Calculus{}
outperform baseline on satisfiable instances, and \Calculus{} outperforms
\Nuxmv{}.

Baseline performs worse on satisfiable instances because it executes a heuristic
meant to detect unsatisfiability early at a heavy penalty to satisfiable
instances roughly similar to \cite{approximate-parikh}. The heuristic is enabled
since we suspect that unsatisifiable instances are more common; at least we have
seen more solved unsatisfiable instances.

\begin{table}
  \centering
  \input{graphs/solved_pivot_table.tex}
  \caption{Number of successful results within a timeout of \RuntimeTimeout{}.
  Instances solved by no backend within the timeout (about half of the set) are
  omitted from the table.}\label{tab:solve-status}
\end{table}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-by-solver.pdf}
    \caption{The division of statuses per backend.}
    \label{fig:solve-division}
    \Description[A bar chart showing three bars, one per backend, illustrating how many instances they could solve]{The bars are divided by satisfiable and unsatisfiable instances. Baseline could seemingly only solve unsatisfiable instances, lazy could solve a few satisfiable and mostly unsatisfiable, and nuxmv could solve about twice as many unsatisfiable as satisfiable instances, and slightly less in total than lazy.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-time-boxplot.pdf}
    \caption{The distribution of runtimes for solved instances per backend. Note that the number of instances solved differs between backends.}
    \label{fig:runtime-boxplot}
    \Description[A box plot showing the distribution of runtime over the three backends]{The middle box plot shows a tiny box centered around 0 seconds for the lazy backend, the rightmost box shows a bigger box between five seconds and 30 seconds for nuxmv, and the leftmost box shows a smaller box between 20 and 30 seconds for baseline. The whiskers of nuxmv span between 0 and the timeout, 60 seconds, while they are much tighter for lazy. On the other hand, lazy has  a lot of outliers.}
  \end{subfigure}
  \vfill
  \begin{subfigure}[b]{0.75\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-cactus.pdf}
    \caption{The number of instances solved as the time budget increases, simulated from one \RuntimeTimeout-timeout run.}
    \label{fig:cactus}
    \Description[A cactus plot comparing the performance of nuxmv, lazy and baseline]{The line for baseline is strictly lower than the two others, and does not head upwards from zero until after 10 seconds of timeout, while the number of instances for nuxmv increases sharply until ca 10 seconds and then linearly after that. Lazy solves much more instances than nuxmv.}
  \end{subfigure}
\end{figure}

\subsubsection{Scalability}\label{sec:scaling}%
To get an idea of how the different backends scale, we additionally execute
\numprint{2000} different randomly sampled benchmarks with a
\numprint{120}-second timeout. In this set we also include two additional
variants of \Calculus{}: one with clause learning and restarts disabled, and one
with only clause learning disabled to show the impact of
\cref{sec:random-restarts}, and \cref{sec:clause-learning} respectively.

A cactus plot showing the number of instances solved within a given timeout for
each backend configuration can be seen in \cref{fig:cactus}. Here, we see
clearly that \Calculus{} outperforms \Nuxmv{} in general, but that \Nuxmv{}
might have better long-term scaling properties. Notably, for machines with
better single-core performance \Nuxmv{}'s scaling benefits kicks in faster,
though the graph looks similar in shape.

\iffalse
\subsection{Finding a Presburger Formula}\label{sec:evaluation:finding-image}

For baseline and \Calculus{}, \Catra{} offers the ability to find the equivalent Presburger formula representing a given instance. For baseline, we use the built-in quantifier elimination facilities of the underlying \Princess{} theorem prover, while for \Catra{} we use the specially tailored approach described in \cref{sec:finding-the-image}. For this experiment, we use only the~\NrKnownSat{} instances known to be satisfiable from the previous experiment detailed in \cref{sec:scaling,sec:runtime}. 

To make sure baseline puts up as much competition as possible, we disable
checking intermittent satisfiability and configure \Catra{} to run in the
maximally eager mode where the product is first computed before any
satisifiability check is performed. We run the experiments with a timeout
of~\ImageTimeout{}. The results of the experiment is summarised in
\cref{fig:cactus:image} and \cref{tab:image-results}. \Fudge{We see here that
something happens}.

\begin{figure}[ht]
  \caption{The number of instances \Catra{} was able to find the Presburger form of the image for within a given number of seconds per backend.}
  \label{fig:cactus:image}
\end{figure}
\fi

\subsection{Threats to Validity}

The most obvious threats to validity would be poor benchmarking or poor
implementation, e.g. if the method described in \cref{sec:calculus,sec:multiple}
deviate from what is actually benchmarked in section \cref{sec:experiments}, or
if the methods used for benchmarking would be unsound. To increase the
probability that our results are representative both in the sense of
representing  on expected inputs and in the statistical sense for a given run of
\Catra{} despite the use of randomness in our implementation we execute many
experiments. Moreover, to address the issue of correctness we have validated all
reported solutions made by \Calculus{} with \Nuxmv{} to ensure that \Calculus{}
is indeed sound. A previous version contained a race condition triggered by
random restarts during product materialisation which would in about
\numprint{0.7} of instances lead to non-deterministically unsound behaviour. We
have observed no instances of this bug or any other soundness issue since fixing
that bug, and have in addition to running all \NrBenchmarks{} we have
re-executed the same instances multiple times for long durations to capture
other soundness issues without any issue.

Another threat to validity would be if the competition (baseline and \Nuxmv)
would be disadvantaged in our comparison. For \Nuxmv{} we use the default
configuration which we believe should be performant (or it should not be the
default). Additionally, tweaking our invocation of \Nuxmv{} is explicitly made
easy for artefact reviewers. For baseline, our best argument is that the
solution we use is close to the one found in the \Ostrich{} string solver and
that it therefore should be realistic.

The greatest threat to validity remaining is our choice of implementation
platform and automata library. We There are some signs that product computation
is inefficient, notably the good performance of low-threshold automata
materialisation that prioritises computing smaller products. This means that
\Calculus{} makes less heavy use of product computation than baseline does,
instead relying more on \Princess{}. This situation would unfairly advantage
\Calculus{} if our automata library was the bottleneck due to our automata
library being unnaturally poor. We believe this is unlikely since similar
performance issues have been reported for string solvers. Additionally,
profiling suggests that both \Princess{}-based backends spend most of their time
in \Princess{}, suggesting that the automaton implementation is not the
bottleneck. Finally, we have compared aggregate results for both native backends
when optimising the automata construction procedures and observed that the trend
in performance differences has been robust as the automata library improved
vastly.