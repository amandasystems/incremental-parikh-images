
We evaluate the performance of \Catra{} on~\NrBenchmarks{} instances of Parikh
automata intersection problems generated by the \OstrichPlus{} string constraint
solver when solving the PyEx benchmarks~\cite{pyex}. After generating an
initial~\InitialNrBenchmarks{},we remove~\NrTrivial{} instances solved in under
five seconds by baseline. We also attempted to benchmark instances generated by
\Ostrich{} solving the kaluza-len (\numprint{38227} instances) and pyex-len
(\numprint{791} instances) benchmark suites, but had to discard them since they
were all trivial. For instance, on a laptop, \Calculus{} could solve every
instance from kaluza-len in under five seconds, mean about 0.1s. The benchmarks
are run on commit~\texttt{\commit}.

The benchmarks are executed in parallel across a cluster of \Fudge{identical}
machines, \BenchmarkRig{}, but as the sole job for each machine.  We compiled
the code using Scala~\ScalaVersion{}, and executed the experiments
on~\JvmVersion{} with a maximum heap of~\MaxHeapSize{}. We used \Nuxmv{}
version~\NuxmvVersion{} invoked as a subprocess for each instance. Instances
were executed in batches of \BatchSize{}, each given a fresh JVM. Each JVM was
warmed up for \numprint{10}s on a random benchmark from the set before starting
to execute. We believe this represents a realistic use case where \Calculus{} is
used to support e.g a string solver. Experiments were executed in random order
for all backends. Each instance got a time budget of~\RuntimeTimeout.

All runtimes are measured in wall-clock time as observed by the JVM when
executing the instance, and exclude time spent parsing (usually far below
\numprint{0.1}s).

\subsection{Execution Time and Ability to Solve Instances}\label{sec:runtime}

In \cref{fig:solve-division} we show how many of the~\NrBenchmarks{} instances
the respective back-end could solve. A summary of their outcomes by instance
type is also available in \cref{tab:solve-status}. Note that many instances lack
a corum ground truth as they are solved by only one backend. We see that
\Calculus{} generally outperforms \Nuxmv{} on determining unsatisfiability, as
does baseline while being similar at satisfiable instances. Both \Calculus{} and
baseline outperforms baseline on satisfiable instances. On satisfiable
instances, \Nuxmv and \Calculus{} has similar performance.

Baseline performs worse on satisfiable instances because it executes a heuristic
meant to detect unsatisfiability early at a heavy penalty to satisfiable
instances roughly similar to \cite{approximate-parikh}. The heuristic is enabled
since the improvement is significant compared to the extra cost and since we
have observed more unsatisfiable instances than satisfiable ones.

\begin{table}
  \centering
  \input{graphs/solved_pivot_table.tex}
  \caption{Number of successful results within a timeout of \RuntimeTimeout{}.
  Instances solved by no backend within the timeout (about half of the set) are
  omitted from the table.}\label{tab:solve-status}
\end{table}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-by-solver.pdf}
    \caption{The division of statuses per backend.}
    \label{fig:solve-division}
    \Description[A bar chart showing three bars, one per backend, illustrating how many instances they could solve]{The bars are divided by satisfiable and unsatisfiable instances. Baseline could seemingly only solve unsatisfiable instances, lazy could solve a few satisfiable and mostly unsatisfiable, and nuxmv could solve about twice as many unsatisfiable as satisfiable instances, and slightly less in total than lazy.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-time-boxplot.pdf}
    \caption{The distribution of runtimes for solved instances per backend. Note that the number of instances solved differs between backends.}
    \label{fig:runtime-boxplot}
    \Description[A box plot showing the distribution of runtime over the three backends]{The middle box plot shows a tiny box centered around 0 seconds for the lazy backend, the rightmost box shows a bigger box between five seconds and 30 seconds for nuxmv, and the leftmost box shows a smaller box between 20 and 30 seconds for baseline. The whiskers of nuxmv span between 0 and the timeout, 60 seconds, while they are much tighter for lazy. On the other hand, lazy has  a lot of outliers.}
  \end{subfigure}
  \vfill
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-cactus.pdf}
    \caption{The number of instances solved as the time budget increases, simulated from one \numprint{120}s-timeout run.}
    \label{fig:cactus}
    \Description[A cactus plot comparing the performance of nuxmv, lazy and baseline]{The line for baseline is strictly lower than the two others, and does not head upwards from zero until after 10 seconds of timeout, while the number of instances for nuxmv increases sharply until ca 10 seconds and then linearly after that. Lazy solves much more instances than nuxmv.}
  \end{subfigure}
\end{figure}

\subsubsection{Scalability}\label{sec:scaling}%
To get an idea of how the different backends scale, we additionally execute
\numprint{2000} randomly sampled (without replacement) benchmarks with a
\numprint{120}-second timeout. In addition to the previous back-ends, we also
execute \Calculus{} with clause learning and restarts disabled, and with only
clause learning disabled to show the impact of \cref{sec:random-restarts}, and
\cref{sec:clause-learning} respectively.

A cactus plot showing the number of instances solved within a given timeout can
be seen in \cref{fig:cactus}. We see that \Calculus{} outperforms \Nuxmv{} in
general but that \Nuxmv{} might scale better on very long runtimes.
Non-systematic observations suggest that \Nuxmv{}'s scaling benefits kicks in
faster on newer machines, so it is likely that the amount of CPU cycles spent is
the underlying factor. This is likely due to two factors. First, \Nuxmv{} is
more mature than \Calculus{}, and it makes sense that its more general model
checking methods would pay off on more difficult instances where more
problem-specific methods offer less payoff. Second, for longer running
calculations clause learning as described in \cref{sec:clause-learning} can be
assumed to matter more. However, the clauses we learn are still unpolished, and
in particular, generalise poorly across product computations. Improvements are
left as future work.

\subsection{Evaluation in \Ostrich{}}\label{}%
\Catra{} was experimentally integrated as the Parikh automata product solver for
an in-progress version of \Ostrich{} (which shares \Princess{} as the backing
theorem prover) with cost-enriched automata (CEFA). \Ostrich{} (\textit{sans}
CEFA) recently participated in the 2023~SMT-COMP, winning the single-query track
for quantifier-free strings (\texttt{QF\_S}) as well as the entire string
solving track on unsatisfiable instances~\cite{smt-comp-23}.

The \texttt{QF\_S} track contains no Parikh automata intersection problems,
however. For that, we need to look to the linear integer arithmetic
(\texttt{QF\_SLIA}), where \Ostrich{} did worse in SMT-COMP. The evaluations in
\cref{sec:scaling} use inputs generated by an older branch of CEFA-enriched
\Ostrich{} on the PyEx benchmarks, which are part of the set used in the
competition. In this section, we evaluated a version of \Ostrich{} using
\Catra{} as its back-end \Fudge{on StarExec, the same cluster SMT-COMP ran on}
on \numprint{1000}~randomly selected instances from the PyEx benchmarks. The
results are shown in comparison \Fudge{to the other provers of the 2023
competition on the same benchmarks, as well as \Ostrich{} using the baseline
back-end described in \cref{sec:implementing-baseline}}.

We include this section to show that \Calculus{} can be used to enhance the
performance of an automaton-based string solver with cost-enriched automata like
\Ostrich{}. However, significant engineering effort remains to bring the
integration to a state where it can participate in the full SMT-COMP.

\Fudge{RESULTS HERE PLEASE!}

\subsection{Threats to Validity}

The most obvious threats to validity would be poor benchmarking or poor
implementation, e.g. if the method described in \cref{sec:calculus,sec:multiple}
deviate from what is actually benchmarked in section \cref{sec:experiments}, or
if the methods used for benchmarking would be unsound. To increase the
probability that our results are representative both in the sense of
representing  on expected inputs and in the statistical sense for a given run of
\Catra{} despite the use of randomness in our implementation we execute many
experiments. Moreover, to address the issue of correctness we have validated all
reported solutions made by \Calculus{} with \Nuxmv{} to ensure that \Calculus{}
is indeed sound. A previous version contained a race condition triggered by
random restarts during product materialisation which would in about
\numprint{0.7} of instances lead to non-deterministically unsound behaviour. We
have observed no instances of this bug or any other soundness issue since fixing
that bug, and have in addition to running all \NrBenchmarks{} we have
re-executed the same instances multiple times for long durations to capture
other soundness issues without any issue.

Another threat to validity would be if the competition (baseline and \Nuxmv)
would be disadvantaged in our comparison. For \Nuxmv{} we use the default
configuration which we believe should be performant (or it should not be the
default). Additionally, tweaking our invocation of \Nuxmv{} is explicitly made
easy for artefact reviewers. For baseline, our best argument is that the
solution we use is close to the one found in the \Ostrich{} string solver and
that it therefore should be realistic.

The greatest threat to validity remaining is our choice of implementation
platform and automata library. We There are some signs that product computation
is inefficient, notably the good performance of low-threshold automata
materialisation that prioritises computing smaller products. This means that
\Calculus{} makes less heavy use of product computation than baseline does,
instead relying more on \Princess{}. This situation would unfairly advantage
\Calculus{} if our automata library was the bottleneck due to our automata
library being unnaturally poor. We believe this is unlikely since similar
performance issues have been reported for string solvers. Additionally,
profiling suggests that both \Princess{}-based backends spend most of their time
in \Princess{}, suggesting that the automaton implementation is not the
bottleneck. Finally, we have compared aggregate results for both native backends
when optimising the automata construction procedures and observed that the trend
in performance differences has been robust as the automata library improved
vastly.