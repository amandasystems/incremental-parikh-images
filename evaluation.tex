
We evaluate the performance of \Catra{} on~\NrBenchmarks{} instances of Parikh
automata intersection problems generated by the \OstrichPlus{} string constraint
solver~\cite{ostrich-plus} when solving the PyEx benchmarks, 
which are string constraints from symbolic execution that are known
to be hard for many solvers~\cite{pyex}. After generating an
initial~\InitialNrBenchmarks{}, we remove~\NrTrivial{} instances solved in under
five seconds by baseline as well as~\NrInvalid{} instances that were duplicates of other instances in the set.

 We also attempted to benchmark instances generated by
\OstrichPlus{} solving the Kaluza benchmarks~\cite{Saxena10:kaluza}
(\numprint{38227}~instances),
\iffalse
and pyex-len
(\numprint{791}~instances) benchmark suites\fi
but discarded them since they all turned out to be trivial for our tool
\Catra{}: every instance
could be solved in under five seconds, with a mean runtime of about 0.1s.
The benchmarks are run
on commit~\texttt{\commit} of \Catra{}\footnote{Github repository redacted for anonymity.}.

The benchmarks are executed in parallel across a cluster of identical
machines, \BenchmarkRig{}, but as the sole job for each machine.  We compiled
the code using Scala~\ScalaVersion{} and executed the experiments
on~\JvmVersion{} with a maximum heap of~\MaxHeapSize{}. We used \Nuxmv{}
version~\NuxmvVersion{} invoked as a subprocess for each instance. Instances
were executed in batches of \BatchSize{}, each given a fresh JVM. Each JVM was
warmed up for \numprint{10}s on a random benchmark from the set before starting
to execute. We believe this represents a realistic use case where \Calculus{} is
used to support, e.g., a string solver. Experiments were executed in random order
for all backends. Each instance got a time budget of~\RuntimeTimeout.

All runtimes are measured in wall-clock time as observed by the JVM when
executing the instance, and exclude time spent parsing (usually far below
\numprint{0.1}s).

\subsection{Execution Time and Ability to Solve Instances}\label{sec:runtime}

In \cref{fig:solve-division}, we show how many of the~\NrBenchmarks{} instances
the respective back-end could solve. A summary of their outcomes by instance
type is also available in \cref{tab:solve-status}. Note that many instances lack
a corum ground truth as they are solved by only one backend. We see that
\Calculus{} generally outperforms \Nuxmv{} on determining unsatisfiability, as
does baseline, while being similar at satisfiable instances. Both \Calculus{} and
\Nuxmv{} outperforms baseline on satisfiable instances. On satisfiable
instances, \Nuxmv{} and \Calculus{} have similar performance.

Baseline performs worse on satisfiable instances because it executes a heuristic
meant to detect unsatisfiability early at a heavy penalty to satisfiable
instances roughly similar to \cite{approximate-parikh}. The heuristic is enabled
since the improvement is significant compared to the extra cost and since we
have observed more unsatisfiable instances than satisfiable ones.

\begin{table}
  \begin{center}
  \input{graphs/solved_pivot_table.tex}
  \end{center}
  \caption{Number of successful results within a timeout of \RuntimeTimeout{}.
  Instances solved by no backend within the timeout (about half of the set) are
  omitted from the table.}\label{tab:solve-status}
\end{table}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-by-solver.pdf}
    \caption{The division of statuses per backend.}
    \label{fig:solve-division}
    \Description[A bar chart showing three bars, one per backend, illustrating how many instances they could solve]{The bars are divided by satisfiable and unsatisfiable instances. Baseline could seemingly only solve unsatisfiable instances, PC* could solve a few satisfiable and mostly unsatisfiable, and nuxmv could solve about twice as many unsatisfiable as satisfiable instances, and slightly less in total than PC*.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-time-boxplot.pdf}
    \caption{The distribution of runtimes for solved instances per backend. Note that the number of instances solved differs between backends.}
    \label{fig:runtime-boxplot}
    \Description[A box plot showing the distribution of runtime over the three backends]{The middle box plot shows a tiny box centered around 0 seconds for the PC* backend, the rightmost box shows a bigger box between five seconds and 30 seconds for nuxmv, and the leftmost box shows a smaller box between 20 and 30 seconds for baseline. The whiskers of nuxmv span between 0 and the timeout, 60 seconds, while they are much tighter for PC*. On the other hand, PC* has a lot of outliers.}
  \end{subfigure}
  \vfill
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-cactus.pdf}
    \caption{The number of instances solved as the time budget increases, simulated from one \numprint{120}s-timeout run.}
    \label{fig:cactus}
    \Description[A cactus plot comparing the performance of nuxmv, PC* and baseline]{The line for baseline is strictly lower than the two others, and does not head upwards from zero until after 10 seconds of timeout, while the number of instances for nuxmv increases sharply until ca 10 seconds and then linearly after that. Lazy solves much more instances than nuxmv.}
  \end{subfigure}
\end{figure}

\subsubsection{Scalability}\label{sec:scaling}%
To get an idea of how the different backends scale, we additionally execute
\numprint{2000} randomly sampled (without replacement) benchmarks with a
\numprint{120}-second timeout. In addition to the previous back-ends, we also
execute \Calculus{} with clause learning and restarts disabled, and with only
clause learning disabled to show the impact of \cref{sec:random-restarts}, and
\cref{sec:clause-learning} respectively.

A cactus plot showing the number of instances solved within a given timeout can
be seen in \cref{fig:cactus}. We see that \Calculus{} outperforms \Nuxmv{} in
general but that \Nuxmv{} might scale better on very long runtimes.
Non-systematic observations suggest that \Nuxmv{}'s scaling benefits kicks in
faster on newer machines, so it is likely that the amount of CPU cycles spent is
the underlying factor. This is likely due to two factors. First, \Nuxmv{} is
more mature than \Calculus{}, and it makes sense that its more general model
checking methods would pay off on more difficult instances where more
problem-specific methods offer less payoff. Second, for longer running
calculations clause learning as described in \cref{sec:clause-learning} can be
assumed to matter more. However, the clauses we learn are still unpolished, and
in particular, generalise poorly across product computations. Improvements are
left as future work.

\subsection{Evaluation in \Ostrich{}}\label{}%

\begin{table}
  \begin{center}
  \input{experiments/qf_slia_comparison.tex}
  \end{center}
  \caption{Number of solved benchmarks from the single-query QF\_SLIA
    track of quantifier-free string linear integer arithmetic at
    SMT-COMP 2023. The numbers for each solver are from the publicly
    available data files, except for CA-Str, which is executed by us
    (600s timeout, AMD Opteron 2220
    SE), and \Ostrich+CA, which is a virtual portfolio. }
  \label{tab:solve-status-smt-comp}
\end{table}

To evaluate the effectiveness of \Calculus{} in an actual string solver,
\Catra{} was experimentally integrated as the Parikh automata product solver
into the string solver \Ostrich{}~version~1.3. \Ostrich{} is an independently
developed solver that participated in the recent SMT-COMP~2023\footnote{\url{https://smt-comp.github.io/2023/participants/ostrich}},
 winning the single-query track
for quantifier-free strings (\texttt{QF\_S}), as well as dominating
other solvers on the complete set of
 unsatisfiable string benchmarks~\cite{smt-comp-23}.

 At SMT-COMP, \Ostrich{} competed using a portfolio of three different
 back-ends:
\begin{itemize}
\item \textbf{BW-Str:} a backward-propagation-based solver, not utilising
  Parikh automata~\cite{ostrich}.
\item \textbf{ADT-Str:} a solver based on algebraic data-types.
\item \textbf{CE-Str:} a re-implementation of the \OstrichPlus{}
  algorithm~\cite{ostrich-plus}, using Parikh automata and the
  encoding from~\cite{generate-parikh-image}.
\end{itemize}

For our experiments, we modified the CE-Str solver to apply \Catra{}
instead of the previous baseline method, resulting in a new back-end
\textbf{CA-Str}. We created also a new virtual portfolio~\Ostrich{}+CA
by combining the three solvers in \Ostrich{}~1.3 with \textbf{CA-Str}.

%As benchmarks, we again focused on the PyEx benchmarks~\cite{pyex}, of
%which we picked 1000~problems uniformly at random. We ran
%different combinations of the
%back-ends on those 1000~problems with a 120s timeout.
%
%The \texttt{QF\_S} track contains no Parikh automata intersection problems,
%however. For that, we need to look to the linear integer arithmetic
%(\texttt{QF\_SLIA}), where \Ostrich{} did worse in SMT-COMP. The evaluations in
%\cref{sec:scaling} use inputs generated by an older branch of CEFA-enriched
%\Ostrich{} on the PyEx benchmarks, which are part of the set used in the
%competition. In this section, we evaluated a version of \Ostrich{} using
%\Catra{} as its back-end \Fudge{on StarExec, the same cluster SMT-COMP ran on}
%on \numprint{1000}~randomly selected instances from the PyEx benchmarks. 
%
% comparison \Fudge{to the other provers of the 2023
%competition on the same benchmarks, as well as \Ostrich{} using the baseline
%back-end described in \cref{sec:implementing-baseline}}.

We evaluate the different solvers in \cref{tab:solve-status-smt-comp}
on 21\,938~benchmarks from the single-query QF\_SLIA track at SMT-COMP 2023, as
QF\_SLIA was the most challenging track for \Ostrich{} in the
competition.
%
As can be seen from the table, integrating \Catra{} as a back-end
leads to gains both on satisfiable and unsatisfiable problems. On
satisfiable benchmarks, the combination \Ostrich+CA is still
outperformed by cvc5 and z3alpha. On unsatisfiable benchmarks,
\Ostrich+CA narrowly beats the other solvers, which is promising given
that also \Ostrich{}~1.3, cvc5, and z3alpha show very strong
performance on this class of benchmarks.

The results show that \Calculus{} can be used to enhance the
performance of an automaton-based string solver.
The results should be considered as preliminary, however, 
as we believe that a deeper integration of our \Catra{} solver into
string solvers can lead to significant performance gains.

\subsection{Threats to Validity}

The most obvious threats to validity would be an unsound implementation. To
address this we have validated all reported solutions made by \Calculus{} with
\Nuxmv{}. A previous version contained a race with random restarts during
product materialisation causing non-deterministic unsoundness in \numprint{0.7}
of instances. Since addressing that bug we have observed no other soundness
issues, despite re-executing the same instances multiple times for long
durations.

The second threat to validity is our implementation of automata operations. As
\Calculus{} by design offloads some of the product computation work onto
\Princess{}, the baseline could be unfairly disadvantaged by a slow automata
product implementation. We believe this is not an issue since similar
performance issues with the baseline approach have been reported for string
solvers. Additionally, profiling shows that baseline spends most of its time in
\Princess{}, suggesting that the automaton implementation is not the bottleneck.
Finally, the difference in performance between \Calculus{} and baseline has been
robust under significant optimisation of the automata library.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
