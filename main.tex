%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[acmsmall,review,anonymous,screen]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=true}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[acmsmall]{acmart}\settopmatter{}


%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{POPL} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2023}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format must update the '\documentclass' and
%% topmatter commands above; see 'acmart-sigplanproc-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption



\usepackage{amsmath,empheq,fancybox}
\usepackage{paralist}
\usepackage{url}
\usepackage{color}
\usepackage{textcomp,listings}
\usepackage{array}
\usepackage{mymacros}
\usepackage{microtype}
\usepackage{listings}
\usepackage{csquotes}
\usepackage{proof}
\usepackage{cleveref}
\usepackage{algorithm2e}      
\usepackage{multirow}
\usepackage{mathpartir}
\usepackage{amsthm}
\usepackage{ebproof}

\usepackage{mathtools} % Bonus


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\lstset{
    columns=fullflexible,
    showspaces=false,
    showtabs=false,
    breaklines=true,
    showstringspaces=false,
    breakatwhitespace=true,
    escapeinside={(*@}{@*)},
    commentstyle=\color{greencomments},
    keywordstyle=\color{bluekeywords},
    stringstyle=\color{redstrings},
    numberstyle=\color{graynumbers},
    basicstyle=\ttfamily\small,
    framesep=12pt,
    xleftmargin=12pt,
    tabsize=4,
    captionpos=b
}


\newif\ifcomments
\commentstrue
\newif\ifoutline
\outlinetrue

\newcommand{\contents}[1]{\ifoutline{\color{blue}
    \begin{itemize}
    #1
    \end{itemize}
  }\fi}

\allowdisplaybreaks[1]


\begin{document}

%% Title information
\title{An Efficient Calculus for Generalised Parikh Images of Regular Languages}
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}                   %% \country is recommended
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
  A common problem in string constraint solvers is computing the Parikh image, a
   set of linear equations that describe all possible combinations of character
   counts in strings of a given language. Automata-based string solvers
   frequently need to compute the Parikh image of large products (intersections)
   of automata, which in many operations is both prohibitively slow and
   memory-intensive. We contribute a novel understanding of how Parikh maps can
   be combined with arbitrary commutative monoid morphisms to represent
   real-world constraints on regular languages, most notably the length
   constraint. Furthermore, we show how this formulation can be efficiently
   implemented as a calculus, \Calculus{}, in an automated theorem prover
   supporting Presburger logic. We also provide an efficient method for deriving
   the Presburger formula of a given Parikh image, \Fudge{which is good for
   something} based on techniques for quantifier elimination. We implement
   \Calculus{} in a tool called \Catra{}, and evaluate it on constraints
   produced by the \Ostrich{} string constraint solver when solving
   \Fudge{standard string constraint benchmarks involving string lengths}. We
   show that our solution strictly outperforms the standard eager approach
   described in~\citeauthor{generate-parikh-image} as well as the
   over-approximating method recently described
   by~\citeauthor{approximate-parikh}, and in many cases also the~\Nuxmv{} model
   checker.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{parikh images, string solvers, model checking}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}

The Parikh map is a characterisation of a context-free language as a vector of
its character counts, famously a semilinear set~\cite{parikh-theorem}. It
appears naturally as part of \Fudge{many operations} in model checking and
solving string constraints in automata-based solvers such as \Ostrich{}
\cite{ostrich}, notably in representing constraints on string lengths.
Specialised string solvers like e.g. Trau \cite{trau} also perform extensive
reasoning using Parikh images, but do so on restricted automata that do not
suffer from the blow-up we seek to address, though it might be possible to
adapts aspects of our approach to theirs.

It is possible to compute the Parikh image of any context-free language using
the method described in~\cite{generate-parikh-image}. Later improvements have
produced a construction taking at most linear time to
produce~\cite{muscholl-linear}. However, the resulting existentially quantified
clauses are costly to eliminate as the number of variables increases, in
practice making many real-world problems intractable.

In addition to the elimination of existentially quantified variables, another
problem arises from length constraints in string solvers. Conjunctions of string
constraints lead to the computation of length images of products (intersections)
of regular languages represented as finite automata. Applying the approach
in~\cite{generate-parikh-image} would require the eager computation of the
product before its length image. In several instances we have observed while
solving real-world string constraints, the computation of the product of
automata exhausts the memory of any machine available due to the exponential
blow-up in size of the product, quickly becoming intractable as the number of
automata in the product increases. The current best published mitigation for
this problem is an over-approximation that works by approximating the Parikh
image of a product of automata to be the conjunction of the image of the
individual automata of the product \cite{approximate-parikh}. This approach only
works for unsatisfiable instances, and comes with a harsh penalty on satisfiable
instances.

Addressing these concerns, we have developed a calculus for Parikh images of
products of regular languages that we call \Calculus{}. It allows us to
interleave the computation of arbitrarily deep products of automata with the
product's Parikh image, and is generalised to an arbitrary Parikh-like
homomorphism over automata labels, including string lengths. This enables us to
let both calculations inform each other, eliminating unnecessary work, and
pruning the size of the partial products considered in the computation for a
smaller memory footprint. Moreover, the method can be used iteratively to tackle
smaller chunks of the product incrementally, thereby avoiding running out of
memory at all.

The problem of computing constraints on Parikh images over products of regular
languages under a given commutative homomorphism amounts to solving products of
Parikh automata. Parikh automata are regular automata extended with integer
counters with given increments and decrements for each transition, where we
allow checking a set of linear constraints on the final values of the counters
(but not their intermittent values) \cite{parikh-automata}. Parikh automata
without constraints on the final values on their registers are also sometimes
called cost-enriched automata, weighted automata or counter automata, depending
on exact definitions and side-constraints. The decision problem tackled in this
paper, determining the emptiness of an intersection of Parikh automata, was
recently shown to be PSPACE-complete~\cite{graph-queries}.

In addition to the decision problem of determining whether an intersection of
Parikh automata is empty and the satisfaction problem of finding a value from
the intersection if it is not, \Catra{} is able to efficiently generate a
quantifier-free Presburger formula representing an intersection of Parikh
automata, something beyond the reach of general model checkers like~\Nuxmv.
\Fudge{This is useful because the moon is made of cheese.}

We implement \Calculus{} as a theory for the \Princess{} automated theorem
prover in a tool that we call \Catra{}. \Catra also supports using the
approximate method of \cite{approximate-parikh}, its fall-back variant adapted
from~\cite{generate-parikh-image}, and an adapter for the \Nuxmv{} model
checker~\cite{nuxmv}. Using \Catra{}, we compare \Calculus{} to the other two
back-ends on \NrBenchmarks{} Parikh automata satisfaction problems generated
\Fudge{by Ostrich+}.

In summary, we contribute:
\begin{itemize}
\item The \Calculus{} calculus to efficiently compute (a homomorphism on) the
Parikh image of products of Parikh automata.
\item A specialised method for quantifier elimination that blends well with \Calculus{} to efficiently compute quantifier-free Presburger representations of a given Parikh image. \Fudge{Why is this good?}
\item Experiments illustrating the performance of \Calculus{} on real-world examples from string solving, including \NrBenchmarks{} instances in a standardised format made available for future study.
\item The \Catra{} tool for solving such instances, containing an implementation of \Calculus{}, the over-approximation described in~\cite{approximate-parikh}, and an adapter for the~\Nuxmv{} model checker~\cite{nuxmv}.
\item Suggestions for how to efficiently implement \Calculus{} in a modern automated theorem prover, including strategies for case splitting, clause learning, and constraint propagation for connectedness.
\end{itemize}

\section{A demonstration of our approach}\label{sec:motivation}

To illustrate the key features of \Calculus, we start with two regular
languages, a commutative homomorphism $f$ on them that we care about, and a side
constraint for which we want to find values in $f$'s image over the intersection
of the two languages using a heavily elided version of \Calculus. We will use
the same two regular languages, $\mathtt{ac^*a|b(bb)^*c}$ and $\mathtt{.^*c.^*}$
for both examples but change the constraint $\varphi$ and homomorphism $f$. In
essence, we solve the following set of constraints for all possible values of
$s$ and $\MonoidElement$:

\begin{equation*}
  \AndComp{}{\begin{aligned}
  & \mathtt{ac^*a|b(bb)^*c} \text{ accepts } s\\
  & \mathtt{.^*c.^*} \text{ accepts } s \\
  & \MonoidElement = f(s) \\
  & \varphi(\MonoidElement)
  \end{aligned}}
 \end{equation*}

We will represent the regular languages as the automata $\SomethingCSomething{}$
 (\cref{fig:something-c-something}) and $\AcaOrBc{}$ (\cref{fig:aca-or-bc})
 respectively.

\subsection{Counting string lengths}

To keep things simple, in this first example we
let $f$ be the function that gives the length of a given string, e.g.
$f\left(\text{curious and curiouser still}\right) = 27$, and $f\left(\text{homo}\right) +
f\left(\text{morphism}\right) = f\left(\text{homomorphism}\right) = 12$.

\begin{figure}[ht]
  \begin{minipage}[b]{0.75\linewidth}
  \centering 
    \includegraphics[width=\textwidth]{aca_or_bc}
    \caption{An automaton recognising the regular expression
    $\mathtt{ac^*a|b(bb)^*c}$.}\label{fig:aca-or-bc}

    \includegraphics[width=\textwidth]{something_c_something}
    \caption{An automaton recognising the regular expression
    $\mathtt{.^*c.^*}$.}\label{fig:something-c-something}
  \end{minipage}
  \end{figure}

The na\"ive approach would be to first compute the product of the automata, then
use the approach from \cite{generate-parikh-image} to find its image under $f$,
and then verify the constraints. However, we can take advantage of both the
constraints and definitions of $f$ to prune the automata before computing the
product and help reduce the blow-up.


To give an intuition for the core idea of \Calculus{}, we will annotate the
transitions of $\SomethingCSomething$ and $\AcaOrBc$ with variables illustrating
how many times they are taken, and perform informal reasoning on them
corresponding to the reasoning performed by our calculus.

\begin{figure}[ht]
  \centering 
    \includegraphics[width=0.75\linewidth]{aca_or_bc_simplified}
    \caption{$\AcaOrBc{}$ with its symbolic transition counters after
    simplification of their equations.}\label{fig:example-simplify-1}
  \end{figure}

Note that we have already performed some reasoning in the starting figures; we
use the same symbol for the number of times each of the upper transitions of
$\AcaOrBc$ are taken, since we know that they must be taken the same number of
times, and we introduced the constant $1$ for the middle transition in
$\SomethingCSomething$, since it must always be taken precisely once to reach
the accepting state. We use standard graph-theoretical flow reasoning to set the
initial flow into the starting state to 1 and use linear equations to enforce
balanced flow throughout the automaton to determine the relations between the
counting variables on each transition (e.g. $1 + {l_b}' = l_a + l_b$, or $l_a +
l_c' = 1$). Strictly speaking, this is where the constant number $1$ in
\cref{fig:something-c-something} comes: from $1 + r_\Sigma = x + r_\Sigma$
solved for $x$. Using analogous linear equation reasoning loses additional
variables and gives us the annotations in \cref{fig:example-simplify-1}. 

We sum the individual transition variables up to get the length we are actually
interested in, e.g. the image of $f$ on $\SomethingCSomething =
\exists_{r_\Sigma, r_\Sigma'} \: r_\Sigma + 1 + r_\Sigma'$. For
$\SomethingCSomething{}$, this is all we need to produce the image since all
transitions are always accessible from any path from the initial state to the
accepting state.

The story gets more complicated with $\AcaOrBc{}$, where we have a choice of two
paths, and where that choice affects the possibility of taking either of the two
loops. In that case (letting the existential quantifiers from now on be
implicit), we would have the image $2l_a + l_c + l_c' + l_b + l_b'$. If we apply
standard linear reasoning on the equations (e.g. $1 + {l_b}' = l_a + l_b$, or
$l_a + l_c' = 1$), we can reduce the number of bound ($\exists$-quantified)
variables somewhat, obtaining the relations in
Figure~\ref{fig:example-simplify-1}, and the corresponding over-approximation on
the $f$-image $2 + l_c + 2l_b'$. It is an over-approximation because it only
considers the number of times each transition is taken and the existence of a
path from an initial to an accepting state, but omits the connectivity of that
path in the presence of loops. For example, no valid path would have both $l_c >
0$ and $l_b' > 0$, but no such restriction exists in the formula above.

\begin{figure}[ht]
  \centering 
    \includegraphics[width=0.75\linewidth]{something_c_something_length_simplified}
    \caption{$\SomethingCSomething{}$ after using linear reasoning over lengths to replace a variable.}\label{fig:example-length-reduced}
  \end{figure}

Still, this approximate solution allows us to perform further reasoning. Since
we know that the length of a string accepted by the intersection of the two
automata's languages must be the same, we also know that their images (string
lengths) must be the same under $f$, e.g. that $r_\Sigma + 1 + r_\Sigma' = 2 +
l_c + 2l_b' \iff r_\Sigma + r_\Sigma' = 1 + l_c + 2l_b'$. This in turn allows us
to replace one additional variable, e.g. $r_\Sigma = 1 + l_c + 2l_b' -
r_\Sigma'$ (see Figure~\ref{fig:example-length-reduced}). Note that the two
automata are now in direct contact. Any reasoning performed on one of them could
be propagated to the other.

Since we cannot take a transition a negative number of times, we have an
implicit constraint on any transition variable that it must be at least $0$.
This means that we have $1 + l_c + 2l_b' - r_\Sigma'$, implying an upper bound
on $r_\Sigma'$: $r_\Sigma' \leq 1 + l_c + 2l_b'$.

We talked initially about having a constraint on the image (or conversely, on
the strings of the language), so let us add one. We are only interested in
strings of odd length, e.g. $2 \nmid  f(\AcaOrBc \times \SomethingCSomething)$.
This means that: $2 \nmid 2 + l_c + 2l_b' \iff 2 \nmid l_c$. To encode this condition, we simply replace $l_c$ with $2k + 1$ with an existentially quantified integer $k \geq 0$.

This is as far as we can get with this reasoning. We must now choose: either we
compute the product and continue our reasoning, or we perform a case split. Let
us try a case split in order to put off computing a potentially large product.
We pick an early transition in the largest automaton with loops, in our case
$\FromLabelTo{start}{a}{sawA}$ of $\AcaOrBc$. This creates two cases: $1-l_c' =
0$ and $1-l_c' > 0$. We start with the first case, and after propagating the now known values and removing any transition that becomes zero, we get the automaton of Figure~\ref{fig:aca-or-bc-length-split-1}.


\begin{figure}[ht]
  \centering 
    \includegraphics[width=0.75\linewidth]{aca_or_bc_length_split_1}
    \caption{$\AcaOrBc{}$ at the split where $\FromLabelTo{start}{a}{sawA}$ is not used.}\label{fig:aca-or-bc-length-split-1}
  \end{figure}

  We immediately notice that the self-loop of state $sawA$ is now disconnected
  and must be removed. This condition can be detected efficiently using standard
  forwards/backwards reachability from the initial and accepting states
  respectively. In other words, $2k+1 = 0$ for this case and is implied by the splitting constraint. However, no choice of integer $k$  will satisfy this equation. Therefore, we close this branch and proceed with the other one -- where $1-l_c' > 0$. Note that we do so without ever computing a potentially expensive product of automata.

  \begin{figure}[ht]
    \centering 
      \includegraphics[width=0.75\linewidth]{aca_or_bc_length_split_2}
      \caption{$\AcaOrBc{}$ at the split where $\FromLabelTo{start}{a}{sawA}$ is used.}\label{fig:aca-or-bc-length-split-2}
    \end{figure}
  
Plugging in the assumption $1-l_c' > 0$ (and its consequence that $l_c' = 0$),
we also get a reduced variant of $\AcaOrBc$, as seen in
Figure~\ref{fig:aca-or-bc-length-split-1}. Note that $sawB$ is no longer
backwards-reachable from $final$ and therefore dead, along with both transitions
into it. We add the conclusion $l_b' = 0$, which severs both transitions into
the state and removes it from our automaton. Following this we are ready to
compute the product, which can be seen in Figure~\ref{fig:length-split-product}.

\begin{figure}[ht]
  \centering 
    \includegraphics[width=0.75\linewidth]{length_split_product}
    \caption{The product of $\AcaOrBc$ and $\SomethingCSomething$ after
    case splitting and removal of dead states.}\label{fig:length-split-product}
  \end{figure}

Before continuing, we need to tie the inequalities for the transitions of the
automata of the product to the new transition counting variables we introduced
in the product automaton to ensure that they are consistent. In our case, we obtain the following equations from the principle that a transition variable in either automaton must be equal to the sum of variables in the resulting edges of the product, which gives the following two interesting inequality from $\AcaOrBc{}$'s transition $\FromLabelTo{sawA}{c}{sawA}$: $2k + 1 = r_1 + r_2 + 1$

Note how this conserves the requirement that the number of loop iterations be
odd even when the same original loop appears three times in the product, twice
as a self-loop and once as a regular state transition. Using the same principle
as before for computing the length by adding up the transition counters, we
obtain the quantified formula $1 + r_1 + 1 = r_2 + 1 = r_1 + r_2 + 3 = 2k + r_1
+ 3$ for the length. Eliminating the final quantifier gives us the image of
$f(\AcaOrBc \times \SomethingCSomething)$ modulo odd lengths: $3 + 2k$, for some
integer $k$.

\subsection{Producing the Parikh image}\label{sec:introduction:parikh}

Now assume that rather than the length (e.g. the count of the characters) we
want to find the whole Parikh image of the intersection of $\AcaOrBc{}$ and
$\SomethingCSomething{}$. In other words, now $f$ is a mapping to an object
(usually represented as a column vector) counting how often each character
occurs in a string. Since extracting values from a vector is tiresome, we will
use the shorthand notation $\TransitionCount{a}$ to refer to the number of
letters a that appear. E.g. $\TransitionCount{a}(\text{curious and curiouser
still}) = 1$. Note that we still have the property that
$\TransitionCount{o}(\text{homomorphism}) = \TransitionCount{o}(\text{homo}) +
\TransitionCount{o}(\text{morphism}) = 2 + 1 = 3$. If you treat the addition as
element-wise vector addition, this approach works for the vector version as
well.

For our constraint, we will use $\TransitionCount{a} > \TransitionCount{b}$,
that is find a count of characters in a string accepted by both automata where
there are more a:s than b:s. Since the underlying calculus is the same, we start immediately from Figure~\ref{fig:example-simplify-1}.

  We now need to relate the number of times a transition was taken to the
  character counts we are really interested in, as opposed to just the length in
  the previous example. Note that this, like the length calculus before, puts
  the two automata into contact with each other, before any product has been
  computed. Some consideration leads us to the definition that the number of
  occurrence of each character corresponds to the sum of usages of transitions
  containing it, e.g. relations of the kind $\TransitionCount{a} = 2 - 2l_c$,
  and $\TransitionCount{b} = 2 l_b' + l_c'$.\footnote{For the attentive reader:
  in this example, we leave the catch-all transitions of $\SomethingCSomething$
  unconstrained since they will be taken care of by the product and would
  complicate the reasoning since we cannot yet know which character they will
  correspond to in the final product. In other words, this is a safe
  under-approximation that only limits our ability to reason before computing
  the product.}

This relation can be combined with the constraint that $\TransitionCount{a} >
\TransitionCount{b}$, that is that there are more instances in the accepted
string of letters a than b, to obtain $2 - 2l_c > 2 l_b' + l_c'$. Combining this
equation with the requirement from $\SomethingCSomething$ that
$\TransitionCount{c} \geq 1$ and $\TransitionCount{c} = l_c + l_c'$ from
$\AcaOrBc$ gives $0 \geq l_b'$, and since we cannot use a transition a negative
number of times $l_b' = 0$, which gives the automaton in
Figure~\ref{fig:example-simplify-2}.

\begin{figure}[ht]
  \centering 
    \includegraphics[width=0.75\linewidth]{aca_or_bc_simplified_2}
    \caption{$\AcaOrBc{}$ after removing the now unused b-transition.}\label{fig:example-simplify-2}
  \end{figure}

  Plugging in the same inequality again on the same principle, we get $2(1 -
  l_c') > l_c' \iff 1 > l_c'$ since we are working with integers, and
  therefore that $l_c' = 0$. This gives us the even smaller automaton of
  Figure~\ref{fig:example-simplify-3}.

  \begin{figure}[ht]
    \centering 
      \includegraphics[width=0.75\linewidth]{aca_or_bc_simplified_3}
      \caption{$\AcaOrBc{}$ after further constraint propagation.}\label{fig:example-simplify-3}
    \end{figure}

This will produce a straightforward product since both automata now have the
same shape, where we essentially only need to expand the self-loops of
$\SomethingCSomething$ and reduce its range labels to only capture a:s, as seen
in Figure~\ref{fig:final-example}. Note that we got this result by only
performing linear inequality reasoning on the automata of the product versus the
number of times each transition would be used.

\begin{figure}[ht]
\centering
    \includegraphics[width=0.75\linewidth]{aca_or_bc_times_something_c_something}
    \caption{$\AcaOrBc{} \times \SomethingCSomething{}$ under the constraint
    $\TransitionCount{a} > \TransitionCount{b}$. Note the fresh variables
    introduced for the transitions (for most transitions the same one, since
    they are all linearly dependent).}\label{fig:final-example}
  \end{figure}


  \begin{figure}[ht]
\centering
    \includegraphics[width=0.75\linewidth]{original_product}
    \caption{$\AcaOrBc{} \times \SomethingCSomething{}$ as it would have
    appeared if we had computed it from the initial
    automata.}\label{fig:original-product}
  \end{figure}


To obtain the final Parikh image, we need to tie the inequalities for the
transitions of the automata of the product to the new transition counters we
introduced in the product automaton. The only interesting transition is the one
carrying the c, so we will choose that one for illustration. In that case, the
bridging equation is $r_2 = l_c \land r_2 = r_{\Sigma}$ (and for all others
$=1$), which gives the Parikh image $\exists_{r_2} \TransitionCount{a} = 1 \land
\TransitionCount{b} = 0 \land \TransitionCount{c} = r_2 + 1$, and after trivial
existence-elimination using reasoning on lower bounds (e.g. $r_2 \geq 0$) we
arrive at $\TransitionCount{c} \geq 1$. For comparison, the $\AcaOrBc{} \times
\SomethingCSomething{}$ we would have obtained if we had eagerly computed the
product can be seen in Figure~\ref{fig:original-product}. This would have
introduced a significantly more complicated existentially quantified formula.

\section{Preliminaries}

\subsection{Finite-state Automata and their Products}
We define a non-deterministic automaton~$\Automaton$ with alphabet~$\Alphabet$
as $\Automaton = \AutomatonTuple$ where $\Transitions = \States \times \Alphabet
\times \States$, $\States$ is its states, $\InitialState$ is w.l.o.g. assumed to
be the single initial state, and $\AcceptingStates$ is its set of accepting
states.  We describe a transition $\Transition$ from state $\State$ to state
$\State'$ with label $\Label \in \Alphabet$ as $\Transition =
\FromLabelTo{\State}{\Label}{\State'}$. When one or more of the states and
labels of a transition are uninteresting we will omit it.

We will let variables $\Transition, \Transition', \Transition_1, \ldots,
\Transition_n$ etc describe transitions, $\State, \ldots, \State_n$ states, and
$\Automaton, \ldots, \Automaton_n$ automata, and use subscript indexing
($\Transitions_\Automaton$) to refer to the transitions, states, etc of a given
automaton.

By a \emph{product} of two automata $\Automaton_1, \Automaton_2$, written
$\Automaton_1 \times \Automaton_2$, we mean an automaton constructed to run
$\Automaton_1$ and $\Automaton_2$ in parallel on an input and only accept the
input if both automata would do so.

We refer to the resulting product states as tuples, $\Tuple{\State, \State'}$,
which represent the state of the product automaton where $\Automaton_1$ would be
in $\State$ and $\Automaton_2$ would be in $\State'$. Note that since we use
ordered tuples the product is technically (but w.l.o.g) not commutative; the
left-hand-side must come from the left-hand term. The sole purpose of this
matching is to allow us to speak with precision about the origins of components
in a product.

\subsection{The Parikh Map and its Image}
Formally, the \textit{Parikh map} over a context-free language $\Strings=
\left\{a_1, \ldots, a_k \right\}$ is defined as in \cite{kozen}:

$$
\begin{aligned}
& \ParikhMap: \MapFromTo{\Strings}{\natural^k} \\
& \ParikhMap(s) = \VectorLiteral{\#a_1(s), \#a_2(s), \ldots, \#a_k(s)}
\end{aligned}
$$

That is, $\ParikhMap(s)$ is a vector of the number of occurrences of each
character in the language for a given string $s$. For example, for  $\Strings =
\Set{a, b}$, we would have $\ParikhMap(abb) = \VectorLiteral{1, 2}$.

We define the image of this map, the \textit{Parikh image}, of some subset of
the language $\Language \subseteq \Strings$ as:

\[
\ParikhMap(\Language) = \Set{\ParikhMap(x) \SuchThat x \in \Language}
\]

Thus, we would have $\ParikhMap(\left\{ab, abb\right\}) = \left\{\left[1,
1\right], \left[1, 2\right]\right\}$. We also sometimes use the standard
notation $\#l$ to talk about an individual letter $l$. For example, for the Parikh vector above, we would have $\CountOf{a} = 1$. You have already seen 
this in Section~\ref{sec:introduction:parikh}.

Parikh's theorem states that ignoring order, any context-free grammar has a
letter-equivalent regular language (c.f.~\cite{construction} for a construction of
such automata from context-free grammars and~\cite{bounds} for bounds on its size). However, there are languages that are
not context-free that also have semilinear images under~$\ParikhMap$ (e.g.
$\ParikhMap(\Set{a^nb^nc^n \SuchThat n \geq 0}) = \ParikhMap((abc)^*) = \CountOf{a} =
\CountOf{b} = \CountOf{c} \land \CountOf{a} \geq 0$).

\subsection{The Parikh image of a regular language expressed in Presburger arithmetic}

Since Parikh images are semilinear, they can be fully expressed by Presburger
arithmetic. This means that any Parikh image can be written as a set of linear
equation. Following~\cite{generate-parikh-image}, we define the Parikh Image of
a regular language recognised by a DFA $\Automaton = \AutomatonTuple$ as:

\begin{equation}
\begin{aligned}
\ParikhMap(\Automaton) := 
& \AndComp{\Letter \in \Alphabet}{\LetterVar_{\Letter} = \sum_{\Transition = \FromLabelTo{}{\Letter}{} \in \Transitions} \TransitionVar_{\Transition}
}\\
& \AndComp{\State \in \AcceptingStates}{\FinalStateVar_{\State} > 0 \implies \StateVar_{\State} > 0}\\
& \AndComp{\State \in \States}{
  \left(\FinalStateVar_{\State} \text{ if } \State \in \AcceptingStates \text{ otherwise } 0 \right) +
  \sum_{\Transition = \FromLabelTo{\State}{}{}} - \sum_{\Transition = \FromLabelTo{}{}{\State}}
= \begin{cases}
    1 \text{  if $\State = \InitialState$} \\
    0 \text{ otherwise}
  \end{cases}
}\\
& \AndComp{\Transition = \FromLabelTo{}{}{\State} \in \Transitions}{
  \TransitionVar_{\Transition} > 0 \implies \StateVar_{\State} > 0
} \\
& \AndComp{\State \in \States}{
  \StateVar_{\State} > 0 \implies
  \left(\FinalStateVar_{\State} = 1 \text{ if } \State \in \AcceptingStates \land \right) \OrComp{\Transition = \FromLabelTo{\State}{}{\State'} \in \Transitions}{
    \StateVar_{\State} = \StateVar_{\State'} + 1 \land 
    \TransitionVar_{\Transition} \geq 1 \land
  \StateVar_{\State'} \geq 1
    }
}
\end{aligned}
\label{eq:generate-parikh}
\end{equation}

where all variables $\TransitionVar_i, \StateVar_i, \FinalStateVar_i$ are
existentially quantified and the free variables $\LetterVar_\Letter$ make up the
actual image. Performing standard quantifier elimination on $\TransitionVar_i,
\StateVar_i, \FinalStateVar_i$ with $\LetterVar_\Letter$ would produce the
Parikh image in its Presburger form. Note that compared to the method described
in~\cite{generate-parikh-image}, ours is backwards. Intuitively the approaches
are equivalent since the Parikh image, by definition, disregards order. The
approach can be described as either following paths through the automaton
forwards from an (in our case, \emph{the}) initial state, or conversely as in
the case here, choose an accepting state and trace a path backwards to the
initial state. 

In this paper we refer to this model as the baseline approach, though we also
apply some optimisations that allow us to detect unsatisfiability early,
described in Section~\ref{sec:implementing-baseline}. Notably, the calculus
introduced in this paper can at its core be described as lazily enforcing the
connectedness constraint of this method (the final two clauses) while also
interleaving the (similarly lazy) computation of products of automata.

\subsubsection{An Example}
We apply Equation~\ref{eq:generate-parikh} to the automaton $\AcaOrBc{}$ of Figure~\ref{fig:aca-or-bc}. We then get the quantified Presburger formula seen in Equation~\ref{eq:parikh-formula}. As before, we omit the implicit existential quantifiers for brevity, along with the constranints requiring all variables to be nonnegative. We also omit the $\land$-clauses between each line. In other words, in this example, only the variables $\LetterVar_a, \LetterVar_b, \LetterVar_c, \LetterVar_d, \LetterVar_e$, and $\LetterVar_f$ are free, and corresponds to the counts of the respective subscript letter.

\begin{equation}
  \begin{aligned}
  \ParikhMap(\Automaton):=\:
  &\LetterVar_{a} = \TransitionVar_{\FromLabelTo{0}{a}{1}} + \TransitionVar_{\FromLabelTo{1}{a}{3}} \land \LetterVar_{b} = \TransitionVar_{\FromLabelTo{0}{b}{2}} + \TransitionVar_{\FromLabelTo{2}{b}{0}} \land \LetterVar_{c} = \TransitionVar_{\FromLabelTo{1}{c}{1}} + \TransitionVar_{\FromLabelTo{2}{c}{3}}\\
  &\FinalStateVar_{3} > 0 \implies \StateVar_{3} > 0\\
  &\TransitionVar_{\FromLabelTo{0}{b}{2}} + \TransitionVar_{\FromLabelTo{0}{a}{1}} - \TransitionVar_{\FromLabelTo{2}{b}{0}} = 1 \land  \TransitionVar_{\FromLabelTo{1}{a}{3}} - \TransitionVar_{\FromLabelTo{0}{a}{1}} = 0\\
  & \TransitionVar_{\FromLabelTo{2}{b}{0}} + \TransitionVar_{\FromLabelTo{2}{c}{3}} - \TransitionVar_{\FromLabelTo{0}{b}{2}} = 0 \land \FinalStateVar_{3}  - \TransitionVar_{\FromLabelTo{1}{a}{3}} - \TransitionVar_{\FromLabelTo{2}{c}{3}} = 0\\
  &\TransitionVar_{\FromLabelTo{0}{a}{1}} > 0 \implies \StateVar_{1} > 0 \land \TransitionVar_{\FromLabelTo{0}{b}{2}} > 0 \implies \StateVar_{2} > 0\\
  &\TransitionVar_{\FromLabelTo{1}{c}{1}} > 0 \implies \StateVar_{1} > 0 \land \TransitionVar_{\FromLabelTo{1}{a}{3}} > 0 \implies \StateVar_{3} > 0\\
  &\TransitionVar_{\FromLabelTo{2}{b}{0}} > 0 \implies \StateVar_{0} > 0 \land \TransitionVar_{\FromLabelTo{2}{c}{3}} > 0 \implies \StateVar_{3} > 0\\
  &\StateVar_{0} > 0 \implies \left(\StateVar_{0} = \StateVar_{1} + 1 \land \TransitionVar_{\FromLabelTo{0}{a}{1}} \geq 1 \land \StateVar_{1} \geq 1\right) \lor \left(\StateVar_{0} = \StateVar_{2} + 1 \land \TransitionVar_{\FromLabelTo{0}{b}{2}} \geq 1 \land \StateVar_{2} \geq 1\right)\\
  &\StateVar_{1} > 0 \implies \left(\StateVar_{1} = \StateVar_{1} + 1 \land \TransitionVar_{\FromLabelTo{1}{c}{1}} \geq 1 \land \StateVar_{1} \geq 1\right) \lor \left(\StateVar_{1} = \StateVar_{3} + 1 \land \TransitionVar_{\FromLabelTo{1}{a}{3}} \geq 1 \land \StateVar_{3} \geq 1\right)\\
  &\StateVar_{2} > 0 \implies \left(\StateVar_{2} = \StateVar_{0} + 1 \land \TransitionVar_{\FromLabelTo{2}{b}{0}} \geq 1 \land \StateVar_{0} \geq 1\right) \lor \left(\StateVar_{2} = \StateVar_{3} + 1 \land \TransitionVar_{\FromLabelTo{2}{c}{3}} \geq 1 \land \StateVar_{3} \geq 1\right)\\
  &\StateVar_{3} > 0 \implies \FinalStateVar_{3} = 1
  \end{aligned}
  \label{eq:parikh-formula}
  \end{equation}
    
  We see here that the formula encodes the choice of path at the start of the
  automaton ($\TransitionVar_{\FromLabelTo{0}{b}{2}} +
  \TransitionVar_{\FromLabelTo{0}{a}{1}} -
  \TransitionVar_{\FromLabelTo{2}{b}{0}} = 1$), and the dependency between that
  choice and the following outgoing transition, e.g. in
  $\TransitionVar_{\FromLabelTo{1}{a}{3}} -
  \TransitionVar_{\FromLabelTo{0}{a}{1}} = 0$. Here we only have one choice of
  incoming flow; from $\FinalStateVar_3$, corresponding to the only accepting
  state.


Note that one of the $\lor$-clauses can be immediately removed since it
contains the contradiction $\StateVar_{1} = \StateVar_{1} + 1$, arising from
cycles. These clauses encode the distance in a spanning tree from an accepting
state, and the unsatisfiability is by design, essentially ensuring that two
states in a cycle cannot vouch for each other's connectedness.

Assume we are really interested in occurrences with the letter a, i.e. we solve
for $\LetterVar_a > 0$. This implies that $\StateVar_{1} > 0 \land \StateVar_{3}
> 0$ (essentially, that the state our transition of interest started in is
reachable), in turn implying that $\FinalStateVar_3 = 1$. Plugging in these
facts, we get:

\begin{equation}
  \begin{aligned}
  &\LetterVar_{a} = 2 \land \LetterVar_{b} = 0 \land \LetterVar_{c} = \TransitionVar_{\FromLabelTo{1}{c}{1}}\\
  & \StateVar_{3} > 0\\
  &\StateVar_{0} > 0 \implies \StateVar_{0} = \StateVar_{3} + 2\\
  \end{aligned}
  \label{eq:parikh-formula-calc}
  \end{equation}

We can then proceed to eliminate the remaining existentially quantified
variables to obtain the formula $\LetterVar_{a} = 2 \land \LetterVar_{b} = 0
\land \LetterVar_{c} \geq 0$ without too much effort.

  This example, in addition to showing how the encoding works, has hopefully
  also illustrated one of the key insights behind the design of \Calculus: the
  fact that even few additional constraints on the Parikh image can dramatically
  reduce the difficulty of computing it.

\section{Overview of our generalised Parikh images}

\subsection{Generalised Parikh Images}\label{sec:generalised}

To generalise the logic over Parikh images, we introduce a morphism $\Map:\: \Sigma
\to \Monoid$ where $\Monoid = \left(X;+_{\Monoid};0_{\Monoid}\right)$ is a commutative monoid where the following holds:
\begin{itemize}
  \item $\Map(\epsilon) = 0_{\Monoid}$
  \item $\Map(a \Concat b) = \Map(a) +_{\Monoid} \Map(b)$, where $\Concat$ is
  the string concatenation operation
\end{itemize}

Many other generalisations of the Parikh map have been studied, most of them
less plain than this one. Prominent examples include generalising the Parikh map
to segments of a fixed length \cite{KARHUMAKI1980155} and the more general
Parikh matrix, which gives more information about a word than the standard
Parikh image \cite{parikh-matrix}. Another notable generalisation is the
p-vector, introduced in~\cite{infinite-words}, which denotes the position of
each letter in the word rather than the number of their occurrences and allows
for generalisations into infinite alphabets. All of these, in some sense, add
something which is not already there in the Parikh map. By contrast, the main
utility of the formulation introduced here is that it allows us to \emph{remove}
something, thereby potentialy obtaining an easier problem.

\subsection{Applications}

\subsubsection{Example~1: String length}

A useful example of such a simplifying morphism can express string length, the
problem that originally motivated our study of the Parikh map. We let $\Monoid =
\left(\mathbb{Z};+;0\right)$ and let $\Map(x) = 1$. Then the length of a string
$s = s_1 \RepeatSum{\Concat}  s_n$ is given by $\Map(s) = \sum \Map(s_i) = 1
\RepeatSum{+} 1$. Note that if we substitute vectors and vector addition for
this monoid, we obtain the regular Parikh map.

Intuitively, this produces a simpler problem than computing the Parikh image, as
we now ignore not just the order of characters but also their differences. This
means that the branching automaton of Figure~\ref{fig:len-branch} becomes
equivalent to the non-branching automaton of Figure~\ref{fig:len-branch-free}.
Note that the elimination of the choice of paths also effectively eliminates the
possibility of disconnecting one of the self-loops. This, in turn, means that we
can eliminate all of the connectedness implications of the quantified formula,
which would in turn simplify it (and the quantifier elimination of it)
considerably.

\begin{figure}
  \includegraphics[width=0.5\linewidth]{choice}
  \caption{An example automaton with a choice of two paths: either we start with an a or a b.}
  \label{fig:len-branch}
\end{figure}

\begin{figure}
  \includegraphics[width=0.5\linewidth]{no-choice}
  \caption{An equivalent automaton to the one in Figure~\ref{fig:len-branch}
  when treating any letter the same as would be the case with a length
  constraint.}\label{fig:len-branch-free}
\end{figure}


\subsubsection{Example~2: Parikh Automata}\label{sec:parikh-automata}

Another application for this generalisation is to express Parikh automata, and
this is indeed what the implementation described in
Section~\ref{sec:implementation} does. Following~\cite{parikh-automata} as
filtered through~\cite{expressiveness} (which offers a better formalisation for
our use), we define a Parikh automaton as an automaton with integer counters
that are incremented at the transitions, and where we have linear
constraints on the final values of the counters after running the automaton, or
formally as in \cref{def:parikh-automata,def:projections}.

In the following definitions, we use (as in~\cite{expressiveness}) the following notation:
\begin{itemize}
  \item $\Alphabet$ is an alphabet
  \item $D$ is a finite subset of $\Naturals^d$, for some $d \in \Naturals^+$ (e.g. vectors of length $d$)
\end{itemize}

\begin{definition}\label{def:projections}
  We define two projections on the composite alphabet $\left(\Alphabet \times D\right)^*$:
  \begin{itemize}
    \item $\pi_{\Alphabet} := (a, \Vector{v}) \mapsto a$ or the projection on $\Alphabet$ is the monoid morphism from $\left(\Alphabet \times D\right)^*$ to $\Alphabet^*$, and the opposite projection
    \item $\pi_{v} := (a, \Vector{v}) \mapsto \Vector{v}$, called the
    \emph{extended Parikh image}.
  \end{itemize}
\end{definition}

\begin{definition}\label{def:parikh-automata} A Parikh automaton of dimension $d
  \geq 0$ is a pair $\Tuple{\Automaton, C}$, where $C \subseteq \Naturals^d$ is a semi-linear set (or, equivalently, a Presburger formula), where $\Automaton$ is a finite automaton with the alphabet $\Alphabet \times D$, where $D \subseteq \Naturals^d$.

  We say that $\Tuple{\Automaton, \varphi}$ \emph{recognises} the language, $\Language(\Automaton, C)$, that is it recognises the language crated by is the $\pi_\Alphabet$-projection on $\Alphabet$ whose $\pi_{v} \subseteq C$.
\end{definition}

\begin{figure}[ht]
  \centering
      \includegraphics[width=0.75\linewidth]{parikh_automaton}
      \caption{A The automaton part of a Parikh automaton for $\AcaOrBc{}$ with
      $\Alphabet = \Set{\text{a, b, c}}, d = 3$. The semilinear set/Presburger
      formula containing the constraints on the final register values cannot be
      visualised.}\label{fig:parikh-automaton}
    \end{figure}
  

An example of a Parikh automata version of $\AcaOrBc{}$ can be seen in \cref{fig:parikh-automaton}. A corresponding Presburger version of the constraint seen in \cref{sec:introduction:motivation} is $\psi(\Vec{v}) = v_1 > v_2$.

Generally, we can encode the problem as follows:
\begin{enumerate}
  \item The morphism ($\Map$) we would use is $\pi_{v}$
  \item The monoid operation is element-wise vector addition, which is trivially commutative.
  \item \Fudge{We would just add the constraints on the final values!?}
\end{enumerate}

\section{A Calculus for Generalised Parikh Images}\label{sec:calculus}

We assume an NFA $\Automaton = \AutomatonTuple$ with $\NrTransitions$
transitions $\Transitions =
\Set{\EllipsisSequence{\Transitions}{\NrTransitions}}$. For convenience, we then introduce the following supporting notations:

\begin{definition}
  A \textit{path} $\Path = \PathEnumeration$ of an automaton~$\Automaton$ with
  transitions $\Transitions$ represents a path through $\Automaton$ using
  transitions in $\Transitions$ (i.e. $\Transitions(\State_k, \Label_{k+1},
  \State_{k+1})$ holds for every $k$), passing zero or more labels $\Label_1,
  \ldots, \Label_n$. The path must begin in the initial state, i.e.~$\State_0 =
  \InitialState$. However, the ending state, $\State_n$, is not necessarily an
  accepting state.
  \end{definition}

\begin{definition}
  Moreover, we also talk about the \textit{set of paths} of an automaton,
  $\Paths(\Automaton)$, a possibly infinite (if $\Automaton$ has loops) set of
  valid paths through $\Automaton$. Additionally, we use the
  notation~$\Paths(\Automaton, \State)$ to mean all paths ending in
  state~$\State$.
\end{definition}

\begin{definition}
  The \textit{word} of a path $\WordOf(\Path) = \Label_1 \RepeatSum{\Concat} \Label_n$ is
  the word read out on its labels.
\end{definition}

\begin{definition}
  The \textit{states} of a path $\StatesOf(\Path) = \Set{\State \SuchThat
  \FromLabelTo{\State}{}{\State'} \in \Path \text{ or }
  \FromLabelTo{\State'}{}{\State} \in \Path}$ are the states visited along a
  path. Note that $\InitialState \in \StatesOf(\Path)$ for every path since all
  paths start in the initial state.
\end{definition}

\begin{definition}
  A \textit{cut}, $C$ of an automaton~$\Automaton = \AutomatonTuple$,
  $\SeparatingCut(C, \Automaton, \State)$, is a minimal set of transitions whose
  removal from~$\Transitions$ would cause~$\State \in \States$ to be unreachable
  from the initial state~$\InitialState$, or cut it off from reaching any
  accepting state. $\SeparatingCut(\Automaton, \InitialState)$ is crucially
  undefined if $\Automaton$ accepts any string since it cannot be made unreachable
  from itself by definition and must have at least onepath to an accepting state
  if it accepts any string.
\end{definition}

\begin{definition}
 The \textit{transition count}, $\TransitionCount(\Transition, \Path)$ is the
 number of times a transition $\Transition =
 \FromLabelTo{\State_1}{\Label}{\State_2} \in \Transitions$ appears in a path
 $p$.
\end{definition}

We then introduce the two predicates into our calculus with the following
definitions:

\begin{definition}\label{def:single-image}
  The Parikh predicate, $\SinglePredicateInstance$, for some automaton
  $\Automaton = \AutomatonTuple$, modulo some map $\Map$ to a commutative monoid
  $\Monoid$ as described in Section~\ref{sec:generalised} and with a transition
  selection function $\Filter:\: \Transitions \to \Naturals$ holds when
  $\MonoidElement \in \Monoid$ is the Parikh image of $\Automaton$ modulo
  $\Map$, or more formally when there is an accepting path $\Path =
  \PathEnumeration \in \Accepting{\Paths(\Automaton)}$ such that:
  \begin{itemize}
    \item $\Filter(\Transition) = \TransitionCount(\Transition, \Path)$
    \item $\MonoidElement = \Map(\WordOf(\Path))$
  \end{itemize}
\end{definition}

\begin{definition}
  $\Connected(\Automaton, \Filter)$ for some automaton $\Automaton =
  \AutomatonTuple$ holds when for every $\Transition =
  \FromLabelTo{\State}{}{\State'} \in \Transitions$, $\Filter(\Transition) > 0
  \implies \exists \Path \in \Paths(\Automaton)$ $\Filter(\Transition_i) > 0$
  for $\Transition_i \in p$, and $\State \in \StatesOf(p)$, or in words that
  there exists some $\Filter$-selected valid path that reaches $\Transition$'s
  starting state, $\State$. Intuitively, it represents the condition that
  $\Automaton$ is connected with respect to the selection function $\Filter$ for
  every transition. It is redundant to $\Image$ by design.
\end{definition}

Finally, we present the rules of our calculus for one automaton in Table~\ref{tbl:rules:single}. Note that we
operate on sets of symbols (terms, clauses). Additionally, we also use the
convention of splitting the formulas into linear inequalities ($\SomeInequalities$)
and any other clauses ($\SomeClause$).

We use the shorthand notation~$\Transitions_\Automaton$ to refer to the
transitions of an automaton~$\Automaton$. Additionally, for an
automaton~$\Automaton = \AutomatonTuple$ we allow mapping the selection function
like so: $\Filter(\Automaton) = \Tuple{\States, \InitialState, \AcceptingStates,
\Set{\Transition \in \Transitions \SuchThat \Filter(\Transition) > 0}}$, i.e.
$\Automaton$~with only the transitions for which~$\Filter$ is positive. In this
instance, the basis for the matched linear inequalities is
implicitly~$\SomeInequalities$.

These rules are meant to be executed in an automated theorem prover, and are
read bottom-up. Since a large part of our rules operate by adding and literally
matching linear (in)equalities in a proof goal, we use the shorthand of listing
the matched inequalities as antecedents. An example of this can be seen in
the~\Propagate{} rule.

Please note that the filtering function~$\Filter$ is evaluated symbolically in
these rules, and can in practice be read as a symbolic function from transitions
to terms (e.g.~\texttt{t} or~\texttt{t+1}), not dissimilar to~$\FlowEq$ below.
In our implementation, $\Filter$~is a vector of fresh unknown constants with the
same size as~$\Transitions$.

We assume that every rule only fires when it would add a new clause. For
example, this means that we cannot use~\Split{} to split on the same term twice.
This suggests a proof strategy where you \Propagate{} when you can, \Split{}
when you must, and \Subsume{} when neither is possible anymore.

\begin{table}[h]
\begin{tabular}{@{}l>{$}c<{$}p{3cm}@{}}\toprule
  Name & Rule & Prerequisites\\
  \midrule

  % EXPAND
  \Expand & 
    \inferrule
  {\Connected(\Automaton, \Filter) \land \FlowEq(\Automaton, \Filter) \land \MonoidElement = \sum_{\Transition \in \Transitions_\Automaton} \Filter(\Transition) \cdot \Map(\Transition), \SomeInequalities, \SomeClause}
  {\SinglePredicateInstance, \SomeInequalities, \SomeClause} & 
  None \\

  % SPLIT
  \Split & 
  \inferrule{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause, \Filter(\Transition) = 0 \mid \Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause, \Filter(\Transition) > 0}{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause} &
  if $\Transition \in \Transitions_\Automaton$ \\

  % PROPAGATE
  \Propagate &
  \inferrule{\Connected(\Automaton, \Filter), \Set{\Filter(\Transition') = 0 \SuchThat \Transition' \in C}, \SomeInequalities, \SomeClause, \Filter(\Transition) = 0}{\Connected(\Automaton, \Filter), \Set{\Filter(\Transition') = 0 \SuchThat \Transition' \in C}, \SomeInequalities, \SomeClause} &
  if $t = \FromLabelTo{\State}{}{\State'} \in \Transitions_\Automaton, \SeparatingCut(C, \Automaton, \State)$\\

  % SUBSUME
  \Subsume &
  \inferrule{\SomeInequalities,\SomeClause}{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause} &
  \Split{} and \Propagate{} cannot be applied \\
  \bottomrule
  \end{tabular}
  \caption{Derivation rules for one automaton.}\label{tbl:rules:single}
\end{table}

We use the symbolic function $\FlowEq(\Automaton, \Filter)$ that generates a set
of existentially quantified linear inequalities with the following definition:
$$
\begin{aligned}
  & \FlowEq(\Automaton, \Filter) = \sum\limits_{\State \in \AcceptingStates} \FinalStateVar_\State = 1 \land \AndComp{\State \in \States}{\In(\State, \Filter) - \Out(\State, \Filter)} = \Sink(\State) \\
  & \Sink(\State) = 0 \text{ if } \State \not\in \AcceptingStates, \FinalStateVar_\State \text{ otherwise.} \\
  & \In(\State, \Filter) = \StartFlow(\State) + \sum_{\Transition = \FromLabelTo{\State_0}{}{\State} \in \Transitions} \Filter(\Transition)\\
  & \StartFlow(\State)  = 1 \text{ if } \State = \InitialState, \text{ otherwise $0$.} \\
  & \Out(\State, \Filter) = \sum_{\Transition = \FromLabelTo{\State}{}{\State'} \in \Transitions} \Filter(\Transition)
\end{aligned}
$$
Where we transparently assign fresh variables to each state $\State \in
\States$ as well as every accepting state $\State' \in \AcceptingStates$ and
existentially quantify them in the whole clause. 

Additionally, we assume the existence of a rule \PresburgerClose{},
corresponding to a sound and complete solver for Presburger formulae, modulo the
monoid~$\Monoid$.

The $\Propagate{}$ rule allows us to propagate (dis-)connectedness across
$\Automaton$. It states that we are only allowed to use transitions attached to
a reachable state, and is necessary to ensure connectedness in the presence of
cycles in~$\Automaton$.

\textsc{Expand} expands the predicate into its most basic rules; one set of
linear equations synchronising the transitions mentioned by~$\Filter$ to the
corresponding Monoid element~$\MonoidElement$, and the linear flow equations of
the standard Parikh image formulation, as described by~\FlowEq. Since
$\Connected$ and $\Image$ are partially redundant and the difference is covered
by~$\FlowEq$, we can remove the instance of~$\Image$ when applying~$\Expand$. In
this sense, we split the semantics of the $\Image$~predicate into its counting
aspect (covered by $\FlowEq$) and its connectedness aspect (covered by
$\Connected$).

Finally, $\Split{}$ allows us to branch the proof tree by trying to exclude a
contested transition from a potential solution before concluding that it must be
included. Intuitively, this is what guarantees our ability to make forward
progress by eliminating paths through~$\Automaton$.

A decision procedure for our predicate in a tableau-based automated theorem
prover would start by expanding the predicate using the $\Expand{}$~rule. A
modestly clever theorem prover would perform algebraic substitution on the
underlying constants of~$\Filter$, boiling them down to choices of branches,
which depend on one single variable, and loop transitions. This logic
corresponds to the placement of counters for optimally edge-profiling the CFG of
a program, making up a MST of the automaton~\cite{path-profiling}.

\subsection{An Example}

Starting with~$\AcaOrBc{}$, where $\Map$ is the length function, in effect
$\Transition \mapsto 1$ for transitions, and the constraints that the length is
odd using the same trick as in \cref{sec:introduction:motivation}, we have the
definitions in \cref{fig:example:single:equivalences} (omitting existential
quantifiers and $x \geq 0$ for every variable to avoid clutter).

\begin{figure}[ht]
  We define $\Filter$ and $\FlowEq(\AcaOrBc{}, \Filter)$ as follows:

  \begin{minipage}[b]{0.45\linewidth}
    \begin{equation*}
      \begin{aligned}
        & \Filter(\FromLabelTo{start}{a}{sawA}) & = \TransitionVar_1 \\
        & \Filter(\FromLabelTo{start}{b}{sawB}) & = \TransitionVar_2 \\
        & \Filter(\FromLabelTo{sawA}{c}{sawA})  & = \TransitionVar_3  \\
        & \Filter(\FromLabelTo{sawB}{b}{start}) & = \TransitionVar_4 \\
        & \Filter(\FromLabelTo{sawA}{a}{final}) & = \TransitionVar_5 \\
        & \Filter(\FromLabelTo{sawB}{c}{final}) & = \TransitionVar_6 \\
      \end{aligned}
    \end{equation*}    
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}[b]{0.45\linewidth}
    \begin{equation*}
      \begin{aligned}
        % start
        &  \TransitionVar_4 = \TransitionVar_5 + \TransitionVar_2 - 1 \\
        % sawA
        & \TransitionVar_1 = \TransitionVar_5 \\
        % sawB
        & \TransitionVar_2 = \TransitionVar_4 + \TransitionVar_6 \\
        % final
        & \TransitionVar_5 + \TransitionVar_6 = \FinalStateVar_1 \\
        & \FinalStateVar_1 = 1 \\
      \end{aligned}  
    \end{equation*}    
  \end{minipage}
  \caption{Equivalences defining $\Filter$ and $\FlowEq(\AcaOrBc{}, \Filter)$
  respectively.}\label{fig:example:single:equivalences}
  \end{figure}


Propagating the equivalences from \FlowEq{} gives us (under the implicit
assumption that every RHS is $\geq 0$):
\begin{equation*}
  \begin{aligned}
    & \Filter(\FromLabelTo{start}{a}{sawA}) & = 1 - \TransitionVar_6 \\
    & \Filter(\FromLabelTo{start}{b}{sawB}) & = \TransitionVar_4 + \TransitionVar_6 \\
    & \Filter(\FromLabelTo{sawA}{c}{sawA})  & = \TransitionVar_3  \\
    & \Filter(\FromLabelTo{sawB}{b}{start}) & = 2\TransitionVar_6 + \TransitionVar_4 \\
    & \Filter(\FromLabelTo{sawA}{a}{final}) & = 1 - \TransitionVar_6 \\
    & \Filter(\FromLabelTo{sawB}{c}{final}) & = \TransitionVar_6 \\
  \end{aligned}
\end{equation*}

\begin{figure}
  \centering
\begin{prooftree}
  \infer0[\PresburgerClose]{
    \begin{matrix}
      1 - \TransitionVar_6 = 0 \land \\
      \TransitionVar_6 = 1 \land \\
      \TransitionVar_4 + 1 > 0 \land \\
      \TransitionVar_3 = 0 \land \\
      2 + \TransitionVar_4 > 0 \land \\
      k = 1 + \TransitionVar_4 
    \end{matrix}
  }
  \infer1[\Subsume{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 = 0 \land \\
      \TransitionVar_6 = 1 \land \\
      \TransitionVar_4 + 1 > 0 \land \\
      \TransitionVar_3 = 0 \land \\
      2 + \TransitionVar_4 > 0 \land \\
      k = 1 + \TransitionVar_4 
    \end{matrix}
  }
  \infer1[\Propagate]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 = 0 \land \\
      \TransitionVar_6 = 1 \land \\
      \TransitionVar_4 + 1 > 0 \land \\
      \TransitionVar_3 \geq 0 \land \\
      2 + \TransitionVar_4 \geq 0 \land \\
      2k = 2 + 2\TransitionVar_4 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[Eq. reason]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 = 0 \land \\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3 + \TransitionVar_6
    \end{matrix}
  }
  % BRANCH:  = 0
  \infer0[\PresburgerClose{}]{
    \begin{matrix}
      \TransitionVar_3 > 0 \\
      \TransitionVar_6 = 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\Subsume{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \TransitionVar_3 > = 0 \\
      \TransitionVar_6 = 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[Eq. reason]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \TransitionVar_6 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0\\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\Propagate{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \TransitionVar_6 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[Eq. reason]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3 + \TransitionVar_6
    \end{matrix} % BRANCH: > 0
  } % SPLIT
  \infer2[\Split{} $1 - \TransitionVar_6$]{ \Connected(\AcaOrBc{}, \Filter) \land 2k = 1 + 2\TransitionVar_4 + \TransitionVar_3 + \TransitionVar_6 }
  \infer1[\Expand{}]{\Image{}_{\AcaOrBc{}, \Transition \mapsto 1}(\Filter, 2k + 1)}
\end{prooftree}
\caption{A derivation for \Calculus{} computing odd lengths in $\AcaOrBc{}$.}\label{fig:derivation:single}
\end{figure}

In \cref{fig:derivation:single}, we see how we start by expanding the predicate
using the (simplified) flow equations. We can also see how interleaving
reasoning on the corresponding linear equations helps the production. In both
branches the reasoning is similar: we conclude that when choosing either path at
the starting state we must avoid the other one, perform propagation based on
that fact, use some equational reasoning to derive the expected forms of the
upper and lower bounds to propagate the now disconnected transitions, and
finally subsume, removing the $\Connected{}$ predicate when we are unable to use
either of the rules, leaving only the satisfiable remnants of the flow
equations.

\subsection{\Calculus{} for one automaton is correct}\label{sec:single:correct}

To be as clear as possible about what it means for our calculus to be correct,
we use more exact definitions than the traditional soundness and completeness.
Since correctness is guaranteed by the formulation of the predicate, our
correctness proof means proving inductively that no rule would invalidate that
definition.

\subsubsection{\Calculus{} terminates}
\begin{lemma}\label{lma:single-terminates}
  A solver implementing the calculus described in \cref{sec:calculus} starting
  with some decidable (modulo the solver) set of clauses $\SomeClause{}$ and an
  instance of our predicate $\SinglePredicateInstance$ will eventually
  terminate.
\end{lemma}

\begin{proof}
The first remaining source of divergence is from leftover predicates. The
$\Image$ predicate can always be trivially removed by applying \Expand{}. This
means a leftover $\Image$ predicate can never cause divergence. The only
remaining predicate then is $\Connected$, which can always be removed by
applying $\Subsume$ when no other of the rules is applicable. Hence all
predicates introduced in the calculus can always be removed.

It remains to prove that each rule is only applied finitely many times. This is
almost obvious since both \Propagate{} and \Split{} can only be applied to
transition variables whose $\Filter$ value has no known bound. However, \Split{}
can always be used to split the proof tree and add clauses in precisely the
right shape, thereby guaranteeing the ability to make forward progress.
Therefore, the number of transition variables whose bounds are unknown is
decreasing monotonically for any given proof goal, and hence termination is
guaranteed.
\end{proof}

\subsubsection{\Calculus{} computes the homomorphic image}
\begin{lemma}\label{lma:single-correct}
  Starting with $\SinglePredicateInstance$, we will close the goal with $\top$
  if $\exists s \in \Language(\Automaton) \HoldsThat \Map(s) = \MonoidElement$
  and $\bot$ otherwise.
\end{lemma}
\begin{proof}
Since we are always guaranteed to terminate following
\cref{lma:single-terminates}, we know that we will close the goal eventually.
Additionally, the property of \cref{lma:single-terminates} is trivially obtained
from the semantics of the $\Image$ predicate as defined in
\cref{def:single-image}. This means that we only need to prove the preservation
of those properties to prove \cref{lma:single-correct}.

\begin{lemma}\label{lma:expand-preserves}
  \Expand{} preserves the semantics of $\Image$.
\end{lemma}
\begin{proof}

  After application of \Expand{}, $\FlowEq$ guarantees that any
  $\Filter(\Transition)$  for every $\Transition \in \Automaton$ is consistent
  with the flow through the automaton. After its application, the proof goal
  either contains an unsatisfiable set of Presburger formulae if the flow is
  infeasible, otherwise a satisfiable set. An infesaible flow can happen in
  either of these conditions:
  \begin{itemize}
    \item if $\Filter$ demands that some transition $\Transition$ is visited more times than would be allowed by $\Automaton$, or
    \item if the path described by $\Filter$ is not consistent with a path from
    $\InitialState$ to some state $\State$ in $\AcceptingStates$.
  \end{itemize}

  Note that \Expand{} is taken directly from the existing formulation of
  \cref{eq:generate-parikh}.
\end{proof}

\begin{lemma}\label{lma:split-preserves}
  \Split{} preserves the semantics of $\Image$.
\end{lemma}
\begin{proof}
  \Split{} allows us to case-split at most once on each transition $\Transition
  \in \Transitions$. This means that our proof tree at its leaves will be
  guaranteed to have all transitions of known bounds in the correct format. This
  means that once \Split{} is at fixpoint, the status (present or absent) of any
  transition is known. Therefore it preserves the semantics of the $\Image$
  predicate.
\end{proof}

\begin{lemma}\label{lma:propagate-preserves}
  \Propagate{} preserves the semantics of $\Image$.
\end{lemma}
\begin{proof}
  \Propagate{} will lazily enforce connectedness to any state. If it can be
  applied, there exists a cut $C$ in $\Automaton$ such that some state $\State$
  for any incoming transition $\Transition \in \Transitions_\Automaton$ has
  $\Filter(t) = 0$  as part of the equations of the proof goal, and such that
  $\State$ has an outgoing transition $\Transition'$ whose lower bound is not
  $0$. Adding this conclusion will make equations unsatisfiable precisely if
  there exists a transition $\Transition \in \Transitions_\Automaton$ such that
  $\Filter(\Transition)$ is inconsistent with the homomorphic image, since it
  implies that the selection of transitions is disconnected.
\end{proof}

\begin{lemma}\label{lma:subsume-preserves}
  \Subsume{} preserves the semantics of $\Image$.
\end{lemma}
\begin{proof}
  Since this rule can \emph{only} be applied if no other rule can, it means that
  the upper and lower bounds of any transition $\Transition \in \Transitions$ as
  defined by $\Filter(\Transition)$ is known, meaning clauses of the shape
  $\Filter(\Transition) > 0$ or $\Filter(\Transition) = 0$ are present in the
  proof goal for every transition and that the $\Image$ predicate has been
  consumed by \Expand{} to be replaced by $\Connected$. This in turns means that
  the goal can be closed only if the assignment of $\Filter$ is consistent with
  an accepting path through $\Automaton$.
  
  It also means that \Propagate{} must have been applied whenever possible, meaning that for any transition $\Transition$ starting in a state $\State$ that is disconnected the corresponding $\Filter$ values, we must have applied $\Propagate{}$ to derive $\Filter(\Transition) = 0$. Note that for an infeasible assignment (such as an unsatisfiable instance) this would contradict other decisions, but it would preserve the semantics of $\Image$.
\end{proof}

Since the final step we execute is \Subsume, which we know from
\cref{lma:subsume-preserves} preserves the semantics of $\Image$ and each step
along the way
(\cref{lma:expand-preserves,lma:split-preserves,lma:propagate-preserves}), it
holds that \Calculus{} for one automaton also preserves the semantics.
\end{proof}

\subsection{Finding the Presburger representation of a homomorphic image}\label{sec:finding-the-image}

To find the Presburger form of the homomorphic image efficiently, we adapt the
quantifier elimination approach of~\cite{qe} to our problem domain. The core
method is the same: we incrementally use \Calculus{} to find models,
\Generalise{} the models we find into a quantifier-free Presburger formula with
the $\MonoidElement$ as the only free variable (algorithm~\ref{alg:generalise}), add the
negated formula as a constraint, and continue enumerating models until none
remain. The disjunction of the generalised models we enumerated is now our
image.

\begin{algorithm}
  \caption{$\Generalise{}(\Automaton, \Filter, a)$ will generalise a final product $\Automaton$ and a model $a$ under the homomorphism $\Map$.}\label{alg:generalise}
  \KwData{$\Automaton$, a product from \Calculus{}, a model $a$ assigning counts to the terms that $\Filter$ associate with each transition of $\Automaton$, and our homomorphism $\Map$ that we want to compute the image modulo.}
  \KwResult{a quantifier-free Presburger formula $P$ representing a partial $\Map$-homomorphic image}
  \SetKwFunction{EliminateQuantifiers}{eliminateQuantifiers}

  $\Automaton' \gets \Tuple{\Automaton_\States, \Automaton_{\InitialState}, \Automaton_\AcceptingStates, \Set{\Transition \in \Automaton_\Transitions \SuchThat a(\Filter(\Transition)) \neq 0}}$

  \Fudge{// This notation might be too much but IT'S OBVIOUS DAMMIT}

  \KwRet{\EliminateQuantifiers{$\Map(\ParikhMap(\Automaton'))$}}
  \end{algorithm}


  \begin{algorithm}
    \DontPrintSemicolon
    \caption{$\FindImage{}(\Automaton_1 \times \ldots \times \Automaton_k, \Map)$ will find the Presburger form for the product $\Automaton_1 \times \ldots \times \Automaton_k$ modulo a homomorphism $\Map$ where the only free variable is/are the one(s) representing the monoid element of $\Map$.}\label{alg:find-image}
    \KwData{$\Automaton$, a product from \Calculus{}, a model $a$ assigning counts to the terms that $\Filter$ associate with each transition of $\Automaton$, and our homomorphism $\Map$ that we want to compute the image modulo.}
    \KwResult{a quantifier-free Presburger formula $P$ representing a partial $\Map$-homomorphic image}
    \SetKwFunction{NewTheoremProver}{newTheoremProver}
    \SetKwFunction{EliminateQuantifiers}{eliminateQuantifiers}
    \SetKwFunction{FreshVariable}{freshVariable}
    \SetKwFunction{Assert}{assert}
    \SetKwFunction{GetModel}{getModel}
    \SetKwData{ImageVar}{image}
  
$p \gets \NewTheoremProver{}$\;
$\Filter(\Transition) := \FreshVariable{p}$ for every $\Transition \in \Transitions_{\Automaton}$\;
$\MonoidElement \gets$ \FreshVariable{$p$}\;
\Assert{$p, \exists \MonoidElement, \Filter(\Transition) \text{ for every } \Transition \in \Transitions_\Automaton \HoldsThat \ImagePredicate{\Automaton}{\Map}{\Filter}{\MonoidElement} \land \AndComp{\Transition \in \Transitions}{\Filter(\Transition) \geq 0}$}\;
$\ImageVar \gets \bot$\;
\While{$p$ has more models}{
  $\Tuple{\Automaton, a} \gets \GetModel(p)$\;
  $G \gets \Generalise{}(\Automaton, \Filter, a)$\;
  $\ImageVar \gets G \lor \Image$\;
  \Assert{$p, \lnot G$}\;
  }
    \KwRet{\ImageVar}
    \end{algorithm}

    The attentive reader will notice that the \GetModel{} function is
    nonstandard in that it returns both the model and the automaton used to
    generate it. Since we implement algorithm~\ref{alg:find-image} inside a theorem
    prover under our control, we are able to glean such internal data structures. This should be considered fair game, since we are in practice implementing a specialised form of quantifier elimination, which is an internal affair.
    
    The performance of this approach is evaluated in
    \cref{sec:evaluation:finding-image}, where we benchmark our implementation
    of it.

\section{Parikh Images from Products of Automata}\label{sec:multiple}

To generalise the calculus to calculations on products of automata, we change the main predicate to take arbitrarily many automata:
\begin{definition}\label{def:multiple}
  $\ImagePredicate{\Automaton_1\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}$
  is true exactly when the single-automaton version of the predicate would hold
  for the automaton.
\end{definition}

  For the calculus, we first extend $\Expand$ to generate flow equations and instances of $\Connected$ for each automaton.

  \begin{table}[h]
    \begin{tabular}{@{}l>{$}c<{$}p{3cm}@{}}\toprule
      Name & Rule & Prerequisites\\
      \midrule
    
      % EXPAND
      \ExpandM & 
      \inferrule
      {
        {\begin{matrix}
          \Set{ 
            \MonoidElement = \sum_{\Transition \in \Transitions_{\Automaton_i}} \Filter(\Transition) \cdot \Map(\Transition)
          \SuchThat \Automaton_i \in \Automaton_1,\ldots,\Automaton_k}, \\
          \Set{\FlowEq(\Automaton_i), \Connected(\Automaton_i) \SuchThat \Automaton_i \in \Automaton_1,\ldots,\Automaton_k}, \\
          \ImagePredicate{\Automaton_1\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \\
          \SomeInequalities, \SomeClause
        \end{matrix}}
        }
      {\ImagePredicate{\Automaton_1\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause} & 
      None \\
      \Materialise &
      \inferrule
      {    \BindingSum(\Automaton', \Filter),\ImagePredicate{\Automaton'\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause}
      {\ImagePredicate{\Automaton_1\times\Automaton_2\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause} &
      $\Automaton' = \Automaton_1\times\Automaton_2$ \\
      \bottomrule
      \end{tabular}
      \caption{Additional derivation rules for products of arbitrarily many automata.}\label{tbl:rules:multi}
    \end{table}

\ExpandM{} must be applied before any other rule, like \Expand{}, but unlike \Expand{}, \ExpandM{} does not remove the $\Image$~predicate since it is needed to keep track of the product.

Before we can begin to define our final rule, we need to talk about product states.

Then we introduce the rule $\Materialise$, used to compute a partial product between two terms:

  With the following helper symbolic function:

  $$
  \BindingSum(\Automaton_1 \times \Automaton_2, \Filter) = \bigcup
  \begin{aligned}
  & \Set{ 
    \left<\Filter(\Transition)  =  \sum\limits_{\Transition' = \FromLabelTo{\Tuple{\State, \State_R}}{\Label}{\Tuple{\State', \State_R}}} \Filter(\Transition')\right>
  \SuchThat \Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions_{\Automaton_1} } , \\ 
  & \Set{
    \left<\Filter(\Transition)  =  \sum\limits_{\Transition' = \FromLabelTo{\Tuple{\State_L, \State}}{\Label}{\Tuple{\State_L, \State'}}} \Filter(\Transition')\right> \SuchThat \Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions_{\Automaton_2}
  }
  \end{aligned}
$$

Note that this definition implies that $\Filter(\Transition) =
\Filter(\Transition')$ whenever two transitions $\Transition \in
\Transitions_{\Automaton_1}, \Transition' \in \Transitions_{\Automaton_2}$
produces a product transition $\Transition'' \in \Transitions_{\Automaton_1
\times \Automaton_2}$. This corresponds to our intuition that the terms of the
product must agree on the value they accept. As before, we implicitly map
$\Filter$ to fresh terms for each transition in the product.

Finally, for instances of precisely one automaton, neither rule applies and we
perform the calculus as before.

\subsection{An Example}\label{sec:multiple:example}

We return again to our example in \cref{sec:introduction:parikh}, where we
compute the whole Parikh image of the product of $\AcaOrBc{}$ and
$\SomethingCSomething$ under the constraint that there are more instances of
letters a than b. We omit the definition of the map $\Map$ that maps each
transition to the corresponding increment vector (e.g.
\Map($\FromLabelTo{start}{a}{sawA}) = \begin{bmatrix}1\\0\\0\end{bmatrix}$),
since it should be apparent by now.

In the interest of space, we use a similar technique as in
\cref{fig:example:single:equivalences} to combine the application of $\FlowEq$
with the definition of $\Filter$.

  We define $\Filter$ after substitutions from both initial applications of
  $\FlowEq$ as follows, note the element-wise expansion for the $\Sigma$ labels:

% Propagated equations
    \begin{equation*}
      \begin{aligned}
        & \Filter(\FromLabelTo{start}{a}{sawA}) & = 1 - \TransitionVar_6 \\
        & \Filter(\FromLabelTo{start}{b}{sawB}) & = \TransitionVar_4 + \TransitionVar_6 \\
        & \Filter(\FromLabelTo{sawA}{c}{sawA})  & = \TransitionVar_3  \\
        & \Filter(\FromLabelTo{sawB}{b}{start}) & = 2\TransitionVar_6 + \TransitionVar_4 \\
        & \Filter(\FromLabelTo{sawA}{a}{final}) & = 1 - \TransitionVar_6 \\
        & \Filter(\FromLabelTo{sawB}{c}{final}) & = \TransitionVar_6 \\
        % Other automaton
        & \Filter(\FromLabelTo{start}{\Sigma}{start}) & = \TransitionVar_{7a}, \TransitionVar_{7b}, \TransitionVar_{7c} \\
        & \Filter(\FromLabelTo{start}{c}{final}) & = 1 \\
        & \Filter(\FromLabelTo{final}{\Sigma}{final}) & = \TransitionVar_{9a}, \TransitionVar_{9b}, \TransitionVar_{9c} \\
      \end{aligned}
    \end{equation*}
    
\begin{prooftree}
  \infer0[\PresburgerClose{}]{
    \begin{matrix}
      1 = \TransitionVar_{10} \\
      \TransitionVar_3 = \TransitionVar_{11} \\
      1 = \TransitionVar_{12} \\
      1 = \TransitionVar_{11} \\
      2 > 0
    \end{matrix}  
  }
  \infer1[\Subsume{}, \Expand{}, \Subsume{}]{
    \begin{matrix}
      1 = \TransitionVar_{10} \\
      \TransitionVar_3 = \TransitionVar_{11} \\
      1 = \TransitionVar_{12} \\
      1 = \TransitionVar_{11} \\
      \Image{}_{\Automaton', \Map}(\Filter, 
      \begin{bmatrix}
        2 \\
        0 \\
        1
        \end{bmatrix}) \land \\
      \Connected(\SomethingCSomething{}, \Filter) \land \\
        2 > 0
    \end{matrix}  
  }
  \infer1[Eq. reasoning]{
    \begin{matrix}
      1 = \TransitionVar_{10} \\
      \TransitionVar_3 = \TransitionVar_{11} \\
      1 = \TransitionVar_{12} \\
      1 = \TransitionVar_{11} \\
      \Image{}_{\Automaton', \Map}(\Filter, 
      \begin{bmatrix}
        a \\
        0 \\
        c
        \end{bmatrix}) \land \\
      \TransitionVar_6 = 0 \land \\
      \TransitionVar_4 = 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
      \begin{bmatrix}
        a \\
        b \\
        c
        \end{bmatrix} = \begin{bmatrix}
          2  \\
          0 \\
          1
        \end{bmatrix} \land
        \\
        \begin{bmatrix}
          a \\
          b \\
          c
          \end{bmatrix} = \begin{bmatrix}
            \TransitionVar_{7a} + \TransitionVar_{9a} \\
            0 \\
            \TransitionVar_{7c} + \TransitionVar_{9c} + 1
          \end{bmatrix} \land \\
      \Connected(\SomethingCSomething{}, \Filter) \land \\
        a > 0
    \end{matrix}  
  }
\end{prooftree}

\begin{prooftree}
  \infer0[\Materialise]{
    \begin{matrix}
      \TransitionVar_6 = 0 \\
      \TransitionVar_4 = 0 \\
      2\TransitionVar_6 + \TransitionVar_4 = 0 \\
      \begin{bmatrix}
        a \\
        b \\
        c
        \end{bmatrix} = \begin{bmatrix}
          2  \\
          0 \\
          \TransitionVar_3 
        \end{bmatrix}
        \\
        \begin{bmatrix}
          a \\
          b \\
          c
          \end{bmatrix} = \begin{bmatrix}
            \TransitionVar_{7a} + \TransitionVar_{9a} \\
            0 \\
            \TransitionVar_{7c} + \TransitionVar_{9c} + 1
          \end{bmatrix} \\
      \Connected(\SomethingCSomething{}, \Filter) \land \\
      \Image{}_{\AcaOrBc{}\times\SomethingCSomething{}, \Map}(\Filter, 
      \begin{bmatrix}
        a \\
        0 \\
        c
        \end{bmatrix}) \land \\
        a > 0
    \end{matrix}  
  }
  \infer1[\Subsume{}]{
  \begin{matrix}
    \TransitionVar_6 + \TransitionVar_4 = 0 \\
    1 - \TransitionVar_6 > 0 \\
    \TransitionVar_6 = 0 \\
    \TransitionVar_4 = 0 \\
    \TransitionVar_3 > 0 \\
    2\TransitionVar_6 + \TransitionVar_4 = 0 \\
    \begin{bmatrix}
      a \\
      b \\
      c
      \end{bmatrix} = \begin{bmatrix}
        2  \\
        0 \\
        \TransitionVar_3 
      \end{bmatrix}
      \\
      \begin{bmatrix} % What to do about sigma!?
        a \\
        b \\
        c
        \end{bmatrix} = \begin{bmatrix}
          \TransitionVar_{7a} + \TransitionVar_{9a} \\
          0 \\
          \TransitionVar_{7c} + \TransitionVar_{9c} + 1
        \end{bmatrix} \\
    \Connected(\AcaOrBc{}, \Filter) \land \\
    \Connected(\SomethingCSomething{}, \Filter) \land \\
    \Image{}_{\AcaOrBc{}\times\SomethingCSomething{}, \Map}(\Filter, 
    \begin{bmatrix}
      a \\
      0 \\
      c
      \end{bmatrix}) \land \\
      a > 0
  \end{matrix}
  }
  \infer1[Eq. reasoning]{
    \begin{matrix}
      \begin{bmatrix}
        a \\
        b \\
        c
        \end{bmatrix} = \begin{bmatrix}
          2 - 2\TransitionVar_6 \\
          2\TransitionVar_4 + 3\TransitionVar_6 \\
          \TransitionVar_3 + \TransitionVar_6
        \end{bmatrix}
        \\
        \begin{bmatrix} % What to do about sigma!?
          a \\
          b \\
          c
          \end{bmatrix} = \begin{bmatrix}
            \TransitionVar_{7a} + \TransitionVar_{9a} \\
            \TransitionVar_{7b} + \TransitionVar_{9b} \\
            \TransitionVar_{7c} + \TransitionVar_{9c} + 1
          \end{bmatrix} \\
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \Connected(\SomethingCSomething{}, \Filter) \land \\
      \Image{}_{\AcaOrBc{}\times\SomethingCSomething{}, \Map}(\Filter, 
      \begin{bmatrix}
        a \\
        b \\
        c
        \end{bmatrix}) \land \\
        a > b
    \end{matrix}
  }
  \infer1[\ExpandM]{\Image{}_{\AcaOrBc{}\times\SomethingCSomething{}, \Map}(\Filter, \begin{bmatrix}
    a \\
    b \\
    c
    \end{bmatrix}) \land a > b}
\end{prooftree}

\subsection{The extended calculus is also correct}

Since the expanded calculus is in practice the addition of two rules with the
purpose of reducing an~$\Image$ predicate containing a product to the form
solved by the previous single-automaton rules of \cref{tbl:rules:single}, we
extend the reasoning from \cref{sec:single:correct} to the single-automaton
rules of \cref{tbl:rules:multi}.

\subsubsection{The calculus terminates}
\begin{lemma}
  The addition of the \ExpandM{} and \Materialise{} rules in \cref{tbl:rules:multi}
  maintains termination as shown in \cref{lma:single-terminates}.
\end{lemma}
\begin{proof}
  Similarly to how we previously showed that the number of possible executions
  of the \Split{} rule is bounded above by the number of transitions of an
  automaton, we can bound the number of applications of \Materialise{} by the
  (monotonically) decreasing number of automata in the product until we have
  approached the starting state for the single-automaton calculus. Trivially, we
  can eagerly apply \Materialise{} repeatedly to do so. Similarly, \ExpandM{} is
  just the generalisation of \Expand{} to multiple automata. In other words there are no possibilities of the calculus diverging.
\end{proof}

\subsubsection{The calculus implemements the homomorphic image of the product}
\begin{lemma}
  Using \Calculus{} to compute $\Image_{\Automaton_1\times\ldots\times\Automaton_k, \Map}(\Filter, \MonoidElement)$ is equivalent to using the single-automaton rules to compute $\Image_{\Automaton', \Map}(\Filter, \MonoidElement)$ where $\Automaton' = \Automaton_1\times\ldots\times\Automaton_k$.
\end{lemma}
\begin{proof}

  We perform this proof equivalent to set inclusion: by proving that neither predicate is stronger than the other, in the sense of rejecting an element that the other accepts and vice versa.

\begin{lemma}\label{lma:multi:rinclude}
  There exists no element $\MonoidElement$ such that $\Image_{\Automaton_1\times\ldots\times\Automaton_k, \Map}(\Filter, \MonoidElement) = \top$ but $\Image_{\Automaton', \Map}(\Filter, \MonoidElement) = \bot$.
\end{lemma}
\begin{proof}
  Assume that such an $\MonoidElement$ exists with the goal of deriving a
  contradiction. Under this assumption, would we be able to use the two new
  rules to close the proof goal with $\top$?

  The \ExpandM{} rule would allow us to close no goal since it only adds
  additional clauses. Our only hope is in using \Materialise{} to get rid of the
  product step by step without introducing an unsatisfiable clause. However, all
  \Materialise{} does is computing the product step by step, eventually arriving
  at $\Automaton'$ modulo permutations. Therefore, it can be no more satisfiable
  than the single-automata predicate.

  \contents{\item Do I need to worry about the fact that I have no extra
  clauses/$\SomeClause$ here? If present, I would need to apply the definition
  of \BindingSum{} since that is what transfers constraints on individual
  automata up and down the materialisation.}
\end{proof}

\begin{lemma}\label{lma:multi:linclude}
  There exists no element $\MonoidElement$ such that $\Image_{\Automaton_1\times\ldots\times\Automaton_k, \Map}(\Filter, \MonoidElement) = \top$ but $\Image_{\Automaton', \Map}(\Filter, \MonoidElement) = \bot$.
\end{lemma}
\begin{proof}
  Assume that such an element exists. Then $\MonoidElement =
  \Map(\WordOf(\Path))$ for some $\Path$ ending up in an accepting state in
  $\Automaton'$, and consistent with $\Filter$ along the transitions of $\Path$.
  Then a corresponding path must exist in each automata of $\Automaton'$,
  although possibly with different labels.

  For \ExpandM{} to introduce unsatisfiability, either of its automata must be
  disconnected (or its corresponding $\Connected{}$ predicate can be removed
  with \Propagate{}), or its flow must be infeasible. However, this contradicts
  the existence of some corresponding path $\Path$ in each automaton. Hence,
  \ExpandM{} cannot be the source of the unsatisfiability.

  The same applies for the second half of \Materialise{}, if the corresponding
  path exists in each individual automaton and in $\Automaton'$, it must also
  exist in every intermittent product on the way to $\Automaton'$, and if $\Map$
  is consistent for intermittent transition labels as it must be, then neither the intermittent $\Image$ predicates nor the final one can be the source of the unsatisfiability.

  This leaves one final potential source of unsatisfiability: the first half of
  \Materialise{}: \BindingSum{}. For that to be unsatisfiable, it requires
  intermittent automata $\Automaton_l, \Automaton_r$ such that adding
  $\BindingSum(\Automaton_l \times \Automaton_r)$ is unsatisfiable. The
  generated equations state that the transitions of $\Automaton_l$ and
  $\Automaton_r$ must be used precisely as often as their corresponding results
  in $\Automaton_l \times \Automaton_r$. For this set of equations to be
  unsatisfiable there must exist no path through $\Automaton_l \times
  \Automaton_r$ that preserves this. However, this is true from the definition
  of the product of automata: since we know the product to be nonempty, it must
  in some sense contain at least one path whose equivalent exists in both
  $\Automaton_l$ and $\Automaton_2$ and cannot use a corresponding transition
  more often, or it would recognise a different langauge. Hence no such case
  exists.
\end{proof}

Since it follows from \cref{lma:multi:linclude,lma:multi:rinclude} that both
variants of the predicate are precisely equally strong, it must mean that they
are equivalent, and we have proven the correctness as defined in
\cref{def:multiple}.
\end{proof}


\section{Extensions}\label{sec:extensions}

This section describes a number of possible extensions to the calculus. Note
that some are partially or fully implemented in \Catra{} already. In particular,
we have some level of support for symbolic transitions over Unicode alphabets to
keep practical automata under a reasonable size, though we do not allow a full Boolean algebra over symbols as described e.g. in~\cite{symbolic-automata}.

\subsection{Backjumping and Learning No-Goods}\label{sec:ext:backjumping}

\Calculus{} can be accelerated for some instances by adding rules for
backjumping. In particular, central connectedness constraints for an automaton
can be learnt. In that case, when discovering that a state $\State$ of an
automaton $\Automaton$ under a certain transition variable $\Filter$ has become
unreachable, we can learn the clause $\Connected(\Automaton, \Filter) \land
\sum_{\Transition \in \SeparatingCut(\Automaton, \State)} \Filter(\Transition) =
0 \implies \sum_{\Transition' = \FromLabelTo{\State}{}{} \in
\Transitions_{\Automaton}} = 0$. A similar rule can be used when a state becomes
backwards-unreachable (but with a corresponding backwards cut). At the moment,
only forward-cut learning is implemented in \Catra, and has provided a slight
improvement in performance on some instances.

The other source of clauses to learn is the \Materialise{} rule. Whenever an
attempt to materialise a product of two ($\Filter$-filtered) automata
$\Automaton_1 \times \Automaton_2$ produces an empty product, we can determine
the cause of the failure with respect to the automata and their respective
transition variables, and learn no-good combinations that can never be part of a
model. In order to be able to do this, we need a few semantic predicates to
record the state of the calculation, as well as a system of disambiguating
automata, since it is possible to arrive at the same automaton by multiple
combinations of decisions and products. In \Catra{}, we use a number of
additional predicates to record the status of the materialisation of products,
to separate instances of our main predicate, and to register the mapping between
automata and their respective transition $\Filter$ terms, described in
\cref{sec:implementation}. However, at this point only rudimentary no-good
learning is implemented.

\subsection{Symbolic Automata}\label{sec:ext:symbolic}

Extending \Calculus{} to support fully symbolic automata is possible within the
framework, depending on your choice of $\Map$. The difficulty consists in
handling the mapping of the homomorphism $\Map$ over symbolic labels, assuming
it maps to finitely many monoid elements. This is not always straightforward, as
seen in the example of \cref{sec:multiple:example}. This complexity is inherent
in computing the full Parikh image, and stems from the fact that we need to
differentiate between the possible interpretations of the $\Sigma$ transitions
without knowing ahead of times which ones will actually be materialied in the
product. If, on the other hand, if our $\Map$ had been length-counting, which
does not differentiate between values, it would have required no adaptation at
all. With a somewhat liberal interpretation of what we are allowed to map to
(e.g. fresh terms), it is possible to use $\Map$ to represent choice operations
like the ones in a range label. In \Catra{}, we frontload this problem by
requiring the user to encode their input as a Parikh automaton. \Fudge{How is
the encoding of Ostrich Plus automata done?}

\subsection{Transducers}\label{sec:ext:transducers}

Another interesting application is applying \Calculus{} to automata with
multiple tracks, e.g. transducers. One application of such a calculus would be
to represent replace operations and other functions on regular languages, and to
be able to answer questions like "does this operation change the length of the
string". The difficulty in implementing it for transducers is first to perform
the mapping on the labels, which for both the length and Parikh cases is
straightforward; just do the same thing in two dimensions. The more complicated
operation is defining the product of transducers. In some cases it would
probably be desirable to perform synchronisation (e.g. requiring overlapping
transitions) on only some transitions, for example the first track.

Implementing such a calculus is straightforward in \Calculus, since the
definition of products was left out of the definition. All that would be needed
is an appropriate update of the definitions of products. Simiarly, \Catra was
written with modularity in mind, and it should be straightforward to extend both
the input grammar and automata implementations to accommodate multi-track
automata with arbitrary synchronisation.

\section{Implementation}\label{sec:implementation}

We implement a calculus for Parikh automata as described in
\cref{sec:parikh-automata}. The artefact submitted along with this paper
is a program that reads one or more products of one or more DFA, the register
incrementations performed along the edges of their transitions, and the labels
of the transitions as ranges of Unicode characters, along with a set of
constraints on the final values of their registers expressed as Presburger
arithmetic. We call this program \Catra.\footnote{If you really must read it
as an acronym, please read it as CAtegory Theory on Register Automata, or if you
object to the somewhat nonstandard use of register automata and category theory,
as Check Assignments of The Registers Afterwards, or alternatively if you find it
all to be too much of a theoretical exercise as Can Anyone Think of A Real
Application.}

\Catra{} is written in Scala, with the calculus described in this paper
implemented as a theory plug-in for the \Princess{} automated theorem
prover~\cite{princess}, which also performs the Presburger reasoning. For
comparison, we also provide an implementation of the baseline method
from~\cite{generate-parikh-image}, a direct translation that uses the~\Nuxmv{}
symbolic model checker~\cite{nuxmv} to solve our constraints, and the
approximation described in~\cite{approximate-parikh} on top of the standard
baseline back-end. An example of an input file corresponding to our running
example introduced in \cref{sec:motivation} can be seen in
\cref{lst:input-example}.

Please note that \Catra{} uses symbolic labels for automata. A symbolic label is
defined as a range of matching single Unicode code points. This allows succinct
representation of many regular expression patterns such as \lstinline{(a-z).*}
which would have otherwise required $27$~nearly identical transitions. In this
instance, the transition would be defined as \lstinline{a -> b [97, 122]}, as the
interval is closed from both edges.

In satisfaction mode, supported by all back-ends, \Catra{} tries to satisfy the
constraints expressed by the input file, reporting satisfiable with register
assignments or unsatisfiable much like traditional~SAT- or SMT solvers would.
Additionally, the baseline and our own back-end also support efficient
generation of the Presburger formula describing the constraints of the input
file using the method described in \cref{sec:finding-the-image}.

\Catra{} can solve equations of Parikh images. An equation like
$\ParikhMap(\Automaton_1 \times \Automaton_2) = \ParikhMap(\Automaton_3 \times
\Automaton_4)$ would be expressed as one \lstinline{synchronised} block per side of
the equation containing their respective terms followed by a
\lstinline{constraint} requiring all their counters to agree (e.g
\lstinline|Aa = Ba && Ab = Bb ...|). Note that the product construction will implicitly require the counters of each term to agree with each other (and the product), so equalities between all counters of all automata are not necessary.

We implement \Calculus{} as a theory plug-in for the \Princess{} theorem prover.
Since \Princess{} does not support multiple-arity predicates like the ones we
use in \Calculus{}, we have implemented variable-length arguments using
additional helper predicates. These are $\Unused{}(\Automaton)$, which marks an
automaton as unused in any product, and $\TransitionMask{}(\Automaton,
\Transition, \Filter(\Transition))$ which associates a transition $\Transition$
and automaton $\Automaton$ with its corresponding transition variable.
Additionally, we associate each of our predicates with an instance variable in
order to differentiate instances of the predicates.

\subsection{Implementing the Baseline}\label{sec:implementing-baseline}

We implement the baseline approach using the same Presburger solver
(\Princess{}), input file parser, and automaton implementation as \Catra. We
do this in order to make the comparison between methods as fair as possible, and
give us the ability to compare the efficiency of the calculus rules themselves.
Using the formula of Equation~\ref{eq:generate-parikh}, we produce quantified
Presburger formulae for each successive term and add them to Princess. In this
fashion we compute the product incrementally term by term, checking
satisfiability in each step. We use a priority queue to select which
term to use for the next step of the product, and order it by the number
of transitions as a heuristic for the size of the automaton. We use this heuristic
to avoid computing large (and therefore slow) products until we have to, banking
our hopes on finding a source of unsatisfiability early. The pseudocode for
our implementation can be seen in Algorithm~\ref{alg:baseline}.

\begin{algorithm}
  \caption{How we implement the baseline approach}\label{alg:baseline}
  \KwData{$\Automaton_1, \ldots, \Automaton_n$ automata, other constraints $\SomeClause$}
  \KwResult{\textsc{Sat} or \textsc{Unsat}}
  \SetKwFunction{NewTheoremProver}{newTheoremProver}
  \SetKwFunction{NewPriorityQueue}{newPriorityQueue}
  \SetKwFunction{Dequeue}{dequeue}
  \SetKwFunction{Enqueue}{enqueue}
  \SetKwFunction{Assert}{assert}

  $p \gets \NewTheoremProver{}$

  \Assert{$p$, $\SomeClause$}

  \ForEach{$\Automaton_i$}{
    \Assert{$p, \ParikhMap(\Automaton_i)$}

    \If{$p$ is \textsc{Unsat}}{break}

  }

  $q \gets \NewPriorityQueue{}$


  \While{$p$ not \textsc{Unsat} and $|q| > 1$}{
    $\Automaton, \Automaton' \gets \Dequeue{q}$ 
    
    \Assert{$p, \ParikhMap(\Automaton \times \Automaton')$}

    \Enqueue{$q, \Automaton \times \Automaton'$}
  }
  
  \KwRet{$p$'s SAT status}

  \end{algorithm}

It should be noted that our automata (including the successive products) are
always by construction forward- and backward- reachable-minimal. We avoid
computing fully minimal automata since it is unclear when an automaton with
registers can be safely minimised. This guarantees that any automaton we produce
only contains states that are both reachable from the initial state and has a
path to an accepting state. We never perform any other minimisation on the
automata for either backend. More complex minimisation was left out since
performing minimisation on automata with counters is non-trivial, and indeed
already minimising symbolic automata is itself risks exponential blowup
\cite{minimising-symbolic}.

\subsection{Heuristics and search strategies}

There are a number of choices left unspecified in \Calculus{} as described in
Sections~\ref{sec:calculus} and~\ref{sec:multiple}. For example, the order of
materialisation of intermediate products and the order of splitting. In
addition, the splitting specified in the general calculus is correct but
inefficient. In this section we describe additional implementation details used
to enhance our solution.

\subsubsection{Splitting, Materialisation, and Propagation}

We order our rule applications so that we first propagate connectedness if
possible, then perform materialisation if tractable (see below!), then finally
resort to splitting if we must.

In addition to the splitting on individual transition variables described in Table~\ref{tbl:rules:single}, where we randomly select a variable whose bounds are not known, we also prefer splitting to sever/include a strongly connected component. We randomly select an automaton where we can compute a cut to at least one strongly connecting component that can be severed from the initial state, e.g. where the strongly connected component does not contain the initial state and where the sum of the transition variables we have associated with the transitions of a minimal cut between the initial state and an arbitrary state in the component is not known to be greater than 0, but also not equal to 0. If there are multiple such strongly connected components we choose one randomly. We then proceed to split on the cut as if it were a regular transition, e.g. its sum being zero or nonzero. In this way we use our splitting strategy to drive the connectedness constraint towards propagation by cutting off loops from the automata.

The implementation of the connectedness constraint is opportunistic and straightforward. We compute a set of dead states by performing forward and backwards reachability computations on an automaton, where we disregard any transition whose associated variable is known to be zero. After that we add clauses ensuring any transition variable associated with a transition starting in a dead state is zero.

Product materialisation, then, is the final piece of the puzzle. In the current implementation, we put off computing intermediate products until at least all but 2 transition variables of one of them have a known status, e.g. either is known to be zero or known to be positive. The number was chosen experimentally, and we observe a consistent trend that to have a low number and therefore a high threshold for materialisation is better throughout the implementation of other features. The other automaton for the product is selected randomly.

\subsubsection{Clause Learning}

\Catra{} enables clause learning by default when using our backend, as it has
been experimentally shown to increase the performance in aggregate (though not
strictly). We do not currently implement all the proposed features of
\cref{sec:ext:backjumping}, but we do implement forward-reachability cut
learning (which had a very modest improvement in performance). No sophisticated
clause learning for products has been implemented.

\subsubsection{Random Restarts}

Finally, we perform restarts scaled by the Luby series~\cite{luby}. Experimental
results have shown this to have a large improvement in performance, which is
unsurprising given how many random choices we make during solving and how
tail-heavy our problem is. It seems to be very easy to get stuck with a poor
choice of product to materialise or transitions to split on.

\section{Evaluation}\label{sec:experiments}

We evaluate the performance of \Catra{} on~\NrBenchmarks{} instances generated
by the \Ostrich{} string constraint solver when solving the \Fudge{pyex
benchmarks}. \Fudge{The instances are all enormous and have some on average 900
products of 400000 automata with 97 counters each and 9999 constraints}. After
generating an initial~\InitialNrBenchmarks{}, we remove~\NrBroken{} misgenerated
instances that did not parse, as well as~\NrTrivial{} instances solved in under
five seconds by the baseline backend. The benchmarks are run on
commit~\texttt{\commit}.

The benchmarks are executed in parallel using GNU Parallel~\cite{parallel},
since they are mostly single-threaded and initial results showed negligible
interference on performance, on \BenchmarkRig{}. We compiled the code using
Scala~\ScalaVersion{}, and executed the experiments on~\JvmVersion{} with a
maximum heap of~\MaxHeapSize{}. We used \Nuxmv{} version~\NuxmvVersion{} invoked
as a subprocess for each instance. Experiments were executed on a fresh JVM per
backend, but the JVM was kept active for each batch of \BatchSize{} instances,
meaning that JIT compilation would make subsequent executions faster, while
still periodically restarting the JVM. We believe this represents a realistic
use case where \Calculus{} is integrated into a string solver or other system
and is called repeatedly. To mitigate systemic impacts of this decision on our
results, experiments were executed in random order for all backends. We run all
benchmarks with a timeout of~\RuntimeTimeout{}.

As a way of gauging the differences in overhead between the native backends and
\Nuxmv, we executed a small experiment where we ran the same trivial instance 30
times and observed the runtimes for all solvers. All backends saw an improvement
from JIT compilation within the first three runs, but subsequent runs only
improved the native backends. The final measured overhead for the fully warm
JVMs was less than~\OverheadSeconds. From this we draw the conclusion that major
differences in runtime between the native backends and \Nuxmv{} is more likely
to stem from innate differences rather than unfair overhead resulting from our
particular configuration. Additionally, one of the advantages of \Calculus{} to
\Nuxmv{} is that the former can me integrated into an automated theorem prover
while the latter has to be called as a separate process. In this sense, our
benchmarking setup can be said to be realistic.

All runtimes are measured in wall-clock time as observed by the JVM when executing the instance, and exclude time spent parsing, which in all observed cases was minimal in comparison to the runtime, usually far below 0.1 second.

It should be noted that despite running in the standard, deterministic
configuration, we observed some degree of nondeterminism in \Nuxmv. Instances
that were solved or unsolved would sometimes be solved or timed out in
subsequent runs. This observation was made even without randomised execution
order, suggesting that the nondeterminism likely stemmed either from our
encoding of the instances into \Nuxmv's format, or the execution environment.
Similarly, \Calculus{} showed similar signs of nondeterminism, but only for
satisfiable instances. To address this, we increased the runtime from our
initial experiments to give us a wider marigin. By contrast, the baseline
implementation showed no signs of nondeterminism. It should be noted that for
both backends exhibiting nondeterminism, the nondeterminism did not change the
general trend of the experiments, even for experiments with few instances.
Therefore, we believe that our large set of benchmarks will protect us from bias
since any backend is unlikely to be neither consistently lucky nor consistently
unlucky.

\subsection{Execution Time and Ability to Solve Instances}\label{sec:runtime}

In Figures~\ref{fig:solve-division} we show how many of the~\NrBenchmarks{} the
respective back-ends could solve and with which status. A full summary of their
outcomes is also available in Table~\ref{tab:solve-status}. We see here that
\Calculus{} generally outperforms \Nuxmv{} on unsatisfiable instances, while
being slightly worse at satisfiable instances. Both \Nuxmv{} and \Calculus{}
massively and strictly outperform the baseline approach on every kind of
instance, but most of all on satisfiable instances. In fact, the baseline
approach was significantly harder to benchmark due to its much higher demand for
RAM. 

\begin{table}[ht]
  \centering
  \input{graphs/solved_pivot_table.tex}
  \caption{The result of running the respective back-ends by instance
  satisifiability (satisifiable or unsatisifiable) with a timeout of
  \RuntimeTimeout. Instances solved by no backend within the timeout are omitted
  from the table. }\label{tab:solve-status}
\end{table}

\begin{figure}[ht]
  \includegraphics[width=0.75\linewidth]{graphs/\commit-by-solver.pdf}
  \caption{The division of statuses per backend.}
  \label{fig:solve-division}
\end{figure}


\begin{figure}[ht]
  \includegraphics[width=0.75\linewidth]{graphs/\commit-time-boxplot.pdf}
  \caption{The distribution of runtimes for solved instances per backend. Note that the number of instances solved differs between backends.}
  \label{fig:runtime-boxplot}
\end{figure}


\begin{figure}[ht]
  \begin{minipage}[b]{0.75\linewidth}
    \centering 
      \includegraphics[width=\textwidth]{graphs/\commit-duels-lazy-baseline-scatter}
      \caption{Runtime duel: \Calculus{} versus baseline.}
      \includegraphics[width=\textwidth]{graphs/\commit-duels-lazy-nuxmv-scatter}
      \caption{Runtime duel: \Calculus{} versus \Nuxmv.}
    \end{minipage}  
  \caption{The pairwise runtime of each instance for instances solved by any backend. Timeouts are reported as a runtime of \RuntimeTimeout.}
  \label{fig:duels}
\end{figure}

\subsubsection{Scalability}\label{sec:scaling}

A cactus plot showing the number of instances solved within a given timeout for each backend can be seen in Figure~\ref{fig:cactus}. \Fudge{We see here that some solver is fast and some other is not}.

\begin{figure}[ht]
  \includegraphics[width=0.75\linewidth]{graphs/\commit-cactus.pdf}
  \caption{The number of instances solved as the time budget increases, simulated from one \RuntimeTimeout-timeout run.}
  \label{fig:cactus}
\end{figure}

\subsection{Finding a Presburger Formula}\label{sec:evaluation:finding-image}

For baseline and \Calculus{}, \Catra{} offers the ability to find the equivalent Presburger formula representing a given instance. For baseline, we use the built-in quantifier elimination facilities of the underlying \Princess{} theorem prover, while for \Catra{} we use the specially tailored approach described in \cref{sec:finding-the-image}. For this experiment, we use only the~\NrKnownSat{} instances known to be satisfiable from the previous experiment detailed in \cref{sec:scaling,sec:runtime}. 

To make sure baseline puts up as much competition as possible, we disable
checking intermittent satisfiability and configure \Catra{} to run in the
maximally eager mode where the product is first computed before any
satisifiability check is performed. We run the experiments with a timeout
of~\ImageTimeout{}. The results of the experiment is summarised in
\cref{fig:cactus:image} and \cref{tab:image-results}. \Fudge{We see here that
something happens}.

\begin{figure}[ht]
  \caption{The number of instances \Catra{} was able to find the Presburger form of the image for within a given number of seconds per backend.}
  \label{fig:cactus:image}
\end{figure}

\subsection{Threats to Validity}

The most obvious threats to validity would be poor benchmarking or poor
implementation, e.g. if the method described in \cref{sec:calculus,sec:multiple}
deviate from what is actually benchmarked in section \cref{sec:experiments}, or
if the methods used for benchmarking would be unsound. The first problem cannot
easily be addressed, except by stating that we have been careful in our
implementation and descriptions alike. The second is more practical. To increase
the probability that our results are indeed representative both in the sense of
representing runs on expected, real world input and in the statistical sense for
a given run of \Catra, despite the fact that we use randomness in our
implementation, we execute many experiments. During the development of \Catra{},
we have also executed experimeints on different hardware and software
configurations on smaller samples of instances to ensure that the general trend
holds. The logs from these executions are distributed along with notes as part
of the artefact in the interest of transparency. Moreover, we have validated
\Fudge{all reported satisfiable instances and their witnesses} with \Nuxmv{} to
ensure that \Calculus{} is indeed sound and does not cheat.

Another threat to validity would be if our implementations of the competition
(baseline and \Nuxmv) would deviate significantly from the expected
implementations. For \Nuxmv{} we use the default configuration which we believe
should be performant (or it should not be the default). Additionally, tweaking our invocation of \Nuxmv{} is explicitly made easy for artefact reviewers.

The most important and most probable threat to validity is our choice of
implementation platform and our choice of automata library, that is our choice
not to use one. Since our automata are more advanced than anything supported by
e.g. the BRICS library, we opted to use our own implementation. There are some
signs that in particular our product computation is inefficient, notably the
good performance of low-threshold automata materialisation that prioritises
computing smaller products. This means that our implementation of \Calculus{}
makes much less heavy use of our product construction library than the baseline
implementation does, instead putting its weight into \Princess{}' theorem
proving abilities. This situation would unfairly advantage \Calculus{} if our
automata library was indeed the bottleneck. We believe this is unlikely since
similar performance issues as the ones in baseline in our string solver was the
instigating reason for this research. Additionally, while we may have a
constant-factor performance impact from bad implementation, it is less likely
for our potentially poor implementation to be asymptotically worse than e.g.
BRICS. However, the performance improvement of \Calculus{} over baseline seen in
\cref{sec:experiments} is generally not constant.

\section{Conclusion}

\contents{
  \item Future extensions: more logics
  \item Loop invariants?
  \item Integration into string solvers?
  \item We have shown fastest
  \item We have shown versatile
  \item We have shown concretely useful
}

%% Acknowledgments
% \begin{acks}                            %% acks environment is optional
%                                         %% contents suppressed with 'anonymous'
%   %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%   %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%   %% acknowledge financial support and will be used by metadata
%   %% extraction tools.
%   This material is based upon work supported by the
%   \grantsponsor{GS100000001}{National Science
%     Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%   No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%   No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%   conclusions or recommendations expressed in this material are those
%   of the author and do not necessarily reflect the views of the
%   National Science Foundation.
% \end{acks}


%% Bibliography
\bibliography{bibliography}


%% Appendix
\appendix
\section{Appendix}

\begin{lstlisting}[caption={An example input file for \Catra{} for the problem introduced in \cref{sec:introduction:motivation}, illustrating every major syntax element. From beginning to end: synchronised (product) automata using the keyword \texttt{synchronised} (automata A and B), labels (except those with ranges), register increments, and constraints on the final values of their counters.}, label=lst:input-example]
  // So far we only support integer counters. Note that we have individual
  // counters for each automaton to avoid surprises for product construction.
  counter int l_a, l_b, l_c, r_c;

  synchronised {
  automaton aca_or_bc {
    init start;
  
    // We use ASCII values here, this is for lowercase a
    start -> sawA [97] { l_a += 1 };
    start -> sawB [98] { l_b += 1 };
  
    sawB -> start  [98] { l_b += 1 };
    sawB -> final [99] { l_c += 1 };
  
    sawA -> sawA [99] { l_c += 1 };
    sawA -> final [97] { l_a += 1 };
  
    accepting final;
  };
  
  automaton something_c_something {
      init start;
  
      // Special short-hand value for the whole Unicode alphabet.
      start -> start [any] ;
      start -> final [99] { r_c += 1 };
  
      final -> final [any];
  
      accepting final;
  };
  };
  
  // Constrain the number of a:s to be larger than the number of c:s. Since the
  // automata are synchronised on every transition and counters are guaranteed
  // to be consistent these constraints are sufficient without also constraining
  // r_c.
  constraint l_a > l_c;
\end{lstlisting}

% Text of appendix \ldots

\end{document}
