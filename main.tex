%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[acmsmall,review,anonymous,screen]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=true}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[acmsmall]{acmart}\settopmatter{}


%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{POPL} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2023}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format must update the '\documentclass' and
%% topmatter commands above; see 'acmart-sigplanproc-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption



\usepackage{amsmath,empheq,fancybox}
\usepackage{paralist}
\usepackage{url}
\usepackage{color}
\usepackage{textcomp,listings}
\usepackage{array}
\usepackage{mymacros}
\usepackage{microtype}
\usepackage{listings}
\usepackage{csquotes}
\usepackage{proof}
\usepackage{cleveref}
\usepackage{algorithm2e}      
\usepackage{multirow}
\usepackage{mathpartir}
\usepackage{amsthm}
\usepackage{numprint}
\usepackage{ebproof}

\usepackage{mathtools} % Bonus


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\lstset{
    columns=fullflexible,
    showspaces=false,
    showtabs=false,
    breaklines=true,
    showstringspaces=false,
    breakatwhitespace=true,
    escapeinside={(*@}{@*)},
    commentstyle=\color{greencomments},
    keywordstyle=\color{bluekeywords},
    stringstyle=\color{redstrings},
    numberstyle=\color{graynumbers},
    basicstyle=\ttfamily\small,
    framesep=12pt,
    xleftmargin=12pt,
    tabsize=4,
    captionpos=b
}


\newif\ifcomments
\commentstrue
\newif\ifoutline
\outlinetrue

\newcommand{\contents}[1]{\ifoutline{\color{blue}
    \begin{itemize}
    #1
    \end{itemize}
  }\fi}

\allowdisplaybreaks[1]


\begin{document}

%% Title information
\title{A Constraint Solving Approach to Parikh Images of Regular Languages}
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}                   %% \country is recommended
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
  A common problem in string constraint solvers is computing the Parikh image, a
   set of linear equations that describe all possible combinations of character
   counts in strings of a given language. Automata-based string solvers
   frequently need to compute the Parikh image of large products (intersections)
   of nondeterministic automata, which in many operations is both prohibitively
   slow and memory-intensive. We contribute a novel understanding of how Parikh
   maps can be tackled as a constraint solving problem to solve real-world
   constraints stemming from functions on regular languages, most notably the
   length constraint. Furthermore, we show how this formulation can be
   efficiently implemented as a calculus, \Calculus{}, in an automated theorem
   prover supporting Presburger logic.  We implement \Calculus{} in a tool
   called \Catra{}, and evaluate it on constraints produced by the
   \OstrichPlus{} string constraint solver when solving \Fudge{standard string
   constraint benchmarks involving string lengths}. We show that our solution
   strictly outperforms the standard eager approach described
   in~\citeauthor{generate-parikh-image} as well as the over-approximating
   method recently described by~\citeauthor{approximate-parikh} by a wide
   marigin, and for realistic timeouts for constraint solving also the~\Nuxmv{}
   model checker.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{parikh images, string solvers, model checking}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}

The Parikh image is a characterisation of formal languages in terms of
their character counts. Given a language over an
alphabet~$\{a_1, \ldots, a_k\}$, the Parikh image is a set of
$k$-dimensional vectors that contains some
vector~$\VectorLiteral{m_1, m_2, \ldots, m_k}$ if and only if the
formal language contains a word in which each $a_i$ occurs $m_i$
times. It is a classical result that the Parikh image of every
context-free language (and, thus, also of every regular language) is a
semilinear set~\cite{parikh-theorem}, i.e., Presburger-definable.

Parikh images play a central role in many automata-based algorithms,
for instance and notably in today's string solvers, which often have
to process constraints that combine regular language membership with
word length. To decide whether a simple formula like
$x \in \Language_1 \wedge y \in \Language_2 \wedge |x| > |y|$,
with string variables~$x, y$ and regular
languages~$\Language_1, \Language_2$, is satisfiable, it is
necessary to reason about the sets of word lengths induced by
$\Language_1, \Language_2$, which is a special case of the Parikh
image.  This required combined reasoning about strings and string
length has long been identified as a major bottleneck in string
solvers
\cite{DBLP:conf/cav/AbdullaACHRRS15,length-aware-solver,approximate-parikh,more}.
Other string solvers make use of Parikh automata~\cite{X}, and thus
Parikh images in the general case, to handle operations including
\verb!str.substr!  and \verb!str.at!, which comes at an even higher
price in terms of computational complexity~\cite{ostrich-plus}.

\iffalse
It appears naturally as part of \Fudge{many operations} in model
checking and solving string constraints in automata-based solvers such
as \Ostrich{} \cite{ostrich}, notably in representing constraints on
string lengths. The enhanced \OstrichPlus{} solver~\cite{ostrich-plus}
makes even more extensive use of Parikh images.
\fi

It is possible to compute an existential Presburger formula describing
the Parikh image of any context-free language in linear
time~\cite{generate-parikh-image}; for the special case of regular
languages, this result was also stated in
\cite{muscholl-linear}. While theoretically elegant, this construction
has several disadvantages, often making it unpractical for integration
into algorithms. Firstly, the constructed Presburger formula contains
a linear number of existential quantifiers in the size of the
considered grammar, as well as complex Boolean structure, which is
needed to express the connectedness of sets of productions considered
in the construction. Eliminating those quantifiers to obtain a
quantifier-free representation of the Parikh image has exponential
complexity~\cite{X}, and is in practice often impossible in reasonable
time. Just solving the Parikh image membership problem is NP-complete,
as it corresponds to computing a satisfying assignment of the
existential Presburger formula, and taxing for solvers as
well~\cite{ostrich-plus}.

\iffalse
Later improvements have produced a construction taking at
most linear time to produce~\cite{muscholl-linear}. However, the
resulting existentially quantified clauses are costly to eliminate as
the number of variables increases, in practice making many real-world
problems intractable.
\fi

Secondly, in applications involving regular languages, it is typically
necessary to consider the Parikh image not only of a single automaton,
but of the intersection of multiple automata. This problem arises in
string solvers in particular, as conjunctions of string constraints
lead to the computation of length images of products (intersections)
of regular languages represented as finite automata. Applying the
approach in~\cite{generate-parikh-image} would in this case require
the eager computation of the product before its length image, and
result in an existential Presburger formula of exponential size (in
the number of automata). In several instances we have observed while
solving real-world string constraints, the computation of the product
of automata exhausts the memory of any machine available due to the
exponential blow-up in size of the product, quickly becoming
intractable as the number of automata in the product increases. The
current best published mitigation for this problem is an
over-approximation that works by approximating the Parikh image of a
product of automata to be the conjunction of the image of the
individual automata of the product \cite{approximate-parikh}. This
approach only works for unsatisfiable instances, and comes with a
harsh penalty for satisfiable instances.

Addressing these concerns, we have developed a calculus for Parikh images of
products of regular languages that we call \Calculus{}. It allows us to
interleave the computation of arbitrarily deep products of automata with the
product's Parikh image, and is generalised to an arbitrary homomorphism over
automata labels, including string lengths. This enables us to let both
calculations inform each other, eliminating unnecessary work, and pruning the
size of the partial products considered in the computation for a smaller memory
footprint. Moreover, the method can be used iteratively to tackle smaller chunks
of the product incrementally, thereby decreasing the memory footprint.

The problem of computing constraints on Parikh images over products of regular
languages under a given commutative homomorphism amounts to solving products of
Parikh automata. Parikh automata are regular automata extended with integer
counters with given increments and decrements for each transition, where we
allow checking a set of linear constraints on the final values of the counters
(but not their intermittent values) \cite{parikh-automata}. Parikh automata
without constraints on the final values on their registers are also sometimes
called cost-enriched automata, weighted automata or counter automata, depending
on exact definitions and side-constraints. The decision problem tackled in this
paper, determining the emptiness of an intersection of Parikh automata, was
recently shown to be PSPACE-complete~\cite{graph-queries}.

We implement \Calculus{} as a theory for the \Princess{} automated theorem
prover in the tool \Catra{}. \Catra also supports using the
approximate method of \cite{approximate-parikh}, its fall-back variant adapted
from~\cite{generate-parikh-image}, and an adapter for the \Nuxmv{} model
checker~\cite{nuxmv}. Using \Catra{}, we compare \Calculus{} to the other two
back-ends on \NrBenchmarks{} Parikh automata satisfaction problems generated
\Fudge{by \OstrichPlus{}}.

In summary, we contribute:
\begin{itemize}
\item The \Calculus{} calculus to efficiently compute (a homomorphism on) the
Parikh image of products of Parikh automata.
\item Experiments illustrating the performance of \Calculus{} on real-world examples from string solving, including \NrBenchmarks{} instances in a standardised format made available for future study.
\item The \Catra{} tool for solving such instances, containing an implementation of \Calculus{}, the over-approximation described in~\cite{approximate-parikh}, and an adapter for the~\Nuxmv{} model checker~\cite{nuxmv}.
\item Suggestions for how to efficiently implement \Calculus{} in a modern automated theorem prover, including strategies for case splitting, clause learning, and constraint propagation for connectedness.
\end{itemize}

\subsection{Related Work}

Parikh image computations as well as Parikh automata \cite{parikh-automata}
feature extensively in string solvers, including as mentioned above \Ostrich{}
and \OstrichPlus{} \cite{ostrich,ostrich-plus}, but also formes the basis of
Trau~\cite{trau-pldi}, and occurs in \textsc{Sloth}~\cite{sloth}. Parikh images
frequently appear when introducing cardinality constraints like length or string
indexing. The state-of-the-art approach to handling Parikh image computation is
to over-approximate the Parikh image of a product of $k$~automata
$\ParikhMap(\Automaton_1 \times \ldots \times \Automaton_k)$ with the
conjunction of the automata's parikh maps, $\AndComp{i=1}^{k}
\ParikhMap(\Automaton_i)$. This approach works only for unsatisfiable instances,
and will require falling back to computing the product of the automata before
using the standard approach for finding its image originally presented in~\cite{generate-parikh-image}.

Outside of string solvers, Parikh automata have been proposed as the basis of
queries~\cite{graph-queries}, and for solving cardinalities in model checking
problems involving epistemic logic~\cite{epistemic-logic}.

Many other generalisations of the Parikh than the ones we use here have been
studied. Prominent examples include generalising the Parikh map to segments of a
fixed length \cite{KARHUMAKI1980155} and the more general Parikh matrix, which
gives more information about a word than the standard Parikh image
\cite{parikh-matrix}. Another notable generalisation is the p-vector, introduced
in~\cite{infinite-words}, which denotes the position of each letter in the word
rather than the number of their occurrences and allows for generalisations into
infinite alphabets. All of these in some sense extend the Parikh map. By
contrast, the main utility of the formulation introduced here is that it allows
us to \emph{remove} something, thereby potentialy obtaining an easier problem.

\section{A demonstration of our approach}\label{sec:motivation}

To illustrate the key features of \Calculus, we will informally present our
reasoning. We begin by considering the following set of string and other
constraints, which we want to solve for all possible values of $\MonoidElement$,
a value in some commutative monoid and the string $s$, where
$\varphi(\MonoidElement)$ is some set of constraints on $\MonoidElement$:
\begin{equation*}
  \AndComp{}{\begin{aligned}
  & \mathtt{/}\SndRegex{}\mathtt{/} \text{ accepts } s\\
  & \mathtt{/}\FstRegex{}\mathtt{/} \text{ accepts } s \\
  & \MonoidElement = f(s) \\
  & \varphi(\MonoidElement)
  \end{aligned}}
 \end{equation*}

We will represent the strings using the nondeterministic finite automata
 $\SomethingCSomething{}$ (\cref{fig:something-c-something}) and $\AcaOrBc{}$
 (\cref{fig:aca-or-bc}) respectively.

 Throughout these examples we will present our three key ideas:
 \emph{automata-aware splitting}, \emph{lazy enforcement of automata
 connectivity}, and \emph{incremental materialisation of products}.
 
\subsection{Counting string lengths}

To keep things simple, in this first example we let $f$ be the function that
gives the length of a given string, e.g. $f\left(\text{curious and curiouser
still}\right) = 27$, and $f\left(\text{homo}\right) +
f\left(\text{morphism}\right) = f\left(\text{homomorphism}\right) = 12$, and we
let $\varphi(\MonoidElement) = \exists k \HoldsThat m = 2k + 1 \land k \geq 0$,
i.e. require that the length of the string is odd.

\begin{figure}[p]
  \centering 
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{aca_or_bc}
    \caption{An automaton recognising the regular expression
      $\FstRegex{}$.}\label{fig:aca-or-bc}
      \Description[An NFA recognising $\FstRegex{}$]{An NFA recognising $\FstRegex{}$. It has 4 states.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth} 
    \includegraphics[width=\textwidth]{something_c_something}
    \caption{An automaton recognising the regular expression
    $\SndRegex{}$.}\label{fig:something-c-something}
    \Description[An NFA recognising $\SndRegex{}$]{An NFA recognising $\FstRegex{}$. It has 2 states.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{aca_or_bc_simplified}
    \caption{$\AcaOrBc{}$ with its symbolic transition counters after
    simplification of their equations.}\label{fig:example-simplify-1}
    \Description[The automaton $\AcaOrBc{}$ labelled with variables counting the number of times each transition is taken, after having simplified the equations.]{The starting state shows an initial flow of 1, with two outgoing transitions to $A$ ($1-l_c'$ on letter a, and $l_b' + l_c'$ on b respectively). $A$ has a flow of $l_c$, and its outgoing transition into the final state, $F$ is the same as the incoming one from $S$, the initial state. The $B$ transition has an outgoing transition labeled $c/ l_c'$ to the final state $F$, and one backwards transition $b / l_b'$ to the starting state.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{something_c_something_length_simplified}
    \caption{$\SomethingCSomething{}$ after using linear reasoning over lengths to replace a variable.}\label{fig:example-length-reduced}
    \Description[The automaton $\SomethingCSomething{}$ after reasoning over lengths.]{The same automaton as before for $\SomethingCSomething{}$, but with the transition from start to final annotated with a constant number 1, and the leftmost self-loop transition replaced with the equation $1 + l_c + 2l_b' -
    r_\Sigma'$.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{aca_or_bc_length_split_1}
      \caption{$\AcaOrBc{}$ at the split where $\FromLabelTo{S}{a}{A}$ is not
      used.}\label{fig:aca-or-bc-length-split-1}
      \Description[The automaton \AcaOrBc{} where we have removed the transition
      from $S$ to $A$.]{ The state $A$ hangs free without incoming transitions,
      and only the lower part of the automaton remains.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{aca_or_bc_length_split_2}
      \caption{$\AcaOrBc{}$ at the split where $\FromLabelTo{S}{a}{A}$ \emph{is} used.}\label{fig:aca-or-bc-length-split-2}
      \Description[$\AcaOrBc{}$ with the transition from $B$ to $F$ removed.]{$\AcaOrBc{}$ when we keep the transition from $S$ to $A$. This removes the transition between $B$ and $F$, but otherwise maintains the automaton.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{length_split_product}
      \caption{The product of $\AcaOrBc$ and $\SomethingCSomething$ after
      case splitting and removal of dead states.}\label{fig:length-split-product}
      \Description[$\AcaOrBc{}$ with the transition from $B$ to $F$ removed.]{$\AcaOrBc{}$ with the transition from $B$ to $F$ removed. There now remains only one path to an accepting state.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{aca_or_bc_simplified_3}
      \caption{$\AcaOrBc{}$ after further constraint propagation.}\label{fig:example-simplify-3}
      \Description[$\AcaOrBc$ without the lower section.]{$\AcaOrBc$ with only its upper section left and the $a$ transitions now set to static 1.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[width=\textwidth]{aca_or_bc_times_something_c_something}
        \caption{$\AcaOrBc{} \times \SomethingCSomething{}$ under the constraint
        $\TransitionCount{a} > \TransitionCount{b}$.}\label{fig:final-example} \Description[The
        final product, still a boring stick automaton with only one loop for an
        arbitrary number of c's.]{A product of the intermittent automata, which
        accepts \lstinline{ac.*ca}}
  \end{subfigure}    
  \end{figure}

The na\"ive approach would be to first compute the product of the automata, then
use the approach from \cite{generate-parikh-image} to find its image under $f$,
and then verify the constraints. However, we can take advantage of both the
constraints and definitions of $f$ to prune the automata before computing the
product and help reduce the blow-up.

Similar to \cite{generate-parikh-image} we associate each transition
$\Transition$ with an integer variable $\TransitionVar_\Transition \geq 0$
representing how many times $\Transition$ is used that we leave implicitly
existentially quantified. These variables are shown as annotations under the
transition labels in \cref{fig:aca-or-bc,fig:something-c-something}. We then
apply graph-like flow reasoning by adding linear equations (e.g. $1 + {l_b}' =
l_a + l_b$, or $l_a + l_c' = 1$) that equate the incoming transitions into a
state to the ones going out. For the initial and accepting states we add an
incoming and outgoing flow of 1, respectively. We will use these linear
equations to perform reasoning to reduce the size of the product before we
compute it. Note that we have already performed some linear reasoning to
eliminate some of the variables in the graphs above, for example to introduce
the constant number $1$ in \cref{fig:something-c-something}, which comes from $1
+ r_\Sigma = x + r_\Sigma$ solved for $x$.

We sum the individual transition variables to get the length $\MonoidElement$ we
are actually interested in. For example the image of $f$ on
$\SomethingCSomething$ is given by $\exists_{r_\Sigma, r_\Sigma'} \: 2k + 1 =
r_\Sigma + 1 + r_\Sigma' \land k \geq 0$. For $\SomethingCSomething{}$, this is
all we need to produce the image since all transitions are always accessible
from any path from the initial state to the accepting state. The difficult
connectivity constraint is not necessary to enforce here, since any path through
$\SomethingCSomething{}$ is connected, and therefore the flow equations
described above are sufficient to capture its image.

The story gets more complicated with $\AcaOrBc{}$, where we have a choice of two
paths, up or down, and where that choice affects the possibility of taking
either of the two loops. In that case, we would have the image $2l_a + l_c + l_c' + l_b + l_b'$.
If we apply standard linear reasoning on the equations (e.g. $1 + {l_b}' = l_a +
l_b$, or $l_a + l_c' = 1$), we can reduce the number of bound
($\exists$-quantified) variables somewhat, obtaining the relations in
\cref{fig:example-simplify-1}, and the corresponding over-approximation on
the $f$-image $2 + l_c + 2l_b'$. It is an over-approximation because it only
considers the number of times each transition is taken and the existence of a
path from an initial to an accepting state, but omits the connectivity of that
path in the presence of loops. For example, no valid path would have both $l_c >
0$ and $1 - l_c' = 0$, but no such restriction exists in the formula above.

Still, this approximate view allows us to perform further reasoning. Since we
know that the length of a string accepted by the intersection of the two
automata's languages must be the same, we also know that their images must be
the same under $f$, e.g. that $r_\Sigma + 1 + r_\Sigma' = 2 + l_c + 2l_b' \iff
r_\Sigma + r_\Sigma' = 1 + l_c + 2l_b'$. This in turn allows us to replace one
additional variable, e.g. $r_\Sigma = 1 + l_c + 2l_b' - r_\Sigma'$ (see
\cref{fig:example-length-reduced}). Note that the two automata are now in
contact, allowing us to propagate reasoning on one to the other.

Since we cannot take a transition a negative number of times, we have an
implicit constraint on any transition variable that it must be at least $0$.
This means that we have $1 + l_c + 2l_b' - r_\Sigma'$, implying an upper bound
on $r_\Sigma'$: $r_\Sigma' \leq 1 + l_c + 2l_b'$.

This is as far as we can get with this reasoning. We must now choose: either we
compute (materialise) the product and continue our reasoning, or we perform a
case split. Let us try a case split in order to put off computing a potentially
large product.

We apply the principle of splitting and select an early transition in the
largest automaton with loops, in our case $\FromLabelTo{S}{a}{A}$ of $\AcaOrBc$.
This creates two cases: $1-l_c' = 0$ and $1-l_c' > 0$. We start with the first
case, and after propagating the now known values and removing any transition
that becomes zero, we get the automaton of \cref{fig:aca-or-bc-length-split-1}.

We immediately notice that the self-loop of state $A$ is now disconnected from
the initial state and must be removed. This condition can be detected
efficiently using standard forwards/backwards reachability from the initial
and accepting states respectively. In other words, $2k+1 = 0$ for this case
and is implied by the splitting constraint. However, no choice of integer $k$
will satisfy this equation. Therefore, we close this branch and proceed with
the other one -- where $1-l_c' > 0$. Note that we do so without ever computing
a potentially expensive product of automata.
  
Plugging in the assumption $1-l_c' > 0$ (and its consequence that $l_c' = 0$),
we also get a reduced variant of $\AcaOrBc$, as seen in
\cref{fig:aca-or-bc-length-split-2}. Note that there is now no way to select any
transition variable to disconnect a loop from a path between $S$ and $F$,
and we can therefore stop worrying about enforcing connectivity at all.

Before continuing, we need to tie the inequalities for the transitions of the
automata of the product to the new transition counting variables we introduced
in the product automaton to ensure that they are consistent. In our case, we
obtain the following equations from the principle that a transition variable in
either automaton must be equal to the sum of variables in the resulting edges of
the product, which gives the following two interesting inequalities from
$\AcaOrBc{}$'s transition $\FromLabelTo{A}{c}{A}$: $2k + 1 = r_1 + r_2 + 1$

Note how this conserves the requirement that the number of loop iterations be
odd even when the same original loop appears three times in the product, twice
as a self-loop and once as a regular state transition. Using the same principle
as before for computing the length by adding up the transition counters, we
obtain the quantified formula $1 + 2k + 1 + 1 + r_b + r_b= 3 + 2k + 2r_b$ for
the length. Eliminating the final quantifier gives us the image of $f(\AcaOrBc
\times \SomethingCSomething)$ modulo odd lengths: $3 + 2k$, for some integer
$k$.

\subsection{Producing the Parikh image}\label{sec:introduction:parikh}

Now assume that rather than the length (e.g. the count of the characters) we
want to find the whole Parikh image of the intersection of $\AcaOrBc{}$ and
$\SomethingCSomething{}$. In other words, now $f$ is a mapping to a 3-vector
counting how often each character occurs in a string. Since extracting values
from a vector is tiresome, we will use the shorthand notation
$\TransitionCount{a}$ to refer to the number of letters a that appear. E.g.
$\TransitionCount{a}(\text{curious and curiouser still}) = 1$. Note that we
still have the property that $\TransitionCount{o}(\text{homomorphism}) =
\TransitionCount{o}(\text{homo}) + \TransitionCount{o}(\text{morphism}) = 2 + 1
= 3$. If you treat the addition as element-wise vector addition, this approach
works for the vector version as well.

For our constraint, we will use $\TransitionCount{a} > \TransitionCount{b}$,
that is finding a count of characters in a string accepted by both automata where
there are more a:s than b:s. Since the underlying calculus is the same, we start immediately from Figure~\ref{fig:example-simplify-1}.

  We now need to relate the number of times a transition was taken to the
  character counts we are really interested in, as opposed to just the length in
  the previous example. For the $\Sigma$ labels, since we do not know which
  letter to use, we introduce a sum $r_{a1} + r_{b1} + r_{c1}$ per $\Sigma$
  transition.

  As in the previous example, we sum the occurrences for each transition to
  obtain the counts, here with the element-wise vector addition unpacked:
  \[
\MonoidElement =   \begin{bmatrix}
  r_{a1} + r_{a2} \\
  r_{b1} + r_{b2} \\
  r_{c1} + r_{c2} + 1 \\
  \end{bmatrix} =
  \begin{bmatrix}
    2 -2l_c' \\
    2l_b' + l_c' \\
    l_c + l_c' \\
    \end{bmatrix} =
    \begin{bmatrix}
      \TransitionCount{a} \\
      \TransitionCount{b} \\
      \TransitionCount{c}
      \end{bmatrix}
  \]
  
These equalities can be combined with the constraint that $\TransitionCount{a} >
\TransitionCount{b}$ to obtain $2 - l_c' > l_b'$. Since we have $1-l_c' \geq 0$
from one of the transition labels, we know $l_c' \geq 0$, and therefore that $1
> l_b' \implies l_b' = 0$. Plugging in the same inequality again on the same
principle, we get $2 - 2l_c' > l_c' \iff l_c' = 0$. This gives us the much
smaller automaton of \cref{fig:example-simplify-3}.

% \begin{figure}[t]
%   \centering 
%     \includegraphics[width=0.70\linewidth]{aca_or_bc_simplified_2}
%     \caption{$\AcaOrBc{}$ after removing the now unused b-transition.}\label{fig:example-simplify-2}
%     \Description[$\AcaOrBc$ without the returning transition from $B$ to $S$]{$\AcaOrBc$ without the returning transition from $B$ to $S$, which now makes the transition from $S$ to $B$ also have the same associated variable $l_c'$.}
%   \end{figure}

This will produce a straightforward product since both automata now have the
same shape, where we essentially only need to expand the self-loops of
$\SomethingCSomething$ and reduce its range labels to only capture a's, as seen
in \cref{fig:final-example}. Note that we got this result by only performing
linear inequality reasoning on the automata of the product versus the number of
times each transition would be used.

%   \begin{figure}[t]
% \centering
%     \includegraphics[width=0.75\linewidth]{original_product}
%     \caption{$\AcaOrBc{} \times \SomethingCSomething{}$ as it would have
%     appeared if we had computed it from the initial
%     automata.}\label{fig:original-product}
%   \end{figure}

To obtain the final Parikh image, we again need to tie the inequalities for the
transitions of the automata of the product to the new transition counters we
introduced in the product automaton. The only interesting transition is the one
carrying the c, so we will choose that one for illustration. In that case, the
bridging equation is $r_2 = l_c \land r_2 = r_{\Sigma}$ (and for all others
$=1$), which gives the Parikh image $\exists_{r_2} \TransitionCount{a} = 1 \land
\TransitionCount{b} = 0 \land \TransitionCount{c} = r_2 + 1$, and after
existence-elimination using reasoning on lower bounds (e.g. $r_2 \geq 0$) we
arrive at $\TransitionCount{c} \geq 1$.

\section{Preliminaries}

\subsection{Monoids}

A monoid $\Monoid = \Tuple{X;\MonoidOp;0_{\Monoid}}$ is an algebraic structure
consisting of the carrier set of elements, $X$, an associative binary operation
$X \times X \rightarrow X$ denoted as $\MonoidOp$, that is where for all $a, b,
c \in X$, $(a \MonoidOp b) \MonoidOp c = a \MonoidOp (b \MonoidOp c)$. Finally,
$\Monoid$ must have an identity element $0_{\Monoid} \in X$ such that
$0_{\Monoid} \MonoidOp a = a \MonoidOp 0_{\Monoid} =   a$ for every $a \in X$.
We sometimes use integer multiplication to represent repeated application of
$\MonoidOp$, e.g. $3a = a \MonoidOp a \MonoidOp a$, for $a \in X$. $\Monoid$ is
called \textit{commutative} if $\MonoidOp$ also commutes, that is if $a
\MonoidOp b = b \MonoidOp a$ for all $a, b \in X$. 

Finally, we sometimes refer to a structure-preserving map between two monoids
$\Monoid_1, \Monoid_2$ as a \textit{homomorphism} between $\Monoid_1$ and
$\Monoid_2$, that is a map $\Map : S_1 \rightarrow S_2$ such that $\Map(a
\MonoidOp_1 b) = \Map(a) \MonoidOp_2 \Map(b)$, where $S_1, S_2$ are the carrier
sets of $\Monoid_1$ and $\Monoid_2$ respectively, and $\MonoidOp_1, \MonoidOp_2$
their binary operations.

\subsection{Languages, Finite-state Automata and their Products}

We define an alphabet as a set of symbols $\Alphabet$ with words $\Strings$, and
the concatenation operation as $s_1 \Concat{} s_2$ over two strings $s_1, s_2$.
Note that $\Strings = \Tuple{\Alphabet;\Concat{};\epsilon}$, is a
non-commutative monoid, referred to as the free monoid. The string length
function, $\Length{s}$ is an example of a homomorphism between $\Strings$
and~$\mathbb{Z}$.

A non-deterministic automaton~$\Automaton$ with alphabet~$\Alphabet$ is
$\AutomatonTuple$ where $\Transitions = \States \times \Alphabet \times
\States$, $\States$ is its states, $\InitialState$ is w.l.o.g. assumed to be the
single initial state, and $\AcceptingStates$ is its set of accepting states.  We
write a transition $\Transition = \Tuple{\State, \Label, \State'} \in
\Transitions$ as $\Transition = \FromLabelTo{\State}{\Label}{\State'}$.
Similarly, we use the notation $\FromLabelTo{\State}{}{}$ to refer to the set of
transitions starting in $\State$, and $\FromLabelTo{}{}{\State}$ to refer to the
set of transitions coming into $\State$, whenever the automaton is clear from
the context.

We will let variables $\Transition, \Transition', \Transition_1, \ldots,
\Transition_n$ etc describe transitions, $\State, \ldots, \State_n$ states, and
$\Automaton, \ldots, \Automaton_n$ automata, and use subscript indexing
($\Transitions_\Automaton$) to refer to the transitions, states, etc of a given
automaton.

By a \emph{product} of two automata $\Automaton_1, \Automaton_2$, written
$\Automaton_1 \times \Automaton_2$, we mean an automaton constructed to run
$\Automaton_1$ and $\Automaton_2$ in parallel on an input and only accept the
input if both automata would do so.

We refer to the resulting product states as tuples, $\Tuple{\State, \State'}$,
which represent the state of the product automaton where $\Automaton_1$ would be
in $\State$ and $\Automaton_2$ would be in $\State'$. Note that since we use
ordered tuples the product is technically (but w.l.o.g) not commutative; the
left-hand-side must come from the left-hand term. The sole purpose of this
matching is to allow us to speak with precision about the origins of components
in a product.

\subsection{The Parikh Map and its Image}
Formally, the \textit{Parikh map} over an alphabet $\Alphabet=
\left\{a_1, \ldots, a_k \right\}$ is defined as in \cite{kozen}:
$$
\begin{aligned}
& \ParikhMap: \MapFromTo{\Strings}{\natural^k} \\
& \ParikhMap(s) = \VectorLiteral{\#a_1(s), \#a_2(s), \ldots, \#a_k(s)}
\end{aligned}
$$

That is, $\ParikhMap(s)$ is a vector of the number of occurrences of each
character in the language for a given string $s$. For example, for  $\Alphabet =
\Set{a, b}$, we would have $\ParikhMap(abb) = \VectorLiteral{1, 2}$.

We define the image of this map, the \textit{Parikh image}, of some subset of
the language $\Language \subseteq \Strings$ as:
\[
\ParikhMap(\Language) = \Set{\ParikhMap(x) \SuchThat x \in \Language}
\]

Thus, we would have $\ParikhMap(\left\{ab, abb\right\}) = \left\{\left[1,
1\right], \left[1, 2\right]\right\}$. We also sometimes use the standard
notation $\#l(w)$ to talk about an individual letter $l$ in a word $w$. For
example, for the Parikh vector above, we would have $\CountOf{a} = 1$. You have
already seen this in \cref{sec:introduction:parikh}.

Parikh's theorem states that any context-free language has a letter-equivalent
regular language (c.f.~\cite{construction} for a construction of such automata
from context-free grammars and~\cite{bounds} for bounds on its size). However,
there are languages that are not context-free that also have semilinear images
under~$\ParikhMap$ (e.g. $\ParikhMap(\Set{a^nb^nc^n \SuchThat n \geq 0}) =
\ParikhMap((abc)^*) = \CountOf{a} = \CountOf{b} = \CountOf{c} \land \CountOf{a}
\geq 0$). This means they can be represented as a quantifier-free Presburger
formula.

Note that while Parikh's theorem applies to context-free languages, in this
paper we focus only on regular languages.

\subsection{The Parikh image of a regular language expressed in Presburger arithmetic}
\label{sec:verma}

Since Parikh images are semilinear, any Parikh image can be written as a set of linear
equations. The following construction, here adapted to work on an NFA $\Automaton = \AutomatonTuple$, was presented in \cite{generate-parikh-image}:
\begin{equation}
\begin{aligned}
\ParikhMap(\Automaton) := 
& \AndComp{\Letter \in \Alphabet}{\LetterVar_{\Letter} = \sum_{\Transition = \FromLabelTo{}{\Letter}{} \in \Transitions} \TransitionVar_{\Transition}
}\\
& \AndComp{\State \in \AcceptingStates}{\FinalStateVar_{\State} > 0 \longrightarrow  \StateVar_{\State} > 0}\\
& \AndComp{\State \in \States}{
  \left(\FinalStateVar_{\State} \text{ if } \State \in \AcceptingStates \text{ otherwise } 0 \right) +
  \sum_{\Transition = \FromLabelTo{\State}{}{}} \Filter(\Transition) - \sum_{\Transition = \FromLabelTo{}{}{\State}} \Filter(\Transition)
= \begin{cases}
    1 \text{  if $\State = \InitialState$} \\
    0 \text{ otherwise}
  \end{cases}
}\\
& \AndComp{\Transition = \FromLabelTo{\State'}{}{\State} \in \Transitions}{
  \TransitionVar_{\Transition} > 0 \longrightarrow \StateVar_{\State} > 0
} \\
& \AndComp{\State \in F}{
  \StateVar_{\State} > 0 \longrightarrow \FinalStateVar_{\State} = 1
} \\
&\AndComp{\State \in \States}{
  \StateVar_{\State} > 0 \longrightarrow
  \left(\FinalStateVar_{\State} = 1 \text{ if } \State \in \AcceptingStates \land \right) \OrComp{\Transition = \FromLabelTo{\State}{}{\State'} \in \Transitions}{
    \StateVar_{\State} = \StateVar_{\State'} + 1 \land 
    \TransitionVar_{\Transition} \geq 1 \land
  \StateVar_{\State'} \geq 1
    }
}
\end{aligned}
\label{eq:generate-parikh}
\end{equation}

All variables $\TransitionVar_i, \StateVar_i, \FinalStateVar_i$ are
existentially quantified and the free variables $\LetterVar_\Letter$ make up the
image. $\StateVar_\State$ represents the distance of state $\State$ from
$\InitialState$ in a spanning tree, $\TransitionVar_\Transition$ how many times
a transition $\Transition$ is used, and $\FinalStateVar_\State$ whether a given
accepting state $\State \in \AcceptingStates$ is the actually used final state.

In this paper we refer to this model as the baseline approach, though we also
apply optimisations as described in \cref{sec:implementing-baseline}. The
calculus introduced in this paper, by contrast, lazily enforces the
connectedness constraint of this encoding (the final three clauses) while also
interleaving the computation of products of automata and propagating information
between the steps to reduce the amount of work that needs to be done.
  
\section{Generalised Parikh Maps}\label{sec:generalised}

It is easy to see that the Parikh map~$\ParikhMap$ represents a
homomorphism from the (free) non-commutative monoid~$\Strings$ to the
(free) commutative monoid~$\Naturals^k$. As we are often not
interested in the full Parikh image, but only in some projections of
it, for the purposes of this paper we use a generalised version of the
Parikh map. We consider \emph{arbitrary} homomorphisms
$\Map:\: \Sigma^* \to \Monoid$, where
$\Monoid = \left(X;\MonoidOp;0_{\Monoid}\right)$ is a commutative
monoid; this means that $\Map(\epsilon) = 0_{\Monoid}$ and
$\Map(u \Concat w) = \Map(u) \MonoidOp \Map(w)$. We give several
examples of such generalised Parikh image later in this section.

Observe that every homomorphism~$\Map:\: \Sigma^* \to \Monoid$ can be
represented as the composition~$h' \circ \ParikhMap$, for some
homomorphism~$h' : \Naturals^k \to M$. One of the insights underlying our
approach is that it is more efficient to directly compute a
generalised Parikh image~$\Map(\Language)$, than to first compute the
standard image~$\ParikhMap(\Language)$ followed by projection to
some property of interest.

%\subsection{Applications of generalised Parikh images}\label{sec:applications}

\subsection{Example~1: Reasoning about String Length}

A useful example of such a simplifying morphism can express string
length, the problem that originally motivated our study of the Parikh
map. This mapping is relevant when solving constraints that combine
language membership as well as word length, for instance the formula
\begin{equation}\label{eq:stringLength}
x \in \Language_1 \wedge y \in \Language_2 \wedge |x| > |y|
\end{equation}
mentioned in the introduction. To solve this formula, let
$\Monoid = \Naturals$, and define the homomorphism~$L$ by $L(a) = 1$
for all characters~$a \in \Alphabet$. The length of a string
$s = s_1 \RepeatSum{\Concat} s_n$ is given by
$L(s) = \sum L(s_i) = 1 \RepeatSum{+} 1 = n$, and to solve
\eqref{eq:stringLength} we can instead solve the equi-satisfiable
formula~$\alpha \in L(\Language_1) \wedge \beta \in L(\Language_2)
\wedge \alpha > \beta$. This paper proposes efficient native
procedures to reason about membership constraints
like~$\alpha \in L(\Language_1)$, avoiding the computation of
the complete image~$L(\Language_1)$.

\subsection{Example~2: Generalised String Constraints with Integer
  Datatype}\label{sec:parikh-automata}

Parikh images are also applicable to decide more general classes of
string constraints~\cite{ostrich-plus}. Consider, for instance, a
constraint involving the substring operation:
\begin{equation}
  \label{eq:substring}
  x \in \Language_1 \wedge \; 0 \leq n \leq m \leq |x| \wedge x[n:m] \in \Language_2
\end{equation}
in which $x$ is a string variable, $n, m$ are integer variables, and
$x[n:m]$ denotes the substring of $x$ starting at position~$n$ running
until position~$m$. This constraint belongs to an expressive fragment
of string logic that cannot be decided by most state-of-the-art string
solvers. To illustrate the decision procedure proposed in
\cite{ostrich-plus} for string constraints of this kind, suppose that
$\Language_2$ is defined by the regular
expression~$\mathtt{(ab|c)*}$. An automaton recognising this language
is shown in Fig.~\ref{fig:parikh-automata} (left).

We can model \eqref{eq:substring} using the notion of a \emph{Parikh
  automaton}~\cite{parikh-automata,expressiveness}. A Parikh automaton
is an automaton in which transitions are labelled both with characters
from the alphabet~$\Alphabet$, and with offset vectors defining the
increments of a finite number of counters. This means that Parikh
automata recognise words over an extended
alphabet~$\Alphabet \times D$, where $D \subseteq \Naturals^d$ is a
finite set of increment vectors (notation as
in~\cite{expressiveness}). We use the symbols~$\pi_\Alphabet, \pi_D$
to denote projections to the first and the second component of a
composite letter~$(a, \Vector{v})$, respectively, and extend those
projections to words:
\begin{equation*}
  \pi_\Alphabet((a_1, \Vector{v}_1) \RepeatSum{\Concat} (a_k, \Vector{v}_k))
  ~=~ a_1 \RepeatSum{\Concat} a_k,
  \qquad
  \pi_D((a_1, \Vector{v}_1) \RepeatSum{\Concat} (a_k, \Vector{v}_k))
  ~=~ \Vector{v}_1 \RepeatSum{+} \Vector{v}_k.
\end{equation*}

\begin{definition}\label{def:parikh-automata} A Parikh automaton of dimension $d
  \geq 0$ is a pair $\Tuple{\Automaton, C}$, where
  $C \subseteq \Naturals^d$ is a semi-linear set (or, equivalently, a
  Presburger formula), and $\Automaton$ is a finite automaton with the
  alphabet $\Alphabet \times D$, where $D \subseteq \Naturals^d$. We
  say that $\Tuple{\Automaton, \varphi}$ recognises a
  word~$w \in \Alphabet^*$ if and only if the automaton has a run
  accepting an extended word~$w' \in (\Alphabet \times D)^*$ such that
  $\pi_\Alphabet(w') = w$ and $\pi_D(w') \in C$.
\end{definition}

    
\begin{figure}[t]
  \centering
  \raisebox{5ex}{
    \includegraphics[scale=0.9]{counter_automaton_pre}
    }
  \hfill
  \includegraphics[scale=0.9]{counter_automaton}
  \caption{An automaton recognising the language $\mathtt{(ab|c)*}$
    (left), and a Parikh automaton describing the pre-image of this
    language under the substring operation~$\cdot[n:m]$ (right). For
    sake of presentation, the Parikh automaton contains
    $\epsilon$-transitions, which could be eliminated in the standard
    way.}\label{fig:parikh-automata}
    \Description[An 2-state automaton (left) and a slightly larger automaton with counters (right).]{
      The rightmost automaton is the starts with an initial state with a self-loop on any character that increments both registers by one, followed by an epsilon transition to a state with a self-loop on c that increments the lower register by one. From that state, the automaton has an epsilon-transition into the accepting state, as well as a transition on a into a state with only a transition back again, both incrementing the lower register by one. The accepting state has a self-loop on any character that does nothing to the registers.
      }
    \end{figure}

    Applied to \eqref{eq:substring}, the decision procedure in
\cite{ostrich-plus} will construct a pre-image of $\Language_2$ under
the substring operation~$\cdot[n:m]$, and check whether this pre-image
is consistent with the constraint~$x \in \Language_1$. Because the
substring operation depends on the values of the integer
variables~$n, m$, a Parikh automaton of dimension~$2$ is a suitable
formalism to describe the pre-image, resulting
in the automaton in Fig.~\ref{fig:parikh-automata} (right). The first
component of the increment vectors is used to count the number of
letters eliminated in the beginning of the string (value~$n$), while
the second component records the beginning of the eliminated suffix
(value~$m$).

Denoting the language described by Fig.~\ref{fig:parikh-automata}
(right) as $\Language_{pre}$, we can then replace \eqref{eq:substring}
with an equi-satisfiable formula that no longer contains any explicit
substring operation:
%
\begin{equation}
  \label{eq:parikh-constraint}
  p \in \Language_{pre} \wedge~
  \pi_\Alphabet(p) \in \Language_1
  \wedge~~ \pi_D(p) =
  \begin{bmatrix}
    n \\ m
  \end{bmatrix}~~
  \wedge 0 \leq n \leq m \leq |p|
\end{equation}
%
To check the satisfiability of \eqref{eq:parikh-constraint}, we need a
decision procedure that can process intersections of regular languages
(in this case, of $\Language_{pre}$ and $\Language_1$, synchronising
on $\Alphabet$), while imposing the side
condition~$0 \leq n \leq m \leq |p|$ on the increment sum. In
\cite{ostrich-plus}, this decision procedure turned out to be main
bottleneck of the string solver, which was one of the motivations to
develop the lazy algorithm proposed in this paper.

\iffalse
\begin{figure}[t]
  \centering
      \includegraphics[width=0.5\linewidth]{parikh_automaton}
      \caption{The automaton part of a Parikh automaton for $\AcaOrBc{}$ with
      $\Alphabet = \Set{\text{a, b, c}}, d = 3$. The semilinear set/Presburger
      formula containing the constraints on the final register values cannot be
      visualised.}\label{fig:parikh-automaton}
    \end{figure}
\fi

\section{A Calculus for Generalised Parikh Images}\label{sec:calculus}

We start by defining our calculus, \Calculus{}, for one automaton, and only
extend it to products of automata in \cref{sec:multiple}. Assume an NFA
$\Automaton = \AutomatonTuple$ with $\NrTransitions$ transitions $\Transitions =
\Set{\EllipsisSequence{\Transition}{\NrTransitions}}$. For convenience, we then
introduce the following supporting notations:

\begin{definition}
  A \textit{path} $\Path = \PathEnumeration$ of an automaton~$\Automaton$ with
  transitions $\Transitions$ represents a path through $\Automaton$ using
  transitions in $\Transitions$ (i.e. $\Transitions(\State_k, \Label_{k+1},
  \State_{k+1})$ holds), passing zero or more labels $\Label_1, \ldots,
  \Label_n$. The path must begin in the initial state, i.e.~$\State_0 =
  \InitialState$. The end state, $\State_n$, is not necessarily accepting. If it
  is, we say the path is also \textit{accepting}.
  \end{definition}

\begin{definition}
  Moreover, we talk about the \textit{set of paths} of an automaton,
  $\Paths(\Automaton)$, a possibly infinite (if $\Automaton$ has loops) set of
  valid paths through $\Automaton$. Additionally, we use the
  notation~$\Paths(\Automaton, \State)$ to mean all paths ending in
  state~$\State$.
\end{definition}

\begin{definition}
  For a path $\Path = \Tuple{\State_0 \Label_1 \State_1 \RepeatSum{,}
  \State_n}$, its \textit{word} $\WordOf(\Path) = \Label_1 \RepeatSum{\Concat}
  \Label_n$ is the word read out on its labels.
\end{definition}

\begin{definition}
  The \textit{states} of a path $\Path = \Tuple{\State_0 \Label_1 \State_1
  \RepeatSum{,} \State_n}$, $\StatesOf(\Path) = \Set{\State_0\RepeatSum{,}
  \State_n}$ are the states visited along $\Path$. Note that $\InitialState \in
  \StatesOf(\Path)$ for every path since all paths start in the initial state.
\end{definition}

\begin{definition}
  A \textit{cut}, $C$ of an automaton~$\Automaton = \AutomatonTuple$ to state
  $\State$, written $\SeparatingCut(C, \Automaton, \State)$, is a minimal set of
  transitions such that every accepting path $p$ where $\State \in
  \StatesOf(\Path)$ contains a transition $\Transition \in C$. A cut does not
  exist for every state; notably it never exists for $\InitialState$.
\end{definition}

\begin{definition}
 The \textit{transition count}, $\TransitionCount(\Transition, \Path)$ is the
 number of times a transition $\Transition =
 \FromLabelTo{\State_1}{\Label}{\State_2} \in \Transitions$ appears in a path
 $p$.
\end{definition}

We then introduce the two predicates into our calculus with the following
definitions:

\begin{definition}\label{def:single-image}
  The Parikh predicate, $\SinglePredicateInstance$, for some automaton
  $\Automaton = \AutomatonTuple$, modulo some map $\Map$ to a
  commutative monoid $\Monoid$ as described in \cref{sec:generalised}
  and with a transition selection function
  $\Filter:\: \Transitions \to \Naturals$ holds when
  $\MonoidElement \in \Monoid$ is the Parikh image of $\Automaton$
  modulo $\Map$, or more formally when there is an accepting path
  $\Path = \PathEnumeration \in \Accepting{\Paths(\Automaton)}$ such
  that $\Filter(\Transition) = \TransitionCount(\Transition, \Path)$
  for all $t \in \Transitions$, and $\MonoidElement = \Map(\WordOf(\Path))$.
\end{definition}

\begin{definition}
  $\Connected(\Automaton, \Filter)$ for some automaton $\Automaton =
  \AutomatonTuple$ holds when for every $\Transition =
  \FromLabelTo{\State}{}{\State'} \in \Transitions$, $\Filter(\Transition) > 0
  \implies \exists \Path \in \Paths(\Automaton)$ $\Filter(\Transition_i) > 0$
  for $\Transition_i \in p$, and $\State \in \StatesOf(p)$, or in words that
  there exists some $\Filter$-selected valid path that reaches $\Transition$'s
  starting state, $\State$. Intuitively, it represents the condition that
  $\Automaton$ is connected with respect to the selection function $\Filter$ for
  every transition. It is redundant to $\Image$ by design.
\end{definition}

We present the rules of \Calculus{} for one automaton in
\cref{tbl:rules:single}. Note that we operate on sets of
formulas. Additionally, we also use the convention of splitting the
formulas into linear equations and inequalities ($\SomeInequalities$)
and other formulas ($\SomeClause$). \Fudge{need to say that we assume
  that the predicates (Im, etc.)  only occur positively.}

Note that the filtering function~$\Filter$ is evaluated symbolically in these
rules, and can in practice be read as a function from transitions to
$\Naturals$-valued terms (e.g.~\texttt{t} or~\texttt{t+1}). In our implementation \Catra{},
described in \cref{sec:implementation}, $\Filter$~is a vector of fresh variables
with the same size as~$\Transitions$.

We use the shorthand notation~$\Transitions_\Automaton$ to refer to the
transitions of an automaton~$\Automaton$. Additionally, for an
automaton~$\Automaton = \AutomatonTuple$ we allow mapping the selection function
like so: $\Filter(\Automaton) = \Tuple{\States, \InitialState, \AcceptingStates,
\Set{\Transition \in \Transitions \SuchThat \Filter(\Transition) > 0}}$, i.e.
$\Automaton$~with only the transitions for which~$\Filter$ is positive. In this
instance, the basis for the matched linear inequalities is
implicitly~$\SomeInequalities$. Similarly, for our commutative monoid $\Monoid =
\left(X;\MonoidOp;0_{\Monoid}\right)$ and the map into it $\Map : \Strings
\rightarrow X$, we also allow mapping over transitions:
$\Map(\FromLabelTo{\State}{\Label}{\State'}) = \Map(\Label)$.

Each of the rules of \cref{tbl:rules:single} should be read and
applied bottom-up, and relates
premises~$\SomeClause_1, \ldots, \SomeClause_k$ with some
conclusion~$\SomeClause$. When constructing a proof, we start from some
root~$\SomeClause$, and then apply proof rules to the goals of the proof
until a goal can be closed, or no more rule is applicable. Since
our rules operate by adding and matching linear (in)equalities in a proof goal,
we use the shorthand of listing the matched inequalities as antecedents. An
example of this can be seen in the~\Propagate{} rule. \Fudge{We pull solutions
out of a hat? I have no idea.}

We require that rules can only be applied when they add new formulas
on every created branch (the notion of \emph{regularity} of a proof is
required~\cite{Fitting96a}). For example, this means that \Split{} can
only be applied to proof goals that contain neither
$\Filter(\Transition) = 0$ nor $\Filter(\Transition) > 0$, and can
never be applied to split on the same term twice on the same branch.
This suggests a proof strategy where you \Propagate{} when you can, \Split{}
when you must, and \Subsume{} when neither is possible anymore.

\begin{table}
\begin{tabular}{@{}l>{$}c<{$}p{3cm}@{}}\toprule
  Name & \text{Rule} & Side conditions\\
  \midrule

  % EXPAND
  \Expand & 
    \inferrule
  {\Connected(\Automaton, \Filter) \land \FlowEq(\Automaton, \Filter) \land \MonoidElement = \sum_{\Transition \in \Transitions_\Automaton} \Filter(\Transition) \cdot \Map(\Transition), \SomeInequalities, \SomeClause}
  {\SinglePredicateInstance, \SomeInequalities, \SomeClause} & 
  None \\[4ex]

  % SPLIT
  \Split & 
  \inferrule{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause, \Filter(\Transition) = 0 \mid \Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause, \Filter(\Transition) > 0}{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause} &
  if $\Transition \in \Transitions_\Automaton$ \\[4ex]

  % PROPAGATE
  \Propagate &
  \inferrule{\Connected(\Automaton, \Filter), \Set{\Filter(\Transition') = 0 \SuchThat \Transition' \in C}, \SomeInequalities, \SomeClause, \Filter(\Transition) = 0}{\Connected(\Automaton, \Filter), \Set{\Filter(\Transition') = 0 \SuchThat \Transition' \in C}, \SomeInequalities, \SomeClause} &
  if $t = \FromLabelTo{\State}{}{\State'} \in \Transitions_\Automaton, \SeparatingCut(C, \Automaton, \State)$ \\[4ex]

  % SUBSUME
  \Subsume &
  \inferrule{\SomeInequalities,\SomeClause}{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause} &
  \Split{} and \Propagate{} cannot be applied \\
  \bottomrule
  \end{tabular}
  \caption{Derivation rules for one automaton.}\label{tbl:rules:single}
\end{table}
We use the symbolic function $\FlowEq(\Automaton, \Filter)$ that generates a set
of existentially quantified linear inequalities with the following definition,
where we assign fresh, existentially quantified variables to
$\FinalStateVar_\State, \Filter(\Transition)$ for every $\State \in
\AcceptingStates, \Transition \in \Transitions$:
\[
\begin{aligned}
  & \FlowEq(\Automaton, \Filter) = \sum\limits_{\State \in \AcceptingStates} \FinalStateVar_\State = 1 \land \AndComp{\State \in \States}{\In(\State, \Filter) - \Out(\State, \Filter)} = \Sink(\State) \land
  \AndComp{\State \in \AcceptingStates}{\FinalStateVar_\State \geq 0}\\
  & \Sink(\State) = 0 \text{ if } \State \not\in \AcceptingStates, \FinalStateVar_\State \text{ otherwise.} \\
  & \In(\State, \Filter) = \StartFlow(\State) + \sum_{\Transition \in \FromLabelTo{\State'}{}{\State}} \Filter(\Transition)\\
  & \StartFlow(\State)  = 1 \text{ if } \State = \InitialState, \text{ otherwise $0$.} \\
  & \Out(\State, \Filter) = \sum_{\Transition \in \FromLabelTo{\State}{}{\State'}} \Filter(\Transition)
\end{aligned}
\]

In addition to \cref{tbl:rules:single}, we assume the existence of a rule
\PresburgerClose{}, corresponding to a sound and complete solver for Presburger
formulae, and for the elements of~$\Monoid$.

The $\Propagate{}$ rule allows us to propagate (dis-)connectedness across
$\Automaton$. It states that we are only allowed to use transitions attached to
a reachable state, and is necessary to ensure connectedness in the presence of
cycles in~$\Automaton$.

\textsc{Expand} expands the predicate into its most basic rules; one set of
linear equations synchronising the transitions mentioned by~$\Filter$ to the
corresponding Monoid element~$\MonoidElement$, and the linear flow equations of
the standard Parikh image formulation, as described by~\FlowEq. Since
$\Connected$ and $\Image$ are partially redundant and the difference is covered
by~$\FlowEq$, we can remove the instance of~$\Image$ when applying~$\Expand$. In
this sense, we split the semantics of the $\Image$~predicate into its counting
aspect (covered by $\FlowEq$) and its connectedness aspect (covered by
$\Connected$).

Finally, $\Split{}$ allows us to branch the proof tree by trying to exclude a
contested transition from a potential solution before concluding that it must be
included. Intuitively, this is what guarantees our ability to make forward
progress by eliminating paths through~$\Automaton$.

A decision procedure for our predicate in a tableau-based automated theorem
prover would start by expanding the predicate using the $\Expand{}$~rule. A
theorem prover would perform algebraic substitution on the underlying constants
of~$\Filter$, boiling them down to choices of branches, which depend on one
single variable, and loop transitions. This logic corresponds to the placement
of counters for optimally edge-profiling the CFG of a program, making up a
minimum-spanning tree of the automaton~\cite{path-profiling}.

In order to make the examples below tractable, we will assume the existence of a
rule \EquationReasoning{} that allows us to perform standard algebraic reasoning
on linear inequalities. This rule is not necessary for correctness or
completeness, but shortens the examples considerably.

\subsection{An Example}

Starting with~$\AcaOrBc{}$, where $\Map$ is the length function, in effect
$\Transition \mapsto 1$ for transitions, and the constraints that the length is
odd using the same trick as in \cref{sec:motivation}, we have the
definitions in \cref{fig:example:single:equivalences} (omitting existential
quantifiers and $x \geq 0$ for every variable to avoid clutter).

\begin{figure}[ht]
  We define $\Filter$ and $\FlowEq(\AcaOrBc{}, \Filter)$ as in the following two
  equations, and then apply \EquationReasoning{} (under the implicit assumption
  that every RHS is $\geq 0$) to obtain the equivalent third definition :
  \begin{minipage}[b]{0.3\linewidth}
    \begin{equation*}
      \begin{aligned}
        & \Filter(\FromLabelTo{S}{a}{A}) & = \TransitionVar_1 \\
        & \Filter(\FromLabelTo{S}{b}{B}) & = \TransitionVar_2 \\
        & \Filter(\FromLabelTo{A}{c}{A})  & = \TransitionVar_3  \\
        & \Filter(\FromLabelTo{B}{b}{S}) & = \TransitionVar_4 \\
        & \Filter(\FromLabelTo{A}{a}{F}) & = \TransitionVar_5 \\
        & \Filter(\FromLabelTo{B}{c}{F}) & = \TransitionVar_6 \\
      \end{aligned}
    \end{equation*}    
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}[b]{0.3\linewidth}
    \begin{equation*}
      \begin{aligned}
        % S
        &  \TransitionVar_4 = \TransitionVar_5 + \TransitionVar_2 - 1 \\
        % A
        & \TransitionVar_1 = \TransitionVar_5 \\
        % B
        & \TransitionVar_2 = \TransitionVar_4 + \TransitionVar_6 \\
        % F
        & \TransitionVar_5 + \TransitionVar_6 = \FinalStateVar_1 \\
        & \FinalStateVar_1 = 1 \\
      \end{aligned}  
    \end{equation*}    
  \end{minipage}
  \begin{minipage}[b]{0.3\linewidth}
    \begin{equation*}
      \begin{aligned}
        & \Filter(\FromLabelTo{S}{a}{A}) & = 1 - \TransitionVar_6 \\
        & \Filter(\FromLabelTo{S}{b}{B}) & = \TransitionVar_4 + \TransitionVar_6 \\
        & \Filter(\FromLabelTo{A}{c}{A})  & = \TransitionVar_3  \\
        & \Filter(\FromLabelTo{B}{b}{S}) & = 2\TransitionVar_6 + \TransitionVar_4 \\
        & \Filter(\FromLabelTo{A}{a}{F}) & = 1 - \TransitionVar_6 \\
        & \Filter(\FromLabelTo{B}{c}{F}) & = \TransitionVar_6 \\
      \end{aligned}
    \end{equation*}    
  \end{minipage}
  \caption{Equivalences defining $\Filter$ and $\FlowEq(\AcaOrBc{}, \Filter)$
  respectively.}\label{fig:example:single:equivalences}
  \end{figure}

\begin{figure}
  \centering
\begin{prooftree}
  \hypo{\bigstar}
  \infer1[]{
    \begin{matrix}
      1 - \TransitionVar_6 = 0 \land \\
      \TransitionVar_6 = 1 \land \\
      \TransitionVar_4 + 1 > 0 \land \\
      \TransitionVar_3 = 0 \land \\
      2 + \TransitionVar_4 > 0 \land \\
      k = 1 + \TransitionVar_4 
    \end{matrix}
  }
  \infer1[\Subsume{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 = 0 \land \\
      \TransitionVar_6 = 1 \land \\
      \TransitionVar_4 + 1 > 0 \land \\
      \TransitionVar_3 = 0 \land \\
      2 + \TransitionVar_4 > 0 \land \\
      k = 1 + \TransitionVar_4 
    \end{matrix}
  }
  \infer1[\Propagate]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 = 0 \land \\
      \TransitionVar_6 = 1 \land \\
      \TransitionVar_4 + 1 > 0 \land \\
      \TransitionVar_3 \geq 0 \land \\
      2 + \TransitionVar_4 \geq 0 \land \\
      2k = 2 + 2\TransitionVar_4 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\EquationReasoning{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 = 0 \land \\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3 + \TransitionVar_6
    \end{matrix}
  }
  % BRANCH:  = 0
  \hypo{\bigstar}
  \infer1[]{
    \begin{matrix}
      \TransitionVar_3 > 0 \\
      \TransitionVar_6 = 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\Subsume{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \TransitionVar_3 > = 0 \\
      \TransitionVar_6 = 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\EquationReasoning{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \TransitionVar_6 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0\\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\Propagate{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \TransitionVar_6 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\EquationReasoning{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3 + \TransitionVar_6
    \end{matrix} % BRANCH: > 0
  } % SPLIT
  \infer2[\Split{} $1 - \TransitionVar_6$]{ \Connected(\AcaOrBc{}, \Filter) \land 2k = 1 + 2\TransitionVar_4 + \TransitionVar_3 + \TransitionVar_6 }
  \infer1[\EquationReasoning{}]{
    \Connected(\AcaOrBc{}, \Filter) \land
    2k + 1 =
    \sum\limits_{\Transition \in \Transitions} (\Transition \mapsto 1)(\Label) \cdot \Filter(\Transition)
  }
  \infer1[\Expand{}]{\Image{}_{\AcaOrBc{}, \Transition \mapsto 1}(\Filter, 2k + 1)}
\end{prooftree}
\caption{A derivation for \Calculus{} computing odd lengths in $\AcaOrBc{}$.}\label{fig:derivation:single}
\end{figure}

In \cref{fig:derivation:single}, we see how we start by expanding the predicate
using the (simplified) flow equations. We can also see how interleaving
reasoning on the corresponding linear equations helps the production. In both
branches the reasoning is similar: we conclude that when choosing either path at
the starting state we must avoid the other one, perform propagation based on
that fact, use some algebraic reasoning to derive the expected forms of the
upper and lower bounds to propagate the now disconnected transitions, and
finally subsume, removing the $\Connected{}$ predicate when we are unable to use
either of the rules, leaving only the satisfiable remnants of the flow equations
at the respective leaves marked with~$\bigstar$. To obtain a solution from the
proof, one would need to perform standard model generation on the remaining
constraints to obtain values for the sought variable $\MonoidElement$. The full
Parikh image is the disjunction of the constraints left at the two
$\bigstar$-marked leaves, after quantifier elimination.

\subsection{Correctness of \Calculus{}}\label{sec:single:correct}

Our correctness proof of \Calculus{} consists of two main parts: we
first show that the construction of a proof always terminates, and
then that each of the proof rules in \cref{tbl:rules:single} is
an equivalence transformation, i.e., does not change the set of
satisfying assignments of a formula. In combination, those two results
immediately imply that \Calculus{} gives rise to a decision procedure.

\subsubsection{\Calculus{} terminates}
\begin{lemma}\label{lma:single-terminates}
  Suppose $\SomeClause{}$ is a set of formulas in which the predicates
  $\Image$ and $\Connected$ only occur positively. There is no
  infinite sequence of proofs~$P_0, P_1, P_2, \ldots$ in which $P_0$
  has $\SomeClause{}$ as root, and each $P_{i+1}$ is derived from
  $P_i$ by applying one of the rules in \cref{tbl:rules:single}.
\end{lemma}

\begin{proof}
  The rule~\Expand{} can only be applied finitely often, since each
  application removes one $\Image$ predicate, and none of the rules
  introduce new instances of the predicate. The rule~\Subsume{} can
  only be applied finitely often, since it strictly decreases the
  combined number of $\Image$ and $\Connected$ predicates in sets of
  formulas, and none of the rules increases that number.

  To show termination of \Split{} and \Propagate{}, observe that the
  $\Filter$ in a predicate~$\Connected(\Automaton, \Filter)$ is never
  updated on a proof branch, which means that the set of terms
  $\Filter(t)$ for $t \in \Automaton$ on every branch is finite. Each
  application of \Split{} and \Propagate{} adds a new
  formula~$\Filter(\Transition) = 0$ or $\Filter(\Transition) > 0$
  to a proof goal, which can only happen finitely often.
\end{proof}

\subsubsection{The rules in Table~\ref{tbl:rules:single} are solution-preserving}

\begin{lemma}\label{lma:single-correct}
  Consider an application of one of the rules in
  Table~\ref{tbl:rules:single}, with
  premises~$\SomeClause_1, \ldots, \SomeClause_k$ and
  conclusion~$\SomeClause$. An assignment~$\beta$ satisfies the
  conclusion~$\SomeClause$ if and only if it satisfies one of the
  premises~$\SomeClause_i$.
\end{lemma}

\begin{proof}
  This property has to be shown by analysing the possible applications
  of each proof rule.

  \Expand{} unfolds the definition of the $\Image$ predicate. To show
  that the rule is solution-preserving we prove the equivalence of the
  upper and lower sets of formulas:
  \begin{itemize}
  \item Assume that $\beta$ satisfies the conclusion, which means that
    there is some accepting path
    $\Path = \PathEnumeration \in \Accepting{\Paths(\Automaton)}$ with
    $\val_\beta(\Filter(\Transition)) = \TransitionCount(\Transition,
    \Path)$ and $\val_\beta(\MonoidElement) =
    \Map(\WordOf(\Path))$. Since immediately implies that $\beta$
    satisfies $\Connected(\Automaton, \Filter)$, since a path is
    connected, and $\FlowEq(\Automaton, \Filter)$ since an accepting
    path satisfies the flow equations. The
    equation~$\MonoidElement = \sum_{\Transition \in
      \Transitions_\Automaton}\Filter(\Transition) \cdot
    \Map(\Transition)$ holds because of
    $\val_\beta(\Filter(\Transition)) = \TransitionCount(\Transition,
    \Path)$.
  \item Assume that $\beta$ satisfies the premise, which implies that
    $\val_\beta(\Filter)$ describes a consistent, connected flow of
    the automaton. By the same argument as in
    \cite{generate-parikh-image} (Section~\ref{sec:verma}), this flow
    can be mapped to an accepting path~$\Path$ of $\Automaton$ such
    that each transition~$\Transition$ occurs on $\Path$ exactly
    $\val_\beta(\Filter(\Transition))$ times. Together with the equation
    $\val_\beta(\Filter(\Transition)) = \TransitionCount(\Transition,
    \Path)$, this implies that $\beta$ satisfies $\SinglePredicateInstance$.
  \end{itemize}

  In \Split{}, we make use of the fact that $\Filter(\Transition)$ is
  $\Naturals$-valued by definition. For any $\beta$, clearly exactly
  one of $\Filter(\Transition) = 0$ or $\Filter(\Transition) > 0$ will
  be satisfied, implying the property.

  For \Propagate{}, suppose that
  $\SeparatingCut(C, \Automaton, \State)$, which means that every
  accepting path visiting~$q$ contains at least one of the transitions
  in $C$. For a $\beta$ satisfying $\Connected(\Automaton, \Filter)$
  and $\Filter(\Transition') = 0$ for $\Transition' \in C$, this means
  that also $\val_\beta(\Filter(\Transition)) = 0$ has to hold.

  Finally, for \Subsume{}, observe that if \Split{} cannot be applied,
  then a goal must contain~$\Filter(\Transition) = 0$ or
  $\Filter(\Transition) > 0$ for every $\Transition$. In case the
  formulas in $\SomeInequalities$ are inconsistent, an application of
  \Subsume{} is trivially solution-preserving; therefore assume that
  $\SomeInequalities$ is consistent, which means that it contains
  exactly one of $\Filter(\Transition) = 0$ or
  $\Filter(\Transition) > 0$ for each $\Transition$.
\end{proof}


\section{Parikh Images from Products of Automata}\label{sec:multiple}

To generalise the calculus to calculations on products of automata, we change the main predicate to take arbitrarily many automata:
\begin{definition}\label{def:multiple}
  $\ImagePredicate{\Automaton_1\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}$
  is true exactly when the single-automaton version of the predicate would hold
  for the automaton.
\end{definition}

  For the calculus, we first extend $\Expand$ to generate flow equations and instances of $\Connected$ for each automaton.

  \begin{table}[t]
    \begin{tabular}{@{}l>{$}c<{$}p{3cm}@{}}\toprule
      Name & \text{Rule} & Side conditions\\
      \midrule
    
      % EXPAND
      \ExpandM & 
      \inferrule
      {
        {\begin{matrix}
          \Set{ 
            \MonoidElement = \sum_{\Transition \in \Transitions_{\Automaton_i}} \Filter(\Transition) \cdot \Map(\Transition)
          \SuchThat \Automaton_i \in \Automaton_1,\ldots,\Automaton_k}, \\
          \Set{\FlowEq(\Automaton_i), \Connected(\Automaton_i) \SuchThat \Automaton_i \in \Automaton_1,\ldots,\Automaton_k}, \\
          \ImagePredicate{\Automaton_1\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \\
          \SomeInequalities, \SomeClause
        \end{matrix}}
        }
      {\ImagePredicate{\Automaton_1\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause} & 
      None \\[5ex]
      \Materialise &
      \inferrule
      {    \BindingSum(\Automaton', \Filter),\ImagePredicate{\Automaton'\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause}
      {\ImagePredicate{\Automaton_1\times\Automaton_2\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause} &
      $\Automaton' = \Automaton_1\times\Automaton_2$ \\
      \bottomrule
      \end{tabular}
      \caption{Additional derivation rules for products of arbitrarily many automata.}\label{tbl:rules:multi}
    \end{table}

\ExpandM{} must be applied before any other rule, like \Expand{}, but unlike \Expand{}, \ExpandM{} does not remove the $\Image$~predicate since it is needed to keep track of the product.

Before we can begin to define our final rule, we need to talk about product states.

Then we introduce the rule $\Materialise$, used to compute a partial product between two terms:

  With the following helper symbolic function:

  $$
  \BindingSum(\Automaton_1 \times \Automaton_2, \Filter) = \bigcup
  \begin{aligned}
  & \Set{ 
    \left<\Filter(\Transition)  =  \sum\limits_{\Transition' = \FromLabelTo{\Tuple{\State, \State_R}}{\Label}{\Tuple{\State', \State_R}}} \Filter(\Transition')\right>
  \SuchThat \Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions_{\Automaton_1} } , \\ 
  & \Set{
    \left<\Filter(\Transition)  =  \sum\limits_{\Transition' = \FromLabelTo{\Tuple{\State_L, \State}}{\Label}{\Tuple{\State_L, \State'}}} \Filter(\Transition')\right> \SuchThat \Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions_{\Automaton_2}
  }
  \end{aligned}
$$

Note that this definition implies that $\Filter(\Transition) =
\Filter(\Transition')$ whenever two transitions $\Transition \in
\Transitions_{\Automaton_1}, \Transition' \in \Transitions_{\Automaton_2}$
produces a product transition $\Transition'' \in \Transitions_{\Automaton_1
\times \Automaton_2}$. This corresponds to our intuition that the terms of the
product must agree on the value they accept. As before, we implicitly map
$\Filter$ to fresh terms for each transition in the product.

Finally, for instances of precisely one automaton, neither rule applies and we
perform the calculus as before.

\subsection{An Example}\label{sec:multiple:example}

We return again to our example in \cref{sec:introduction:parikh}, where we
compute the whole Parikh image of the product of $\AcaOrBc{}$ and
$\SomethingCSomething$ under the constraint that there are more instances of
letters a than b. This time our monoid $\Monoid$ is 3-dimensional vectors with
element-wise addition, and $\Map$ that maps each transition to the corresponding
increment vector, e.g $\Map(\FromLabelTo{S}{a}{A}) = \VectorLiteral{1,0,0}$.


In the interest of space, we refer back to
\cref{fig:example:single:equivalences} for the definitions of $\Filter$ for
$\AcaOrBc$ and only define $\Filter$ for $\SomethingCSomething{}$ after
substitutions as follows. Note the expansion of the $\Sigma$ labels:
    \begin{equation*}
      \begin{aligned}
        % & \Filter(\FromLabelTo{S}{a}{A}) & = 1 - \TransitionVar_6 \\
        % & \Filter(\FromLabelTo{S}{b}{B}) & = \TransitionVar_4 + \TransitionVar_6 \\
        % & \Filter(\FromLabelTo{A}{c}{A})  & = \TransitionVar_3  \\
        % & \Filter(\FromLabelTo{B}{b}{S}) & = 2\TransitionVar_6 + \TransitionVar_4 \\
        % & \Filter(\FromLabelTo{A}{a}{F}) & = 1 - \TransitionVar_6 \\
        % & \Filter(\FromLabelTo{B}{c}{F}) & = \TransitionVar_6 \\
        % Other automaton
        & \Filter(\FromLabelTo{S}{\Sigma}{S}) & = \VectorLiteral{\TransitionVar_{7a}, \TransitionVar_{7b}, \TransitionVar_{7c}} \\
        & \Filter(\FromLabelTo{S}{c}{F}) & = 1 \\
        & \Filter(\FromLabelTo{F}{\Sigma}{F}) & = \VectorLiteral{\TransitionVar_{9a}, \TransitionVar_{9b}, \TransitionVar_{9c}} \\
      \end{aligned}
    \end{equation*}
    
In the derivation tree of \cref{fig:derivation:multi} we start as before, but
with the product version of the $\Image$ predicate. The only possible rule here
is $\ExpandM$, which we use to add the corresponding constraints on each
automata of the product as we would have had in the single-automaton version. We
perform algebraic reasoning on those equations, along with the constraint on
$\MonoidElement$ to determine bounds on transition variables that will enable us
to \Subsume{} and remove one of the $\Connected{}$ predicates. When we have
finished doing so, we use \Materialise{} to compute the product of the now
filtered automata. Further algebraic reasoning allows us to remove the final
instance of $\Connected$. We then perform \Expand{} to get rid of the final
$\Image$ instance, and are immediately able to $\Subsume$ the resulting
$\Connected{}$ predicate. This, again, leaves us with a set of linear
inequalities that we can use to obtain a model of our final $\MonoidElement$
values (now a vector). Performing quantifier elimination on the remaining clauses would also produce the Presburger formula of the Parikh image under the constraint we started with.

\begin{figure}
  \centering
\begin{prooftree}
  \hypo{\bigstar}
  \infer1[]{
    \begin{matrix}
      1 = \TransitionVar_{10} \land 
      \TransitionVar_3 = \TransitionVar_{11} \land \\
      1 = \TransitionVar_{12} \land 
      1 = \TransitionVar_{11} \land 
      2 > 0
    \end{matrix}  
  }
  \infer1[\Subsume{}, \Expand{}, \Subsume{}]{
    \begin{matrix}
      1 = \TransitionVar_{10} \land 
      \TransitionVar_3 = \TransitionVar_{11} \land  \\
      1 = \TransitionVar_{12} \land 
      1 = \TransitionVar_{11} \land \\
      \Image{}_{\Automaton', \Map}(\Filter, 
      \VectorLiteral{2, 0, 1}) \land 
      \Connected(\SomethingCSomething{}, \Filter) \land
        2 > 0
    \end{matrix}  
  }
  \infer1[\EquationReasoning{}]{
    \begin{matrix}
      1 = \TransitionVar_{10} \land
      \TransitionVar_3 = \TransitionVar_{11} 
      1 = \TransitionVar_{12} \land
      1 = \TransitionVar_{11} \\
      \Image{}_{\Automaton', \Map}(\Filter, 
      \VectorLiteral{
        \TransitionVar_{7a} + \TransitionVar_{9a},
        0,
        \TransitionVar_{7c} + \TransitionVar_{9c} + 1}) \land \\
        \TransitionVar_6 + \TransitionVar_4 = 0 \land 
        1 - \TransitionVar_6 > 0 \land
        \TransitionVar_6 = 0 \land 
        \TransitionVar_4 = 0 \land
        \TransitionVar_3 > 0 \land
        2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
          \VectorLiteral{
        \TransitionVar_{7a} + \TransitionVar_{9a},
        0,
        \TransitionVar_{7c} + \TransitionVar_{9c} + 1}
        = \VectorLiteral{
          2,
          0,
          \TransitionVar_3} \land \\
      \Connected(\SomethingCSomething{}, \Filter) \land
      \land \TransitionVar_{7a} + \TransitionVar_{9a} > 0
    \end{matrix}  
  }
  \infer1[\Materialise]{
    \begin{matrix}
      \TransitionVar_6 + \TransitionVar_4 = 0 \land 
      1 - \TransitionVar_6 > 0 \land
      \TransitionVar_6 = 0 \land 
      \TransitionVar_4 = 0 \land
      \TransitionVar_3 > 0 \land
      2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
      \VectorLiteral{
        \TransitionVar_{7a} + \TransitionVar_{9a},
        0,
        \TransitionVar_{7c} + \TransitionVar_{9c} + 1}
        = \VectorLiteral{
          2,
          0,
          \TransitionVar_3} \land \\
      \Connected(\SomethingCSomething{}, \Filter) \land \\
      \Image{}_{\AcaOrBc{}\times\SomethingCSomething{}, \Map}(\Filter, 
      \VectorLiteral{\TransitionVar_{7a} + \TransitionVar_{9a}, 0, \TransitionVar_{7c} + \TransitionVar_{9c} + 1}) \land \TransitionVar_{7a} + \TransitionVar_{9a} > 0
    \end{matrix}
  }
  \infer1[\Subsume{}]{
  \begin{matrix}
    \TransitionVar_6 + \TransitionVar_4 = 0 \land 
    1 - \TransitionVar_6 > 0 \land
    \TransitionVar_6 = 0 \land 
    \TransitionVar_4 = 0 \land
    \TransitionVar_3 > 0 \land
    2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
    \VectorLiteral{
      \TransitionVar_{7a} + \TransitionVar_{9a},
      0,
      \TransitionVar_{7c} + \TransitionVar_{9c} + 1}
      = \VectorLiteral{
        2,
        0,
        \TransitionVar_3} \land \\
    \Connected(\AcaOrBc{}, \Filter) \land 
    \Connected(\SomethingCSomething{}, \Filter) \land \\
    \Image{}_{\AcaOrBc{}\times\SomethingCSomething{}, \Map}(\Filter, 
    \VectorLiteral{\TransitionVar_{7a} + \TransitionVar_{9a}, 0, \TransitionVar_{7c} + \TransitionVar_{9c} + 1}) \land \TransitionVar_{7a} + \TransitionVar_{9a} > 0
  \end{matrix}
  }
  \infer1[\EquationReasoning]{
    \begin{matrix}
      \VectorLiteral{a, b, c} = \VectorLiteral{
          2 - 2\TransitionVar_6,
          2\TransitionVar_4 + 3\TransitionVar_6,
          \TransitionVar_3 + \TransitionVar_6
        } \land
        \\
        \VectorLiteral{a, b, c} = \VectorLiteral{
            \TransitionVar_{7a} + \TransitionVar_{9a},
            \TransitionVar_{7b} + \TransitionVar_{9b},
            \TransitionVar_{7c} + \TransitionVar_{9c} + 1
          } \land \\
      \Connected(\AcaOrBc{}, \Filter) \land 
      \Connected(\SomethingCSomething{}, \Filter) \land \\
      \Image{}_{\AcaOrBc{}\times\SomethingCSomething{}, \Map}(\Filter, 
      \VectorLiteral{a, b, c}) \land a > b
    \end{matrix}
  }
  \infer1[\ExpandM]{\Image{}_{\AcaOrBc{}\times\SomethingCSomething{}, \Map}(\Filter, \VectorLiteral{a, b, c}) \land a > b}
\end{prooftree}
\caption{A derivation for \Calculus{} on the Parikh image of strings with more a's than b's in the product of $\AcaOrBc{}$ and $\SomethingCSomething{}$.}\label{fig:derivation:multi}
\end{figure}

\subsection{The extended calculus is also correct}

Since the expanded calculus is in practice the addition of two rules with the
purpose of reducing an~$\Image$ predicate containing a product to the form
solved by the previous single-automaton rules of \cref{tbl:rules:single}, we
extend the reasoning from \cref{sec:single:correct} to the single-automaton
rules of \cref{tbl:rules:multi}.

\subsubsection{The calculus terminates}
\begin{lemma}
  The addition of the \ExpandM{} and \Materialise{} rules in \cref{tbl:rules:multi}
  maintains termination as shown in \cref{lma:single-terminates}.
\end{lemma}
\begin{proof}
  Similarly to how we previously showed that the number of possible executions
  of the \Split{} rule is bounded above by the number of transitions of an
  automaton, we can bound the number of applications of \Materialise{} by the
  (monotonically) decreasing number of automata in the product until we have
  approached the starting state for the single-automaton calculus. Trivially, we
  can eagerly apply \Materialise{} repeatedly to do so. Similarly, \ExpandM{} is
  just the generalisation of \Expand{} to multiple automata. In other words there are no possibilities of the calculus diverging.
\end{proof}

\subsubsection{The calculus implemements the homomorphic image of the product}
\begin{lemma}
  Using \Calculus{} to compute $\Image_{\Automaton_1\times\ldots\times\Automaton_k, \Map}(\Filter, \MonoidElement)$ is equivalent to using the single-automaton rules to compute $\Image_{\Automaton', \Map}(\Filter, \MonoidElement)$ where $\Automaton' = \Automaton_1\times\ldots\times\Automaton_k$.
\end{lemma}
\begin{proof}

  We perform this proof equivalent to set inclusion: by proving that neither predicate is stronger than the other, in the sense of rejecting an element that the other accepts and vice versa.

\begin{lemma}\label{lma:multi:rinclude}
  There exists no element $\MonoidElement$ such that $\Image_{\Automaton_1\times\ldots\times\Automaton_k, \Map}(\Filter, \MonoidElement) = \top$ but $\Image_{\Automaton', \Map}(\Filter, \MonoidElement) = \bot$.
\end{lemma}
\begin{proof}
  Assume that such an $\MonoidElement$ exists with the goal of deriving a
  contradiction. Under this assumption, would we be able to use the two new
  rules to close the proof goal with $\top$?

  The \ExpandM{} rule would allow us to close no goal since it only adds
  additional clauses. Our only hope is in using \Materialise{} to get rid of the
  product step by step without introducing an unsatisfiable clause. However, all
  \Materialise{} does is computing the product step by step, eventually arriving
  at $\Automaton'$ modulo permutations. Therefore, it can be no more satisfiable
  than the single-automata predicate.

  \contents{\item Do I need to worry about the fact that I have no extra
  clauses/$\SomeClause$ here? If present, I would need to apply the definition
  of \BindingSum{} since that is what transfers constraints on individual
  automata up and down the materialisation.}
\end{proof}

\begin{lemma}\label{lma:multi:linclude}
  There exists no element $\MonoidElement$ such that $\Image_{\Automaton_1\times\ldots\times\Automaton_k, \Map}(\Filter, \MonoidElement) = \top$ but $\Image_{\Automaton', \Map}(\Filter, \MonoidElement) = \bot$.
\end{lemma}
\begin{proof}
  Assume that such an element exists. Then $\MonoidElement =
  \Map(\WordOf(\Path))$ for some $\Path$ ending up in an accepting state in
  $\Automaton'$, and consistent with $\Filter$ along the transitions of $\Path$.
  Then a corresponding path must exist in each automata of $\Automaton'$,
  although possibly with different labels.

  For \ExpandM{} to introduce unsatisfiability, either of its automata must be
  disconnected (or its corresponding $\Connected{}$ predicate can be removed
  with \Propagate{}), or its flow must be infeasible. However, this contradicts
  the existence of some corresponding path $\Path$ in each automaton. Hence,
  \ExpandM{} cannot be the source of the unsatisfiability.

  The same applies for the second half of \Materialise{}, if the corresponding
  path exists in each individual automaton and in $\Automaton'$, it must also
  exist in every intermittent product on the way to $\Automaton'$, and if $\Map$
  is consistent for intermittent transition labels as it must be, then neither the intermittent $\Image$ predicates nor the final one can be the source of the unsatisfiability.

  This leaves one final potential source of unsatisfiability: the first half of
  \Materialise{}: \BindingSum{}. For that to be unsatisfiable, it requires
  intermittent automata $\Automaton_l, \Automaton_r$ such that adding
  $\BindingSum(\Automaton_l \times \Automaton_r)$ is unsatisfiable. The
  generated equations state that the transitions of $\Automaton_l$ and
  $\Automaton_r$ must be used precisely as often as their corresponding results
  in $\Automaton_l \times \Automaton_r$. For this set of equations to be
  unsatisfiable there must exist no path through $\Automaton_l \times
  \Automaton_r$ that preserves this. However, this is true from the definition
  of the product of automata: since we know the product to be nonempty, it must
  in some sense contain at least one path whose equivalent exists in both
  $\Automaton_l$ and $\Automaton_2$ and cannot use a corresponding transition
  more often, or it would recognise a different langauge. Hence no such case
  exists.
\end{proof}

Since it follows from \cref{lma:multi:linclude,lma:multi:rinclude} that both
variants of the predicate are precisely equally strong, it must mean that they
are equivalent, and we have proven the correctness as defined in
\cref{def:multiple}.
\end{proof}


\section{Extensions}\label{sec:extensions}

This section describes a number of possible extensions to the calculus. Note
that some are partially or fully implemented in \Catra{} already. In particular,
we have some level of support for symbolic transitions over Unicode alphabets to
keep practical automata under a reasonable size, though we do not allow a full Boolean algebra over symbols as described e.g. in~\cite{symbolic-automata}.

\subsection{Backjumping and Learning No-Goods}\label{sec:ext:backjumping}

\Calculus{} can be accelerated for some instances by adding rules for
backjumping. In particular, central connectedness constraints for an automaton
can be learnt. In that case, when discovering that a state $\State$ of an
automaton $\Automaton$ under a certain transition variable $\Filter$ has become
unreachable, we can learn the clause $\Connected(\Automaton, \Filter) \land
\sum_{\Transition \in \SeparatingCut(\Automaton, \State)} \Filter(\Transition) =
0 \implies \sum_{\Transition' = \FromLabelTo{\State}{}{} \in
\Transitions_{\Automaton}} = 0$. A similar rule can be used when a state becomes
backwards-unreachable (but with a corresponding backwards cut). At the moment,
only forward-cut learning is implemented in \Catra, and has provided a slight
improvement in performance on some instances.

The other source of clauses to learn is the \Materialise{} rule. Whenever an
attempt to materialise a product of two ($\Filter$-filtered) automata
$\Automaton_1 \times \Automaton_2$ produces an empty product, we can determine
the cause of the failure with respect to the automata and their respective
transition variables, and learn no-good combinations that can never be part of a
model. In order to be able to do this, we need a few semantic predicates to
record the state of the calculation, as well as a system of disambiguating
automata, since it is possible to arrive at the same automaton by multiple
combinations of decisions and products. In \Catra{}, we use a number of
additional predicates to record the status of the materialisation of products,
to separate instances of our main predicate, and to register the mapping between
automata and their respective transition $\Filter$ terms, described in
\cref{sec:implementation}. However, at this point only rudimentary no-good
learning is implemented.

\subsection{Symbolic Automata}\label{sec:ext:symbolic}

Extending \Calculus{} to support fully symbolic automata is possible within the
framework, depending on your choice of $\Map$. The difficulty consists in
handling the mapping of the homomorphism $\Map$ over symbolic labels, assuming
it maps to finitely many monoid elements. This is not always straightforward, as
seen in the example of \cref{sec:multiple:example}. This complexity is inherent
in computing the full Parikh image, and stems from the fact that we need to
differentiate between the possible interpretations of the $\Sigma$ transitions
without knowing ahead of times which ones will actually be materialied in the
product. If, on the other hand, if our $\Map$ had been length-counting, which
does not differentiate between values, it would have required no adaptation at
all. With a somewhat liberal interpretation of what we are allowed to map to
(e.g. fresh terms), it is possible to use $\Map$ to represent choice operations
like the ones in a range label. In \Catra{}, we frontload this problem by
requiring the user to encode their input as a Parikh automaton. \Fudge{How is
the encoding of Ostrich Plus automata done?}

\subsection{Transducers}\label{sec:ext:transducers}

Another interesting application is applying \Calculus{} to automata with
multiple tracks, e.g. transducers. One application of such a calculus would be
to represent replace operations and other functions on regular languages, and to
be able to answer questions like "does this operation change the length of the
string". The difficulty in implementing it for transducers is first to perform
the mapping on the labels, which for both the length and Parikh cases is
straightforward; just do the same thing in two dimensions. The more complicated
operation is defining the product of transducers. In some cases it would
probably be desirable to perform synchronisation (e.g. requiring overlapping
transitions) on only some transitions, for example the first track.

Implementing such a calculus is straightforward in \Calculus, since the
definition of products was left out of the definition. All that would be needed
is an appropriate update of the definitions of products. Simiarly, \Catra was
written with modularity in mind, and it should be straightforward to extend both
the input grammar and automata implementations to accommodate multi-track
automata with arbitrary synchronisation.

\subsection{Finding the Presburger representation of a homomorphic image}\label{sec:finding-the-image}

To find the Presburger form of the homomorphic image efficiently, we adapt the
quantifier elimination approach of~\cite{qe} to our problem domain. The core
method is the same: we incrementally use \Calculus{} to find models,
\Generalise{} the models we find into a quantifier-free Presburger formula with
the $\MonoidElement$ as the only free variable (algorithm~\ref{alg:generalise}), add the
negated formula as a constraint, and continue enumerating models until none
remain. The disjunction of the generalised models we enumerated is now our
image.

\begin{algorithm}
  \caption{$\Generalise{}(\Automaton, \Filter, a)$ will generalise a final product $\Automaton$ and a model $a$ under the homomorphism $\Map$.}\label{alg:generalise}
  \KwData{$\Automaton$, a product from \Calculus{}, a model $a$ assigning counts to the terms that $\Filter$ associate with each transition of $\Automaton$, and our homomorphism $\Map$ that we want to compute the image modulo.}
  \KwResult{a quantifier-free Presburger formula $P$ representing a partial $\Map$-homomorphic image}
  \SetKwFunction{EliminateQuantifiers}{eliminateQuantifiers}

  $\Automaton' \gets \Tuple{\Automaton_\States, \Automaton_{\InitialState}, \Automaton_\AcceptingStates, \Set{\Transition \in \Automaton_\Transitions \SuchThat a(\Filter(\Transition)) \neq 0}}$

  \KwRet{\EliminateQuantifiers{$\Map(\ParikhMap(\Automaton'))$}}
  \end{algorithm}


  \begin{algorithm}
    \DontPrintSemicolon
    \caption{$\FindImage{}(\Automaton_1 \times \ldots \times \Automaton_k, \Map)$ will find the Presburger form for the product $\Automaton_1 \times \ldots \times \Automaton_k$ modulo a homomorphism $\Map$ where the only free variable is/are the one(s) representing the monoid element of $\Map$.}\label{alg:find-image}
    \KwData{$\Automaton$, a product from \Calculus{}, a model $a$ assigning counts to the terms that $\Filter$ associate with each transition of $\Automaton$, and our homomorphism $\Map$ that we want to compute the image modulo.}
    \KwResult{a quantifier-free Presburger formula $P$ representing a partial $\Map$-homomorphic image}
    \SetKwFunction{NewTheoremProver}{newTheoremProver}
    \SetKwFunction{EliminateQuantifiers}{eliminateQuantifiers}
    \SetKwFunction{FreshVariable}{freshVariable}
    \SetKwFunction{Assert}{assert}
    \SetKwFunction{GetModel}{getModel}
    \SetKwData{ImageVar}{image}
  
$p \gets \NewTheoremProver{}$\;
$\Filter(\Transition) := \FreshVariable{p}$ for every $\Transition \in \Transitions_{\Automaton}$\;
$\MonoidElement \gets$ \FreshVariable{$p$}\;
\Assert{$p, \exists \MonoidElement, \Filter(\Transition) \text{ for every } \Transition \in \Transitions_\Automaton \HoldsThat \ImagePredicate{\Automaton}{\Map}{\Filter}{\MonoidElement} \land \AndComp{\Transition \in \Transitions}{\Filter(\Transition) \geq 0}$}\;
$\ImageVar \gets \bot$\;
\While{$p$ has more models}{
  $\Tuple{\Automaton, a} \gets \GetModel(p)$\;
  $G \gets \Generalise{}(\Automaton, \Filter, a)$\;
  $\ImageVar \gets G \lor \Image$\;
  \Assert{$p, \lnot G$}\;
  }
    \KwRet{\ImageVar}
    \end{algorithm}

    The \GetModel{} function is nonstandard in that it returns both the model
    and its associated automaton. Since algorithm~\ref{alg:find-image} is
    essentially a form of quantifier elimination and therefore an internal affair
    to the theorem prover, this should be considered fair game.
    
    A preliminary implementation of this approach is available as part
    of~\Catra, but has been neither optimised nor fully tested.

\section{Implementation}\label{sec:implementation}

We implement \Calculus{} for Parikh automata as described in
\cref{sec:parikh-automata}. The artefact submitted along with this paper is a
program that reads an instance file with one or more products of one or more
Parikh automaton with transition labels defined as ranges of Unicode characters,
along with a set of constraints on the final values of their registers expressed
as Presburger arithmetic in a C-like syntax. We call this program
\Catra.\footnote{If you really must read it as an acronym, please read it as
CAtegory Theory on Register Automata, or if you object to the somewhat
nonstandard use of register automata and category theory, as Check Assignments
of The Registers Afterwards. Or alternatively if you find it all to be too much
of a theoretical exercise, as Can Anyone Think of A Real Application.}

\Catra{} is written in Scala, with the calculus described in this paper
implemented as a theory plug-in for the \Princess{} automated theorem
prover~\cite{princess}, which also performs the Presburger reasoning. For
comparison, we also provide an implementation of the baseline method
from~\cite{generate-parikh-image}, a direct translation that uses the~\Nuxmv{}
symbolic model checker~\cite{nuxmv} to solve our constraints, and the
approximation described in~\cite{approximate-parikh} on top of the standard
baseline back-end. An example of an input file corresponding to our running
example introduced in \cref{sec:motivation} can be seen in
\cref{lst:input-example}.

\Catra{} uses symbolic labels for automata. A symbolic label is defined as a
finite range of Unicode code points. This allows succinct representation of many
regular expression patterns such as \lstinline{(a-z).*} which would have
otherwise required $27$~transitions. The transition for the range would be
written \lstinline{a -> b [97, 122]}.

In satisfaction mode, supported by all backends, \Catra{} tries to satisfy the
constraints expressed by the input file, reporting \Sat{} with register
assignments or \Unsat{} much like traditional~SAT- or SMT solvers would.
Additionally, baseline and \Calculus{} also support generation of the Presburger
formula describing the constraints of the input file. Baseline uses standard
quantifier elimination, and \Calculus{} uses the method described in
\cref{sec:finding-the-image}.

Since \Princess{} does not support multiple-arity predicates like the ones we
use in \Calculus{}, we have implemented variable-length arguments using
additional helper predicates. These are $\Unused{}(\Automaton)$, which marks an
automaton as unused in any product, and $\TransitionMask{}(\Automaton,
\Transition, \Filter(\Transition))$ which associates a transition $\Transition$
and automaton $\Automaton$ with its corresponding transition variable.
Additionally, we associate each of our predicates with an instance variable in
order to differentiate instances of the predicates.

\subsection{Implementing the Baseline}\label{sec:implementing-baseline}

We baseline using the same Presburger solver (\Princess{}), input file parser,
and automaton implementation as \Catra. We do this in order to better analyse
the impact of the calculus rules themselves. Using the formula of
\eqref{eq:generate-parikh}, we produce quantified Presburger formulae for each
successive term and add them to \Princess. We compute the product incrementally
term by term, checking satisfiability at each step. We use a priority queue to
select automata for each step, and order it by the number of transitions as a
heuristic for the size of the automata. We use this heuristic to avoid computing
large (and therefore slow) products until we have to, banking our hopes on
computing an empty intermittent product early. The pseudocode for our
implementation can be seen in \cref{alg:baseline}.

\begin{algorithm}
  \caption{How we implement the baseline approach}\label{alg:baseline}
  \KwData{$\Automaton_1, \ldots, \Automaton_n$ automata, other constraints $\SomeClause$}
  \KwResult{\textsc{Sat} or \textsc{Unsat}}
  \SetKwFunction{NewTheoremProver}{newTheoremProver}
  \SetKwFunction{NewPriorityQueue}{newPriorityQueue}
  \SetKwFunction{Dequeue}{dequeue}
  \SetKwFunction{Enqueue}{enqueue}
  \SetKwFunction{Assert}{assert}

  $p \gets \NewTheoremProver{}$

  \Assert{$p$, $\SomeClause$}

  \ForEach{$\Automaton_i$}{
    \Assert{$p, \ParikhMap(\Automaton_i)$}

    \If{$p$ is \textsc{Unsat}}{break}

  }

  $q \gets \NewPriorityQueue{}$


  \While{$p$ not \textsc{Unsat} and $|q| > 1$}{
    $\Automaton, \Automaton' \gets \Dequeue{q}$ 
    
    \Assert{$p, \ParikhMap(\Automaton \times \Automaton')$}

    \Enqueue{$q, \Automaton \times \Automaton'$}
  }
  
  \KwRet{$p$'s SAT status}

  \end{algorithm}

As an optimisation, our automata (including intermittent products) are created
forward- and backward- reachable-minimal. Any automaton we produce only contains
states that are both reachable from the initial state and has a path to an
accepting state. We never perform any other minimisation on the automata for
either backend. More complex minimisation was left out since performing
minimisation on automata with counters is non-trivial, and minimising symbolic
automata risks exponential blowup \cite{minimising-symbolic}.

\subsection{Heuristics and search strategies}

There are a number of choices left unspecified in \Calculus{} as described in
\cref{sec:calculus,sec:multiple}. For example, the order of materialisation of
intermediate products and the order of splitting. In this section we describe
additional implementation details and techniques used to enhance \Catra.

\subsubsection{Splitting, Materialisation, and Propagation}

We order our rule applications so that we first propagate connectedness if
possible, then perform materialisation if tractable as defined below, then
finally resort to splitting if we must.

In addition to applying \Split{} as described in \cref{tbl:rules:single} to
randomly selected transitions, we prefer splitting to sever a strongly connected
from the initial state. We randomly select an automaton where we can compute a
cut between an SCC and the initial state, that is where the SCC does not contain
the initial state and where the sum of the transition variables of the
transitions in the cut is not known to be positive. If there are multiple such
strongly connected components we choose one randomly. We then proceed to split
on the sum of the transition variables of the cut as if it were a regular
transition, e.g. its sum being zero or nonzero. In this way we drive \Calculus{}
towards applying \Propagate{}.

The implementation of the connectedness constraint is opportunistic and straightforward. We compute a set of dead states by performing forward and backwards reachability computations on an automaton, where we disregard any transition whose associated variable is known to be zero. After that we add clauses ensuring any transition variable associated with a transition starting in a dead state is zero.

Product materialisation is the final piece of the puzzle. In the current
implementation we put off computing intermediate products until at least all but
two transition variables of one of the automata is known to be either present or
absent. The number was chosen experimentally, and we observe a consistent trend
towards lower numbers being better. The other automaton for the product is
selected randomly.

\subsubsection{Clause Learning}

\Catra{} enables clause learning by default when using our backend, as it has
been experimentally shown to increase the performance in aggregate (though not
strictly). We do not currently implement all the proposed features of
\cref{sec:ext:backjumping}, but we do implement forward-reachability cut
learning. No sophisticated clause learning for products has been implemented.

\subsubsection{Random Restarts}

Finally, we perform restarts scaled by the Luby series~\cite{luby}. Experimental
results have shown this to have a large improvement in performance, which is
unsurprising given how many random choices we make during solving and how
tail-heavy our problem is.

\section{Evaluation}\label{sec:experiments}

We evaluate the performance of \Catra{} on~\NrBenchmarks{} instances of Parikh
automata intersection problems generated by the \Ostrich{} string constraint
solver when solving the \Fudge{pyex benchmarks}. After generating an
initial~\InitialNrBenchmarks{}, we remove~\NrBroken{} misgenerated instances
that did not parse, as well as~\NrTrivial{} instances solved in under five
seconds by baseline. The benchmarks are run on commit~\texttt{\commit}.

The benchmarks are executed in parallel using GNU Parallel~\cite{parallel},
since they are mostly single-threaded and initial results showed negligible
interference on performance, on \BenchmarkRig{}. We compiled the code using
Scala~\ScalaVersion{}, and executed the experiments on~\JvmVersion{} with a
maximum heap of~\MaxHeapSize{}. We used \Nuxmv{} version~\NuxmvVersion{} invoked
as a subprocess for each instance. Instances were executed in batches of
\BatchSize{}, each given a fresh JVM. We believe this represents a realistic use
case where \Calculus{} is used to support e.g a string solver. Experiments were
executed in random order for all backends. Each instance got a time budget
of~\RuntimeTimeout.

All runtimes are measured in wall-clock time as observed by the JVM when
executing the instance, and exclude time spent parsing (usually far below
\numprint{0.1}s).

\subsection{Execution Time and Ability to Solve Instances}\label{sec:runtime}

In \cref{fig:solve-division} we show how many of the~\NrBenchmarks{} instances
the respective back-ends could solve and with which status. A full summary of
their outcomes is also available in \cref{tab:solve-status}. We see that
\Calculus{} generally outperforms \Nuxmv{} on unsatisfiable instances, while
being worse at satisfiable instances. Both \Nuxmv{} and \Calculus{} strictly
outperform baseline on every kind of instance, but most of all on satisfiable
instances. \Cref{tab:solve-status} also shows a small number of bugs in
\Calculus{}: 27~satisfiable instances are misreported as \Unsat{}, and 20
additional instances fail with an error. Both problems are due to
non-deterministic bugs in the clause learning. Note that \Calculus{} is sound:
it never misreports an unsatisfiable instance as \Sat{}, and all
\Fudge{\numprint{6218}} satisfying assignments have been verified by \Nuxmv{}.
Baseline performs worse on satisfiable instances because it executes a heuristic
meant to detect unsatisfiability early at a heavy penalty to satisfiable
instances.

\begin{table}
  \centering
  \input{graphs/solved_pivot_table.tex}
  \caption{The result of running the respective back-ends by instance
  satisifiability (satisifiable or unsatisifiable) with a timeout of
  \RuntimeTimeout. Instances solved by no backend within the timeout are omitted
  from the table. }\label{tab:solve-status}
\end{table}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-by-solver.pdf}
    \caption{The division of statuses per backend.}
    \label{fig:solve-division}
    \Description[A bar chart showing three bars, one per backend, illustrating how many instances they could solve]{The bars are divided by satisfiable and unsatisfiable instances. Baseline could seemingly only solve unsatisfiable instances, lazy could solve a few satisfiable and mostly unsatisfiable, and nuxmv could solve about twice as many unsatisfiable as satisfiable instances, and slightly more in total than lazy.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-time-boxplot.pdf}
    \caption{The distribution of runtimes for solved instances per backend. Note that the number of instances solved differs between backends.}
    \label{fig:runtime-boxplot}
    \Description[A box plot showing the distribution of runtime over the three backends]{The middle box plot shows a tiny box centered around 0 seconds for the lazy backend, the rightmost box shows a bigger box between five seconds and 30 seconds for nuxmv, and the leftmost box shows a smaller box between 20 and 30 seconds for baseline. The whiskers of nuxmv span between 0 and the timeout, 60 seconds, while they are much tighter for lazy. On the other hand, lazy has  a lot of outliers.}
  \end{subfigure}
  \vfill
  \begin{subfigure}[b]{0.75\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-cactus.pdf}
    \caption{The number of instances solved as the time budget increases, simulated from one \RuntimeTimeout-timeout run.}
    \label{fig:cactus}
    \Description[A cactus plot comparing the performance of nuxmv, lazy and baseline]{The line for baseline is strictly lower than the two others, and does not head upwards from zero until after 10 seconds of timeout, while the number of instances for nuxmv increases sharply until ca 10 seconds and then linearly after that. Lazy solves much more instances than nuxmv on shorter runtimes, but is overtaken by just before the 60-second timeout used in these experiments.}
  \end{subfigure}
\end{figure}

\subsubsection{Scalability}\label{sec:scaling}

A cactus plot showing the number of instances solved within a given timeout for
each backend can be seen in \cref{fig:cactus}. Here, we see clearly that
\Calculus{} outperforms \Nuxmv{} for shorter timeouts, and that \Nuxmv{} is
only able to catch up once we increase the timeout to far beyond what would be
normal for a string solver.

\iffalse
\subsection{Finding a Presburger Formula}\label{sec:evaluation:finding-image}

For baseline and \Calculus{}, \Catra{} offers the ability to find the equivalent Presburger formula representing a given instance. For baseline, we use the built-in quantifier elimination facilities of the underlying \Princess{} theorem prover, while for \Catra{} we use the specially tailored approach described in \cref{sec:finding-the-image}. For this experiment, we use only the~\NrKnownSat{} instances known to be satisfiable from the previous experiment detailed in \cref{sec:scaling,sec:runtime}. 

To make sure baseline puts up as much competition as possible, we disable
checking intermittent satisfiability and configure \Catra{} to run in the
maximally eager mode where the product is first computed before any
satisifiability check is performed. We run the experiments with a timeout
of~\ImageTimeout{}. The results of the experiment is summarised in
\cref{fig:cactus:image} and \cref{tab:image-results}. \Fudge{We see here that
something happens}.

\begin{figure}[ht]
  \caption{The number of instances \Catra{} was able to find the Presburger form of the image for within a given number of seconds per backend.}
  \label{fig:cactus:image}
\end{figure}
\fi

\subsection{Threats to Validity}

The most obvious threats to validity would be poor benchmarking or poor
implementation, e.g. if the method described in \cref{sec:calculus,sec:multiple}
deviate from what is actually benchmarked in section \cref{sec:experiments}, or
if the methods used for benchmarking would be unsound.  To increase the
probability that our results are representative both in the sense of
representing  on expected inputs and in the statistical sense for a given run of
\Catra{} despite the use of randomness in our implementation we execute many
experiments. Moreover, to address the issue of correctness we have validated all
reported solutions made by \Calculus{} with \Nuxmv{} to ensure that \Calculus{}
is indeed sound.

Another threat to validity would be if the competition (baseline and \Nuxmv)
would be disadvantaged in our comparison. For \Nuxmv{} we use the default
configuration which we believe should be performant (or it should not be the
default). Additionally, tweaking our invocation of \Nuxmv{} is explicitly made
easy for artefact reviewers. For baseline, our best argument is that the
solution we use is close to the one found in the \Ostrich{} string solver and
that it therefore should be realistic.

The greatest threat to validity is our choice of implementation platform and
automata library. There are some signs that product computation is inefficient,
notably the good performance of low-threshold automata materialisation that
prioritises computing smaller products. This means that \Calculus{} makes less
heavy use of product computation than baseline does, instead relying more on
\Princess{}'. This situation would unfairly advantage \Calculus{} if our
automata library was the bottleneck due to our automata library being
unnaturally poor. We believe this is unlikely since similar performance issues
have been reported for string solvers. Additionally, profiling suggests that
both \Princess{}-based backends spend most of their time in \Princess{},
suggesting that the automaton implementation is not the bottleneck.

\section{Conclusion}

In this paper we have introduced a calculus to compute commutating operations on
intersections of regular languages that we call \Calculus{}. We have evaluated
it on \NrBenchmarks{} Parikh automata intersection problems generated \Fudge{by
the Ostrich+} string solver \cite{ostrich} solving \Fudge{standard string
constraint benchmarks} using our Parikh automata solver \Catra{}.

Within \Catra{}, \Calculus{} shows astonishing performance in terms of memory
usage and solve-time compared to the baseline approach laid out in
\cite{generate-parikh-image} when implemented on the same underlying automated
theorem prover (\Princess{}, \cite{princess}). It is also competitive with the
\Nuxmv{} model checker \cite{nuxmv}, outperforming it on unsatisfiable instances
and generally outperforming it for timeouts under 30 seconds with its advantage
increasing drastically for even shorter timeouts. 30 seconds would generally be
considered a long timeout for our intended use as supporting infrastructure to a
string constraint solver.

Future investigations involve two tracks. The first one is integration into
existing string solvers (wich \Ostrich{} being a particularly promising
candidate due to its shared use of \Princess{}), and further adaptation to that
use case. Closer inspection of the instances where we currently time out should
be useful to further improve our heuristics.

The second track for future improvements is the extension into other problem
domains, including other logics, model checking problems, as well as to more
powerful automata such as transducers. In principle, we are also already able to
express stronger constraints than Parikh automata, due to our use of a full
automated theorem prover which allows adding arbitrary constraints in addition
to the expected Presburger formulae.

%% Acknowledgments
% \begin{acks}                            %% acks environment is optional
%                                         %% contents suppressed with 'anonymous'
%   %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%   %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%   %% acknowledge financial support and will be used by metadata
%   %% extraction tools.
%   This material is based upon work supported by the
%   \grantsponsor{GS100000001}{National Science
%     Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%   No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%   No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%   conclusions or recommendations expressed in this material are those
%   of the author and do not necessarily reflect the views of the
%   National Science Foundation.
% \end{acks}


%% Bibliography
\bibliography{bibliography}


%% Appendix
\appendix
\section{Appendix}

\begin{lstlisting}[caption={An example input file for \Catra{} for the problem introduced in \cref{sec:motivation}, illustrating every major syntax element. From beginning to end: synchronised (product) automata using the keyword \texttt{synchronised} (automata A and B), labels (except those with ranges), register increments, and constraints on the final values of their counters.}, label=lst:input-example]
  // So far we only support integer counters. Note that we have individual
  // counters for each automaton to avoid surprises for product construction.
  counter int l_a, l_b, l_c, r_c;

  synchronised {
  automaton aca_or_bc {
    init S;
  
    // We use ASCII values here, this is for lowercase a
    S -> A [97] { l_a += 1 };
    S -> B [98] { l_b += 1 };
  
    B -> S  [98] { l_b += 1 };
    B -> final [99] { l_c += 1 };
  
    A -> A [99] { l_c += 1 };
    A -> final [97] { l_a += 1 };
  
    accepting final;
  };
  
  automaton something_c_something {
      init S;
  
      // Special short-hand value for the whole Unicode alphabet.
      S -> S [any] ;
      S -> F [99] { r_c += 1 };
  
      F -> F [any];
  
      accepting F;
  };
  };
  
  // Constrain the number of a:s to be larger than the number of c:s. Since the
  // automata are synchronised on every transition and counters are guaranteed
  // to be consistent these constraints are sufficient without also constraining
  // r_c.
  constraint l_a > l_c;
\end{lstlisting}

% Text of appendix \ldots

\end{document}
