%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[acmsmall,review,anonymous,screen]{acmart}\settopmatter{printfolios=true,printccs=true,printacmref=true}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[acmsmall]{acmart}\settopmatter{}


%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{POPL} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2023}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format must update the '\documentclass' and
%% topmatter commands above; see 'acmart-sigplanproc-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption



\usepackage{amsmath,empheq,fancybox}
\usepackage{paralist}
\usepackage{url}
\usepackage{color}
\usepackage{textcomp,listings}
\usepackage{array}
\usepackage{mymacros}
\usepackage{microtype}
\usepackage{listings}
\usepackage{csquotes}
\usepackage{proof}
\usepackage[capitalise]{cleveref}
\usepackage{algorithm2e}      
\usepackage{multirow}
\usepackage{mathpartir}
\usepackage{amsthm}
\usepackage{numprint}
\usepackage{ebproof}

\usepackage{mathtools} % Bonus


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\lstset{
    columns=fullflexible,
    showspaces=false,
    showtabs=false,
    breaklines=true,
    showstringspaces=false,
    breakatwhitespace=true,
    escapeinside={(*@}{@*)},
    commentstyle=\color{greencomments},
    keywordstyle=\color{bluekeywords},
    stringstyle=\color{redstrings},
    numberstyle=\color{graynumbers},
    basicstyle=\ttfamily\small,
    framesep=12pt,
    xleftmargin=12pt,
    tabsize=4,
    captionpos=b
}


\newif\ifcomments
\commentstrue
\newif\ifoutline
\outlinetrue

\newcommand{\contents}[1]{\ifoutline{\color{blue}
    \begin{itemize}
    #1
    \end{itemize}
  }\fi}

\allowdisplaybreaks[1]


\begin{document}

%% Title information
\title{A Constraint Solving Approach to Parikh Images of Regular Languages}
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}                   %% \country is recommended
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
  A common problem in string constraint solvers is computing the Parikh image, a
   set of linear equations that describe all possible combinations of character
   counts in strings of a given language. Automata-based string solvers
   frequently need to compute the Parikh image of large products (intersections)
   of nondeterministic automata, which in many operations is both prohibitively
   slow and memory-intensive. We contribute a novel understanding of how Parikh
   maps can be tackled as a constraint solving problem to solve real-world
   constraints stemming from functions on regular languages, most notably the
   length constraint. Furthermore, we show how this formulation can be
   efficiently implemented as a calculus, \Calculus{}, in an automated theorem
   prover supporting Presburger logic.  We implement \Calculus{} in a tool
   called \Catra{}, and evaluate it on constraints produced by the
   \OstrichPlus{} string constraint solver when solving Parikh automata
   intersection problems produced when solving standard string constraint
   benchmarks involving string length constraints. We show that our solution
   strictly outperforms the standard approach described
   in~\citeauthor{generate-parikh-image} as well as the over-approximating
   method recently described by~\citeauthor{approximate-parikh} by a wide
   marigin, and for realistic timeouts for constraint solving also the~\Nuxmv{}
   model checker.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>10003752.10003766.10003773.10003775</concept_id>
  <concept_desc>Theory of computation~Quantitative automata</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10003752.10003766.10003776</concept_id>
  <concept_desc>Theory of computation~Regular languages</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10003752.10003790.10003794</concept_id>
  <concept_desc>Theory of computation~Automated reasoning</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10003752.10010124.10010138.10010142</concept_id>
  <concept_desc>Theory of computation~Program verification</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10003752.10003790.10011192</concept_id>
  <concept_desc>Theory of computation~Verification by model checking</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  </ccs2012>  
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Quantitative automata}
\ccsdesc[300]{Theory of computation~Regular languages}
\ccsdesc[500]{Theory of computation~Automated reasoning}
\ccsdesc[300]{Theory of computation~Program verification}
\ccsdesc[300]{Theory of computation~Verification by model checking}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{Parikh images, string solvers, model checking}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}

The Parikh image is a characterisation of formal languages in terms of
their character counts. Given a language over an
alphabet~$\{a_1, \ldots, a_k\}$, the Parikh image is a set of
$k$-dimensional vectors that contains some
vector~$\VectorLiteral{m_1, m_2, \ldots, m_k}$ if and only if the
formal language contains a word in which each $a_i$ occurs $m_i$
times. It is a classical result that the Parikh image of every
context-free language (and, thus, also of every regular language) is a
semilinear set~\cite{parikh-theorem}, i.e., Presburger-definable.

Parikh images play a central role in many automata-based algorithms,
for instance and notably in today's string solvers, which often have
to process constraints that combine regular language membership with
word length. To decide whether a simple formula like
$x \in \Language_1 \wedge y \in \Language_2 \wedge |x| > |y|$,
with string variables~$x, y$ and regular
languages~$\Language_1, \Language_2$, is satisfiable, it is
necessary to reason about the sets of word lengths induced by
$\Language_1, \Language_2$, which are special cases of the Parikh
image.  This required combined reasoning about strings and string
length has long been identified as a major bottleneck in string
solvers
\cite{DBLP:conf/cav/AbdullaACHRRS15,length-aware-solver,approximate-parikh,DBLP:journals/corr/BerzishZG17}.
Other string solvers make use of Parikh automata~\cite{parikh-automata}, and thus
Parikh images in the general case, to handle operations including
\verb!str.substr!  and \verb!str.at!, which comes at an even higher
price in terms of computational complexity~\cite{ostrich-plus}.

\iffalse
It appears naturally as part of \Fudge{many operations} in model
checking and solving string constraints in automata-based solvers such
as \Ostrich{} \cite{ostrich}, notably in representing constraints on
string lengths. The enhanced \OstrichPlus{} solver~\cite{ostrich-plus}
makes even more extensive use of Parikh images.
\fi

It is possible to compute an existential Presburger formula describing
the Parikh image of any context-free language in linear
time~\cite{generate-parikh-image}; for the special case of regular
languages, this result was also stated in
\cite{muscholl-linear}. While theoretically elegant, this construction
has several disadvantages, often making it unpractical for integration
into algorithms. Firstly, the constructed Presburger formula contains
a linear number of existential quantifiers in the size of the
considered grammar, as well as complex Boolean structure, which is
needed to express the connectedness of sets of productions considered
in the construction. Eliminating those quantifiers to obtain a
quantifier-free representation of the Parikh image has exponential
complexity~\cite{DBLP:conf/issac/Weispfenning97}, and is often impossible in reasonable
time. Just solving the Parikh image membership problem is NP-complete,
as it corresponds to computing a satisfying assignment of the
existential Presburger formula, and taxing for solvers as
well~\cite{ostrich-plus}.

\iffalse
Later improvements have produced a construction taking at
most linear time to produce~\cite{muscholl-linear}. However, the
resulting existentially quantified clauses are costly to eliminate as
the number of variables increases, in practice making many real-world
problems intractable.
\fi

Secondly, in applications involving regular languages, it is typically
necessary to consider the Parikh image not only of a single automaton,
but of the intersection of multiple automata. This problem arises in
string solvers in particular, as conjunctions of string constraints
lead to the computation of length images of products (intersections)
of regular languages represented as finite automata. Applying the
approach in~\cite{generate-parikh-image} would in this case require
the eager computation of the product before its length image, and
result in an existential Presburger formula of exponential size (in
the number of automata). In several instances we have observed while
solving real-world string constraints, the computation of the product
of automata exhausts the memory of any machine available due to the
exponential blow-up in size of the product, quickly becoming
intractable as the number of automata in the product increases. The
current best published mitigation for this problem is an
over-approximation that works by approximating the Parikh image of a
product of automata to be the conjunction of the image of the
individual automata of the product \cite{approximate-parikh}. This
approach only works for unsatisfiable instances, and comes with a
harsh penalty for satisfiable instances.

Addressing these concerns, we have developed a calculus for Parikh images of
products of regular languages that we call \Calculus{}. It allows us to
interleave the computation of arbitrarily deep products of automata with the
product's Parikh image, and is generalised to an arbitrary homomorphism over
automata labels, including string lengths. This enables us to let both
calculations inform each other, eliminating unnecessary work, and pruning the
size of the partial products considered in the computation for a smaller memory
footprint. Moreover, the method can be used iteratively to tackle smaller chunks
of the product incrementally, thereby decreasing the memory footprint.

The key ideas of \Calculus{} is a combination of problem-aware case splitting,
lazy enforcement of automata connectivity, and lazy computation of products.

We implement \Calculus{} as a theory for the \Princess{} automated theorem
prover in the tool \Catra. \Catra{} also supports the approximate method of
\cite{approximate-parikh}, its fall-back variant adapted
from~\cite{generate-parikh-image}, and an adapter for the \Nuxmv{} model
checker~\cite{nuxmv}. Using \Catra, we compare \Calculus{} to the other two
back-ends on \NrBenchmarks{} Parikh automata intersection problems generated by
\OstrichPlus{} when solving the PyEx string constraint benchmark suite involving
string length constraints \cite{pyex}.

In summary, we contribute:
\begin{itemize}
\item The \Calculus{} calculus to efficiently compute (a homomorphism on) the
Parikh image of products of Parikh automata.
\item Experiments illustrating the performance of \Calculus{} on real-world examples from string solving, including \NrBenchmarks{} instances in a standardised format made available for future study.
\item The \Catra{} tool for solving such instances, containing an implementation of \Calculus{}, the over-approximation described in~\cite{approximate-parikh}, and an adapter for the~\Nuxmv{} model checker~\cite{nuxmv}.
\item Suggestions for how to efficiently implement \Calculus{} in a modern automated theorem prover, including strategies for case splitting, clause learning, and constraint propagation for connectedness.
\end{itemize}

\subsection{Related Work}

The problem of computing constraints on Parikh images over products of regular
languages under a given commutative homomorphism amounts to solving products of
Parikh automata. Parikh automata are regular automata extended with integer
counters with given increments and decrements for each transition, where we
allow checking a set of linear constraints on the final values of the counters
(but not their intermittent values) \cite{parikh-automata}. Parikh automata
without constraints on the final values on their registers are also sometimes
called cost-enriched automata, weighted automata or counter automata, depending
on exact definitions and side-constraints. The decision problem tackled in this
paper, determining the emptiness of an intersection of Parikh automata, was
recently shown to be PSPACE-complete~\cite{graph-queries}.

Parikh image computations as well as Parikh automata feature extensively in
string solvers, including as mentioned above \Ostrich{} and \OstrichPlus{}
\cite{ostrich,ostrich-plus}, but also forms the basis of Trau~\cite{trau-pldi},
and occurs in \textsc{Sloth}~\cite{sloth}. Parikh images frequently appear when
introducing cardinality constraints like length or string indexing. The
state-of-the-art approach to handling Parikh image computation is to
over-approximate the Parikh image of a product of $k$~automata
$\ParikhMap(\Language(\Automaton_1) \cap \ldots \cap \Language(\Automaton_k))$ with the
conjunction of the automata's parikh maps, $\bigwedge_{i=1}^{k}
\ParikhMap(\Automaton_i)$. This approach works only for unsatisfiable instances,
and will require falling back to computing the product of the automata before
using the standard approach for finding its image originally presented
in~\cite{generate-parikh-image}.

Outside of string solvers, Parikh automata have been proposed as the basis of
queries~\cite{graph-queries}, and for solving cardinalities in model checking
problems involving epistemic logic~\cite{epistemic-logic}.

Many other generalisations of the Parikh image than the projections we use here have been
studied. Prominent examples include generalising the Parikh map to segments of a
fixed length \cite{KARHUMAKI1980155} and the more general Parikh matrix, which
gives more information about a word than the standard Parikh image
\cite{parikh-matrix}. Another notable generalisation is the p-vector, introduced
in~\cite{infinite-words}, which denotes the position of each letter in the word
rather than the number of their occurrences and allows for generalisations into
infinite alphabets. All of these in some sense extend the Parikh map. By
contrast, the main utility of the formulation introduced here is that it allows
us to \emph{remove} something, thereby potentialy obtaining an easier problem.

\section{A demonstration of our approach}\label{sec:motivation}

To illustrate the key features of \Calculus, we will informally present our
reasoning. We begin by considering the following set of string and other
constraints, which we want to solve for all possible values of $\MonoidElement$,
a value in some commutative monoid and the string $s$, where
$\varphi(\MonoidElement)$ is some set of constraints on $\MonoidElement$:
\[
   s \in \Language(\mathtt{/}\SndRegex{}\mathtt{/})  \land
   s \in \Language(\mathtt{/}\FstRegex{}\mathtt{/})  \land
   \MonoidElement = f(s)  \land
   \varphi(\MonoidElement)
\]

We will represent the strings using the nondeterministic finite automata
 $\SomethingCSomething{}$ (\cref{fig:something-c-something}) and $\AcaOrBc{}$
 (\cref{fig:aca-or-bc}) respectively.

 Throughout these examples we will present our three key ideas:
 \emph{automata-aware splitting}, \emph{lazy enforcement of automata
 connectivity}, and \emph{incremental materialisation of products}.
 
\subsection{Counting String Lengths}

To keep things simple, in this first example we let $f$ be the function that
gives the length of a given string; $f\left(\text{curious and curiouser
still}\right) = 27$, and $f\left(\text{homo}\right) +
f\left(\text{morphism}\right) = f\left(\text{homomorphism}\right) = 12$, and we
let $\varphi(\MonoidElement) = \exists k \HoldsThat m = 2k + 1 \land k \geq 0$,
i.e. require that the length of the string is odd.

\begin{figure}[p]
  \centering 
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[scale=\autscale]{aca_or_bc}
    \caption{An automaton recognising the regular expression
      $\FstRegex{}$.}\label{fig:aca-or-bc}
      \Description[An NFA recognising $\FstRegex{}$]{An NFA recognising $\FstRegex{}$. It has 4 states.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth} 
    \centering
    \includegraphics[scale=\autscale]{something_c_something}
    \caption{An automaton recognising the regular expression
    $\SndRegex{}$.}\label{fig:something-c-something}
    \Description[An NFA recognising $\SndRegex{}$]{An NFA recognising $\FstRegex{}$. It has 2 states.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[scale=\autscale]{aca_or_bc_simplified}
    \caption{$\AcaOrBc{}$ with its symbolic transition counters after
    simplification of their equations.}\label{fig:example-simplify-1}
    \Description[The automaton $\AcaOrBc{}$ labelled with variables counting the number of times each transition is taken, after having simplified the equations.]{The starting state shows an initial flow of 1, with two outgoing transitions to $A$ ($1-l_c'$ on letter a, and $l_b' + l_c'$ on b respectively). $A$ has a flow of $l_c$, and its outgoing transition into the final state, $F$ is the same as the incoming one from $S$, the initial state. The $B$ transition has an outgoing transition labeled $c/ l_c'$ to the final state $F$, and one backwards transition $b / l_b'$ to the starting state.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[scale=\autscale]{something_c_something_length_simplified}
    \caption{$\SomethingCSomething{}$ after using linear reasoning over lengths to replace a variable.}\label{fig:example-length-reduced}
    \Description[The automaton $\SomethingCSomething{}$ after reasoning over lengths.]{The same automaton as before for $\SomethingCSomething{}$, but with the transition from start to final annotated with a constant number 1, and the leftmost self-loop transition replaced with the equation $1 + l_c + 2l_b' -
    r_\Sigma'$.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
      \includegraphics[scale=\autscale]{aca_or_bc_length_split_1}
      \caption{$\AcaOrBc{}$ at the split where $\FromLabelTo{S}{a}{A}$ is not
      used.}\label{fig:aca-or-bc-length-split-1}
      \Description[The automaton \AcaOrBc{} where we have removed the transition
      from $S$ to $A$.]{ The state $A$ hangs free without incoming transitions,
      and only the lower part of the automaton remains.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
      \includegraphics[scale=\autscale]{aca_or_bc_length_split_2}
      \caption{$\AcaOrBc{}$ at the split where $\FromLabelTo{S}{a}{A}$ \emph{is} used.}\label{fig:aca-or-bc-length-split-2}
      \Description[$\AcaOrBc{}$ with the transition from $B$ to $F$ removed.]{$\AcaOrBc{}$ when we keep the transition from $S$ to $A$. This removes the transition between $B$ and $F$, but otherwise maintains the automaton.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
      \includegraphics[scale=\autscale]{length_split_product}
      \caption{The product of $\AcaOrBc$ and $\SomethingCSomething$ after
      case splitting and removal of dead states.}\label{fig:length-split-product}
      \Description[$\AcaOrBc{}$ with the transition from $B$ to $F$ removed.]{$\AcaOrBc{}$ with the transition from $B$ to $F$ removed. There now remains only one path to an accepting state.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
      \includegraphics[scale=\autscale]{aca_or_bc_simplified_3}
      \caption{$\AcaOrBc{}$ after further constraint propagation.}\label{fig:example-simplify-3}
      \Description[$\AcaOrBc$ without the lower section.]{$\AcaOrBc$ with only its upper section left and the $a$ transitions now set to static 1.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.6\textwidth}
    \centering
        \includegraphics[scale=\autscale]{aca_or_bc_times_something_c_something}
        \caption{$\AcaOrBc{} \times \SomethingCSomething{}$ under the constraint
        $\TransitionCount{a} > \TransitionCount{b}$.}\label{fig:final-example} \Description[The
        final product, still a boring stick automaton with only one loop for an
        arbitrary number of c's.]{A product of the intermittent automata, which
        accepts \lstinline{ac.*ca}}
  \end{subfigure}
  \caption{The collection of automata we use as running examples in this paper, with annotations illustrating intermittent states of our calculus.}    
  \end{figure}

The na\"ive approach would be to first compute the product of the automata, then
use the approach from \cite{generate-parikh-image} to find its image under $f$,
and then solve the constraints in an automated theorem prover. However, we can
take advantage of both the constraints and definitions of $f$ to prune the
automata before computing the product and help reduce the blow-up.

Similar to \cite{generate-parikh-image} we associate each transition
$\Transition$ with an integer variable $\TransitionVar_\Transition \geq 0$
representing how many times $\Transition$ is used that we leave implicitly
existentially quantified. These variables are shown as annotations under the
transition labels in \cref{fig:aca-or-bc,fig:something-c-something}. We then
apply graph-like flow reasoning by adding linear equations (e.g. $1 + {l_b}' =
l_a + l_b$, or $l_a + l_c' = 1$) that equate the incoming transitions into a
state to the ones going out. For the initial and accepting states we add an
incoming and outgoing flow of 1, respectively. We will use these linear
equations to perform reasoning to reduce the size of the product before we
compute it. Note that we have already performed some linear reasoning to
eliminate some of the variables in the graphs above, for example to introduce
the constant number $1$ in \cref{fig:something-c-something}, which comes from $1
+ r_\Sigma = x + r_\Sigma$ solved for $x$.

We sum the individual transition variables to get the length $\MonoidElement$ we
are actually interested in. For example the image of $f$ on
$\SomethingCSomething$ is given by $\exists_{r_\Sigma, r_\Sigma'} \: 2k + 1 =
r_\Sigma + 1 + r_\Sigma' \land k \geq 0$. For $\SomethingCSomething{}$, this is
all we need to produce the image since all transitions are always accessible
from any path from the initial state to the accepting state. The difficult
connectivity constraint is not necessary to enforce here, since any path through
$\SomethingCSomething{}$ is connected, and therefore the flow equations
described above are sufficient to capture its image.

The story gets more complicated with $\AcaOrBc{}$, where we have a choice of two
paths, up or down, and where that choice affects the possibility of taking
either of the two loops. In that case, we would have the image $2l_a + l_c + l_c' + l_b + l_b'$.
If we apply standard linear reasoning on the equations (e.g. $1 + {l_b}' = l_a +
l_b$, or $l_a + l_c' = 1$), we can reduce the number of bound
($\exists$-quantified) variables somewhat, obtaining the relations in
\cref{fig:example-simplify-1}, and the corresponding over-approximation on
the $f$-image $2 + l_c + 2l_b'$. It is an over-approximation because it only
considers the number of times each transition is taken and the existence of a
path from an initial to an accepting state, but omits the connectivity of that
path in the presence of loops. For example, no valid path would have both $l_c >
0$ and $1 - l_c' = 0$, but no such restriction exists in the formula above.

Still, this approximate view allows us to perform further reasoning. Since we
know that the length of a string accepted by the intersection of the two
automata's languages must be the same, we also know that their images must be
the same under $f$, e.g. that $r_\Sigma + 1 + r_\Sigma' = 2 + l_c + 2l_b' \iff
r_\Sigma + r_\Sigma' = 1 + l_c + 2l_b'$. This in turn allows us to replace one
additional variable, e.g. $r_\Sigma = 1 + l_c + 2l_b' - r_\Sigma'$ (see
\cref{fig:example-length-reduced}). Note that the two automata are now in
contact, allowing us to propagate reasoning on one to the other.

Since we cannot take a transition a negative number of times, we have an
implicit constraint on any transition variable that it must be at least $0$.
This means that we have $1 + l_c + 2l_b' - r_\Sigma'$, implying an upper bound
on $r_\Sigma'$: $r_\Sigma' \leq 1 + l_c + 2l_b'$.

This is as far as we can get with this reasoning. We must now choose: either we
compute (materialise) the product and continue our reasoning, or we perform a
case split. Let us try a case split in order to put off computing a potentially
large product.

We apply the principle of splitting and select an early transition in the
largest automaton with loops, in our case $\FromLabelTo{S}{a}{A}$ of $\AcaOrBc$.
This creates two cases: $1-l_c' = 0$ and $1-l_c' > 0$. We start with the first
case, and after propagating the now known values and removing any transition
that becomes zero, we get the automaton of \cref{fig:aca-or-bc-length-split-1}.

We immediately notice that the self-loop of state $A$ is now disconnected from
the initial state and must be removed. This condition can be detected
efficiently using standard forwards/backwards reachability from the initial
and accepting states respectively. In other words, $2k+1 = 0$ for this case
and is implied by the splitting constraint. However, no choice of integer $k$
will satisfy this equation. Therefore, we close this branch and proceed with
the other one -- where $1-l_c' > 0$. Note that we do so without ever computing
a potentially expensive product of automata.
  
Plugging in the assumption $1-l_c' > 0$ (and its consequence that $l_c' = 0$),
we also get a reduced variant of $\AcaOrBc$, as seen in
\cref{fig:aca-or-bc-length-split-2}. Note that there is now no way to select any
transition variable to disconnect a loop from a path between $S$ and $F$,
and we can therefore stop worrying about enforcing connectivity at all.

Before continuing, we need to tie the inequalities for the transitions of the
automata of the product to the new transition counting variables we introduced
in the product automaton to ensure that they are consistent. In our case, we
obtain the following equations from the principle that a transition variable in
either automaton must be equal to the sum of variables in the resulting edges of
the product, which gives the following two interesting inequalities from
$\AcaOrBc{}$'s transition $\FromLabelTo{A}{c}{A}$: $2k + 1 = r_1 + r_2 + 1$

Note how this conserves the requirement that the number of loop iterations be
odd even when the same original loop appears three times in the product, twice
as a self-loop and once as a regular state transition. Using the same principle
as before for computing the length by adding up the transition counters, we
obtain the quantified formula $1 + 2k + 1 + 1 + r_b + r_b= 3 + 2k + 2r_b$ for
the length. Eliminating the final quantifier gives us the image of $f(\AcaOrBc
\times \SomethingCSomething)$ modulo odd lengths: $3 + 2k$, for some integer
$k$.

\subsection{Counting Letters}\label{sec:introduction:parikh}

Now assume that rather than the length (e.g. the count of the characters) we
want to find the whole Parikh image of the intersection of $\AcaOrBc{}$ and
$\SomethingCSomething{}$. In other words, now $f$ is a mapping to a 3-vector
counting how often each character occurs in a string. Since extracting values
from a vector is tiresome, we will use the shorthand notation
$\TransitionCount{a}$ to refer to the number of letters a that appear. E.g.
$\TransitionCount{a}(\text{curious and curiouser still}) = 1$. Note that we
still have the property that $\TransitionCount{o}(\text{homomorphism}) =
\TransitionCount{o}(\text{homo}) + \TransitionCount{o}(\text{morphism}) = 2 + 1
= 3$. If you treat the addition as element-wise vector addition, this approach
works for the vector version as well.

For our constraint, we will use $\TransitionCount{a} > \TransitionCount{b}$,
that is finding a count of characters in a string accepted by both automata where
there are more a:s than b:s. Since the underlying calculus is the same, we start immediately from \cref{fig:example-simplify-1}.

  We now need to relate the number of times a transition was taken to the
  character counts we are really interested in, as opposed to just the length in
  the previous example. For the $\Sigma$ labels, since we do not know which
  letter to use, we introduce a sum $r_{a1} + r_{b1} + r_{c1}$ per $\Sigma$
  transition.

  As in the previous example, we sum the occurrences for each transition to
  obtain the counts, here with the element-wise vector addition unpacked:
  \[
\MonoidElement =   \begin{bmatrix}
  r_{a1} + r_{a2} \\
  r_{b1} + r_{b2} \\
  r_{c1} + r_{c2} + 1 \\
  \end{bmatrix} =
  \begin{bmatrix}
    2 -2l_c' \\
    2l_b' + l_c' \\
    l_c + l_c' \\
    \end{bmatrix} =
    \begin{bmatrix}
      \TransitionCount{a} \\
      \TransitionCount{b} \\
      \TransitionCount{c}
      \end{bmatrix}
  \]
  
These equalities can be combined with the constraint that $\TransitionCount{a} >
\TransitionCount{b}$ to obtain $2 - l_c' > l_b'$. Since we have $1-l_c' \geq 0$
from one of the transition labels, we know $l_c' \geq 0$, and therefore that $1
> l_b' \implies l_b' = 0$. Plugging in the same inequality again on the same
principle, we get $2 - 2l_c' > l_c' \iff l_c' = 0$. This gives us the much
smaller automaton of \cref{fig:example-simplify-3}.

% \begin{figure}[t]
%   \centering 
%     \includegraphics[width=0.70\linewidth]{aca_or_bc_simplified_2}
%     \caption{$\AcaOrBc{}$ after removing the now unused b-transition.}\label{fig:example-simplify-2}
%     \Description[$\AcaOrBc$ without the returning transition from $B$ to $S$]{$\AcaOrBc$ without the returning transition from $B$ to $S$, which now makes the transition from $S$ to $B$ also have the same associated variable $l_c'$.}
%   \end{figure}

This will produce a straightforward product since both automata now have the
same shape, where we essentially only need to expand the self-loops of
$\SomethingCSomething$ and reduce its range labels to only capture a's, as seen
in \cref{fig:final-example}. Note that we got this result by only performing
linear inequality reasoning on the automata of the product versus the number of
times each transition would be used.

%   \begin{figure}[t]
% \centering
%     \includegraphics[width=0.75\linewidth]{original_product}
%     \caption{$\AcaOrBc{} \times \SomethingCSomething{}$ as it would have
%     appeared if we had computed it from the initial
%     automata.}\label{fig:original-product}
%   \end{figure}

To obtain the final Parikh image, we again need to tie the inequalities for the
transitions of the automata of the product to the new transition counters we
introduced in the product automaton. The only interesting transition is the one
carrying the c, so we will choose that one for illustration. In that case, the
bridging equation is $r_2 = l_c \land r_2 = r_{\Sigma}$ (and for all others
$=1$), which gives the Parikh image $\exists_{r_2} \TransitionCount{a} = 1 \land
\TransitionCount{b} = 0 \land \TransitionCount{c} = r_2 + 1$, and after
existence-elimination using reasoning on lower bounds (e.g. $r_2 \geq 0$) we
arrive at $\TransitionCount{c} \geq 1$.

\section{Preliminaries}

\subsection{Monoids}

A monoid $\Monoid = \Tuple{X;\MonoidOp;0_{\Monoid}}$ is an algebraic structure
consisting of the non-empty carrier set of elements, $X$, an associative binary operation
$X \times X \rightarrow X$ denoted as $\MonoidOp$, that is where for all $a, b,
c \in X$, $(a \MonoidOp b) \MonoidOp c = a \MonoidOp (b \MonoidOp c)$. Finally,
$\Monoid$ must have an identity element $0_{\Monoid} \in X$ such that
$0_{\Monoid} \MonoidOp a = a \MonoidOp 0_{\Monoid} =   a$ for every $a \in X$.
We sometimes use integer multiplication to represent repeated application of
$\MonoidOp$, e.g. $3a = a \MonoidOp a \MonoidOp a$, for $a \in X$. $\Monoid$ is
called \textit{commutative} if $\MonoidOp$ also commutes, that is if $a
\MonoidOp b = b \MonoidOp a$ for all $a, b \in X$. 

Finally, a \textit{homomorphism} is a structure-preserving map between two monoids
$\Monoid_1, \Monoid_2$, that is a map $\Map : S_1 \rightarrow S_2$ such that $\Map(a
\MonoidOp_1 b) = \Map(a) \MonoidOp_2 \Map(b)$, where $S_1, S_2$ are the carrier
sets of $\Monoid_1$ and $\Monoid_2$ respectively, and $\MonoidOp_1, \MonoidOp_2$
their binary operations.

\subsection{Languages, Finite-state Automata and their Products}

We define an alphabet as a finite set of symbols $\Alphabet$ with words $\Strings$, and
the concatenation operation as $s_1 \Concat{} s_2$ over two strings $s_1, s_2$.
Note that $\Strings = \Tuple{\Alphabet;\Concat{};\epsilon}$, is a
non-commutative monoid, referred to as the free monoid. The string length
function, $\Length{s}$ is an example of a homomorphism between $\Strings$
and~$\mathbb{Z}$.

An automaton~$\Automaton$ with alphabet~$\Alphabet$ is
$\AutomatonTuple$ where $\Transitions \subseteq \States \times \Alphabet \times
\States$, $\States$ is its states, $\InitialState$ its
initial state, and $\AcceptingStates$ is its set of accepting states.  We
write a transition $\Transition = \Tuple{\State, \Label, \State'} \in
\Transitions$ as $\Transition = \FromLabelTo{\State}{\Label}{\State'}$.
Similarly, we use the notation $\FromLabelTo{\State}{}{}$ to refer to the set of
transitions starting in $\State$, and $\FromLabelTo{}{}{\State}$ to refer to the
set of transitions coming into $\State$, whenever the automaton is clear from
the context.

We will let variables $\Transition, \Transition', \Transition_1, \ldots,
\Transition_n$ etc describe transitions, $\State, \ldots, \State_n$ states, and
$\Automaton, \ldots, \Automaton_n$ automata, and use subscript indexing
($\Transitions_\Automaton$) to refer to the transitions, states, etc of a given
automaton.

By a \emph{product} of two automata $\Automaton_1, \Automaton_2$, written
$\Automaton_1 \times \Automaton_2$, we mean an automaton constructed to run
$\Automaton_1$ and $\Automaton_2$ in parallel on an input and only accept the
input if both automata would do so.

We refer to the resulting product states as tuples, $\Tuple{\State, \State'}$,
which represent the state of the product automaton where $\Automaton_1$ would be
in $\State$ and $\Automaton_2$ would be in $\State'$. Note that since we use
ordered tuples the product is technically (but w.l.o.g) not commutative; the
left-hand-side must come from the left-hand term. The sole purpose of this
matching is to allow us to speak with precision about the origins of components
in a product.

\subsection{The Parikh Map and its Image}
Formally, the \textit{Parikh map} over an alphabet $\Alphabet=
\left\{a_1, \ldots, a_k \right\}$ is defined as in \cite{kozen}:
$$
\begin{aligned}
& \ParikhMap: \MapFromTo{\Strings}{\natural^k} \\
& \ParikhMap(s) = \VectorLiteral{\#a_1(s), \#a_2(s), \ldots, \#a_k(s)}
\end{aligned}
$$

That is, $\ParikhMap(s)$ is a vector of the number of occurrences of each
character in the language for a given string $s$. For example, for  $\Alphabet =
\Set{a, b}$, we would have $\ParikhMap(abb) = \VectorLiteral{1, 2}$.

We define the image of this map, the \textit{Parikh image}, of some subset of
the language $\Language \subseteq \Strings$ as:
\[
\ParikhMap(\Language) = \Set{\ParikhMap(x) \SuchThat x \in \Language}
\]

Thus, we would have $\ParikhMap(\left\{ab, abb\right\}) = \left\{\left[1,
1\right], \left[1, 2\right]\right\}$. We also sometimes use the standard
notation $\#l(w)$ to talk about an individual letter $l$ in a word $w$. For
example, for the Parikh vector above, we would have $\CountOf{a} = 1$. You have
already seen this in \cref{sec:introduction:parikh}.

Parikh's theorem states that any context-free language has a letter-equivalent
regular language (c.f.~\cite{construction} for a construction of such automata
from context-free grammars and~\cite{bounds} for bounds on its size). However,
there are languages that are not context-free that also have semilinear images
under~$\ParikhMap$ (e.g. $\ParikhMap(\Set{a^nb^nc^n \SuchThat n \geq 0}) =
\ParikhMap((abc)^*) = \CountOf{a} = \CountOf{b} = \CountOf{c} \land \CountOf{a}
\geq 0$). This means they can be represented as a quantifier-free Presburger
formula.

Note that while Parikh's theorem applies to context-free languages, in this
paper we focus only on regular languages.

\subsection{The Parikh image of a regular language expressed in Presburger arithmetic}
\label{sec:verma}

Since Parikh images are semilinear, any Parikh image can be written as a set of linear
equations. The following construction for an NFA $\Automaton = \AutomatonTuple$ is based on the version for context-free grammars presented in \cite{generate-parikh-image}:
\begin{equation}
\begin{aligned}
\ParikhMap(\Automaton) := 
& \AndComp{\Letter \in \Alphabet}{\LetterVar_{\Letter} = \sum_{\Transition = \FromLabelTo{}{\Letter}{} \in \Transitions} \TransitionVar_{\Transition}
}\\
& \AndComp{\State \in \AcceptingStates}{\FinalStateVar_{\State} > 0 \longrightarrow  \StateVar_{\State} > 0}\\
& \AndComp{\State \in \States}{
  \left(\FinalStateVar_{\State} \text{ if } \State \in \AcceptingStates \text{ otherwise } 0 \right) +
  \sum_{\Transition \in \FromLabelTo{\State}{}{}} \Filter(\Transition) - \sum_{\Transition \in \FromLabelTo{}{}{\State}} \Filter(\Transition)
= \begin{cases}
    1 \text{  if $\State = \InitialState$} \\
    0 \text{ otherwise}
  \end{cases}
}\\
& \AndComp{\Transition = \FromLabelTo{\State'}{}{\State} \in \Transitions}{
  \TransitionVar_{\Transition} > 0 \longrightarrow \StateVar_{\State} > 0
} \\
& \AndComp{\State \in F}{
  \StateVar_{\State} > 0 \longrightarrow \FinalStateVar_{\State} = 1
} \\
&\AndComp{\State \in \States}{
  \StateVar_{\State} > 0 \longrightarrow
  \OrComp{\Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions}{
    \StateVar_{\State} = \StateVar_{\State'} + 1 \land 
    \TransitionVar_{\Transition} \geq 1 \land
  \StateVar_{\State'} \geq 1
    }
}
\end{aligned}
\label{eq:generate-parikh}
\end{equation}

All variables $\TransitionVar_\Transition, \StateVar_\State, \FinalStateVar_\State$ are
existentially quantified and the free variables $\LetterVar_\Letter$ make up the
image. $\StateVar_\State$ represents the distance of state $\State$ from
$\InitialState$ in a spanning tree, $\TransitionVar_\Transition$ how many times
a transition $\Transition$ is used, and $\FinalStateVar_\State$ whether a given
accepting state $\State \in \AcceptingStates$ is the actually used final state.

In this paper we refer to this model as the baseline approach, though we also
apply optimisations as described in \cref{sec:implementing-baseline}. The
calculus introduced in this paper, by contrast, lazily enforces the
connectedness constraint of this encoding (the final three clauses) while also
interleaving the computation of products of automata and propagating information
between the steps to reduce the amount of work that needs to be done.
  
\subsection{Theorem Proving}

We assume basic familiarity with first-order logic, Presburger
arithmetic, and the classical sequent calculus. For reference, see
e.g. \cite{Fitting96a}.

\section{Projections on Parikh Images}\label{sec:generalised}

It is easy to see that the Parikh map~$\ParikhMap$ represents a
homomorphism from the (free) non-commutative monoid~$\Strings$ to the
(free) commutative monoid~$\Naturals^k$. As we are often not
interested in the full Parikh image, but only in some projections of
it. We consider \emph{arbitrary} homomorphisms
$\Map:\: \Sigma^* \to \Monoid$, where
$\Monoid = \left(X;\MonoidOp;0_{\Monoid}\right)$ is a commutative
monoid. We give several
examples of such projections on Parikh images later in this section.

Observe that every homomorphism~$\Map:\: \Sigma^* \to \Monoid$ can be
represented as the composition~$h' \circ \ParikhMap$, for some
homomorphism~$h' : \Naturals^k \to M$. One of the insights underlying our
approach is that it is more efficient to directly compute a
projection on the Parikh image~$\Map(\Language)$, than to first compute the
standard image~$\ParikhMap(\Language)$ followed by projection to
some property of interest.

%\subsection{Applications of generalised Parikh images}\label{sec:applications}

\subsection{Example~1: Reasoning about String Length}

One such simplifying homomorphism can express string
length, the problem that originally motivated our study of the Parikh
map. This mapping is relevant when solving constraints that combine
language membership as well as word length, for instance the formula
\begin{equation}\label{eq:stringLength}
x \in \Language_1 \wedge y \in \Language_2 \wedge |x| > |y|
\end{equation}
mentioned in the introduction. To solve this formula, let
$\Monoid = \Naturals$, and define the homomorphism~$L$ by $L(a) = 1$
for all characters~$a \in \Alphabet$. The length of a string
$s = s_1 \RepeatSum{\Concat} s_n$ is given by
$L(s) = \sum L(s_i) = 1 \RepeatSum{+} 1 = n$, and to solve
\eqref{eq:stringLength} we can instead solve the equi-satisfiable
formula~$\alpha \in L(\Language_1) \wedge \beta \in L(\Language_2)
\wedge \alpha > \beta$. This paper proposes efficient native
procedures to reason about membership constraints
like~$\alpha \in L(\Language_1)$, avoiding the computation of
the complete image~$L(\Language_1)$.

\subsection{Example~2: Generalised String Constraints with Integer
  Datatype}\label{sec:parikh-automata}

Parikh images are also applicable for deciding more general classes of
string constraints~\cite{ostrich-plus}. Consider, for instance, a
constraint involving the substring operation:
\begin{equation}
  \label{eq:substring}
  x \in \Language_1 \wedge \; 0 \leq n \leq m \leq |x| \wedge x[n:m] \in \Language_2
\end{equation}
in which $x$ is a string variable, $n, m$ are integer variables, and
$x[n:m]$ denotes the substring of $x$ starting at position~$n$ running
until position~$m$. This constraint belongs to an expressive fragment
of string logic that cannot be decided by most state-of-the-art string
solvers. To illustrate the decision procedure proposed in
\cite{ostrich-plus} for string constraints of this kind, suppose that
$\Language_2$ is defined by the regular
expression~$\mathtt{(ab|c)*}$. An automaton recognising this language
is shown in \cref{fig:parikh-automata} (left).

We can model \eqref{eq:substring} using the notion of a \emph{Parikh
  automaton}~\cite{parikh-automata,expressiveness}. A Parikh automaton
is an automaton in which transitions are labelled both with characters
from the alphabet~$\Alphabet$, and with offset vectors defining the
increments of a finite number of counters. This means that Parikh
automata recognise words over an extended
alphabet~$\Alphabet \times D$, where $D \subseteq \Naturals^d$ is a
finite set of increment vectors (notation as
in~\cite{expressiveness}). We use the symbols~$\pi_\Alphabet, \pi_D$
to denote projections to the first and the second component of a
composite letter~$(a, \Vector{v})$, respectively, and extend those
projections to words:
\begin{equation*}
  \pi_\Alphabet((a_1, \Vector{v}_1) \RepeatSum{\Concat} (a_k, \Vector{v}_k))
  ~=~ a_1 \RepeatSum{\Concat} a_k,
  \qquad
  \pi_D((a_1, \Vector{v}_1) \RepeatSum{\Concat} (a_k, \Vector{v}_k))
  ~=~ \Vector{v}_1 \RepeatSum{+} \Vector{v}_k.
\end{equation*}

\begin{definition}\label{def:parikh-automata} A Parikh automaton of dimension $d
  \geq 0$ is a pair $\Tuple{\Automaton, C}$, where
  $C \subseteq \Naturals^d$ is a semi-linear set (or, equivalently, a
  Presburger formula), and $\Automaton$ is a finite automaton with the
  alphabet $\Alphabet \times D$, where $D \subseteq \Naturals^d$. We
  say that $\Tuple{\Automaton, \varphi}$ recognises a
  word~$w \in \Alphabet^*$ if and only if the automaton has a run
  accepting an extended word~$w' \in (\Alphabet \times D)^*$ such that
  $\pi_\Alphabet(w') = w$ and $\pi_D(w') \in C$.
\end{definition}

    
\begin{figure}[t]
  \centering
  \raisebox{5ex}{
    \includegraphics[scale=0.9]{counter_automaton_pre}
    }
  \hfill
  \includegraphics[scale=0.9]{counter_automaton}
  \caption{An automaton recognising the language $\mathtt{(ab|c)*}$
    (left), and a Parikh automaton describing the pre-image of this
    language under the substring operation~$\cdot[n:m]$ (right). For
    sake of presentation, the Parikh automaton contains
    $\epsilon$-transitions, which could be eliminated in the standard
    way.}\label{fig:parikh-automata}
    \Description[An 2-state automaton (left) and a slightly larger automaton with counters (right).]{
      The rightmost automaton is the starts with an initial state with a self-loop on any character that increments both registers by one, followed by an epsilon transition to a state with a self-loop on c that increments the lower register by one. From that state, the automaton has an epsilon-transition into the accepting state, as well as a transition on a into a state with only a transition back again, both incrementing the lower register by one. The accepting state has a self-loop on any character that does nothing to the registers.
      }
    \end{figure}

    Applied to \eqref{eq:substring}, the decision procedure in
\cite{ostrich-plus} will construct a pre-image of $\Language_2$ under
the substring operation~$\cdot[n:m]$, and check whether this pre-image
is consistent with the constraint~$x \in \Language_1$. Because the
substring operation depends on the values of the integer
variables~$n, m$, a Parikh automaton of dimension~$2$ is a suitable
formalism to describe the pre-image, resulting
in the automaton in Fig.~\ref{fig:parikh-automata} (right). The first
component of the increment vectors is used to count the number of
letters eliminated in the beginning of the string (value~$n$), while
the second component records the beginning of the eliminated suffix
(value~$m$).

Denoting the language described by Fig.~\ref{fig:parikh-automata}
(right) as $\Language_{pre}$, we can then replace \eqref{eq:substring}
with an equi-satisfiable formula that no longer contains any explicit
substring operation:
%
\begin{equation}
  \label{eq:parikh-constraint}
  p \in \Language_{pre} \wedge~
  \pi_\Alphabet(p) \in \Language_1
  \wedge~~ \pi_D(p) =
  \begin{bmatrix}
    n \\ m
  \end{bmatrix}~~
  \wedge 0 \leq n \leq m \leq |p|
\end{equation}
%
To check the satisfiability of \eqref{eq:parikh-constraint}, we need a
decision procedure that can process intersections of regular languages
(in this case, of $\Language_{pre}$ and $\Language_1$, synchronising
on $\Alphabet$), while imposing the side
condition~$0 \leq n \leq m \leq |p|$ on the increment sum. In
\cite{ostrich-plus}, this decision procedure turned out to be main
bottleneck of the string solver, which was one of the motivations to
develop the lazy algorithm proposed in this paper.

\iffalse
\begin{figure}[t]
  \centering
      \includegraphics[width=0.5\linewidth]{parikh_automaton}
      \caption{The automaton part of a Parikh automaton for $\AcaOrBc{}$ with
      $\Alphabet = \Set{\text{a, b, c}}, d = 3$. The semilinear set/Presburger
      formula containing the constraints on the final register values cannot be
      visualised.}\label{fig:parikh-automaton}
    \end{figure}
\fi

\section{A Calculus for Projections on Parikh Images}\label{sec:calculus}

We start by defining our calculus, \Calculus{}, for one automaton, and only
extend it to products of automata in \cref{sec:multiple}. Assume an NFA
$\Automaton = \AutomatonTuple$ with $\NrTransitions$ transitions $\Transitions =
\Set{\EllipsisSequence{\Transition}{\NrTransitions}}$. For convenience, we then
introduce the following supporting notations:

\begin{definition}
  A \textit{path} $\Path = \PathEnumeration$ of an automaton~$\Automaton$ with
  transitions $\Transitions$ represents a path through $\Automaton$ using
  transitions in $\Transitions$ (i.e. $\Transitions(\State_k, \Label_{k+1},
  \State_{k+1})$ holds), passing zero or more labels $\Label_1, \ldots,
  \Label_n$. The path must begin in the initial state, i.e.~$\State_0 =
  \InitialState$. The end state, $\State_n$, is not necessarily accepting. If it
  is, we say the path is also \textit{accepting}.
  \end{definition}

\begin{definition}
  Moreover, we talk about the \textit{set of paths} of an automaton,
  $\Paths(\Automaton)$, a possibly infinite (if $\Automaton$ has loops) set of
  valid paths through $\Automaton$. Additionally, we use the
  notation~$\Paths(\Automaton, \State)$ to mean all paths ending in
  state~$\State$.
\end{definition}

\begin{definition}
  For a path $\Path = \Tuple{\State_0 \Label_1 \State_1 \RepeatSum{,}
  \State_n}$, its \textit{word} $\WordOf(\Path) = \Label_1 \RepeatSum{\Concat}
  \Label_n$ is the word read out on its labels.
\end{definition}

\begin{definition}
  The \textit{states} of a path $\Path = \Tuple{\State_0 \Label_1 \State_1
  \RepeatSum{,} \State_n}$, $\StatesOf(\Path) = \Set{\State_0\RepeatSum{,}
  \State_n}$ are the states visited along $\Path$. Note that $\InitialState \in
  \StatesOf(\Path)$ for every path since all paths start in the initial state.
\end{definition}

\begin{definition}
  A \textit{cut}, $C$ of an automaton~$\Automaton = \AutomatonTuple$ to state
  $\State$, written $\SeparatingCut(C, \Automaton, \State)$, is a minimal set of
  transitions such that every accepting path $p$ where $\State \in
  \StatesOf(\Path)$ contains a transition $\Transition \in C$. A cut does not
  exist for every state; notably it never exists for $\InitialState$.
\end{definition}

\begin{definition}
 The \textit{transition count}, $\TransitionCount(\Transition, \Path)$ is the
 number of times a transition $\Transition =
 \FromLabelTo{\State_1}{\Label}{\State_2} \in \Transitions$ appears in a path
 $p$.
\end{definition}

We then introduce the two predicates into our calculus with the following
definitions:

\begin{definition}\label{def:single-image}
  The Parikh predicate, $\SinglePredicateInstance$, for some automaton
  $\Automaton = \AutomatonTuple$, modulo some map $\Map : \Alphabet^* \to \Monoid$ to a
  commutative monoid $\Monoid$ as described in \cref{sec:generalised}
  and with a transition selection function
  $\Filter:\: \Transitions \to \Naturals$ holds when
  $\MonoidElement \in \Monoid$ is an element of the Parikh image of
  $\Language(\Automaton)$
  modulo $\Map$, or more formally when there is an accepting path
  $\Path = \PathEnumeration \in \Accepting{\Paths(\Automaton)}$ such
  that $\Filter(\Transition) = \TransitionCount(\Transition, \Path)$
  for all $t \in \Transitions$, and $\MonoidElement = \Map(\WordOf(\Path))$.
\end{definition}

\begin{definition}
  $\Connected(\Automaton, \Filter)$ for some automaton $\Automaton =
  \AutomatonTuple$ holds when for every $\Transition =
  \FromLabelTo{\State}{}{\State'} \in \Transitions$, $\Filter(\Transition) > 0
  \implies \exists \Path \in \Paths(\Automaton)$ $\Filter(\Transition_i) > 0$
  for $\Transition_i \in p$, and $\State \in \StatesOf(p)$, or in words that
  there exists some $\Filter$-selected valid path that reaches $\Transition$'s
  starting state, $\State$. Intuitively, it represents the condition that
  $\Automaton$ is connected with respect to the selection function $\Filter$ for
  every transition. It is redundant to $\Image$ by design.
\end{definition}

We present the rules of \Calculus{} for one automaton in
\cref{tbl:rules:single}. The rules operate on sets of formulas, and
can be interpreted as rules of a one-sided sequent calculus, in
which all formulas are located in the antecedent~\cite{Fitting96a}.
Each of the rules should be read and
applied bottom-up, and relates
premises~$\SomeClause_1, \ldots, \SomeClause_k$ with some
conclusion~$\SomeClause$. When constructing a proof, we start from some
root~$\SomeClause$, and then apply proof rules to the goals of the proof
until a goal can be closed, or no more rule is applicable.

A proof in which all proof goals could be closed shows that the formulas
in the root~$\SomeClause$ of the proof are inconsistent, and do not
have any solutions. A proof goal that could not be closed, but to which no
more rules are applicable, gives rise to  a solution of the formulas in the
root~$\SomeClause$. Such an open proof goal will only contain formulas
in Presburger arithmetic, so that a solution can be computed using
standard algorithms~\cite{Fitting96a}.

We use the convention of splitting the
formulas into linear (in)equalities ($\SomeInequalities$)
and other formulas ($\SomeClause$), and assume that predicates
$\Image$ and $\Connected$ only occur positively.
 Since
our rules operate by adding and matching linear (in)equalities in a proof goal,
we use the shorthand of listing the matched inequalities as antecedents
e.g., in \Propagate{}).
The filtering function~$\Filter$ is evaluated symbolically, and can in practice
be read as a function from transitions to $\Naturals$-valued terms
(e.g.~\texttt{t} or~\texttt{t+1}). In our implementation \Catra{}, described in
\cref{sec:implementation}, $\Filter$~is a vector of fresh variables with the
same size as~$\Transitions$.

We use the shorthand notation~$\Transitions_\Automaton$ to refer to the
transitions of an automaton~$\Automaton$. Additionally, for an
automaton~$\Automaton = \AutomatonTuple$ we allow mapping the selection function
like so: $\Filter(\Automaton) = \Tuple{\States, \InitialState, \AcceptingStates,
\Set{\Transition \in \Transitions \SuchThat \Filter(\Transition) > 0}}$, i.e.
$\Automaton$~with only the transitions for which~$\Filter$ is positive. In this
instance, the basis for the matched linear inequalities is
implicitly~$\SomeInequalities$. Similarly, for our commutative monoid $\Monoid =
\left(X;\MonoidOp;0_{\Monoid}\right)$ and the map into it $\Map : \Strings
\rightarrow X$, we also allow mapping over transitions:
$\Map(\FromLabelTo{\State}{\Label}{\State'}) = \Map(\Label)$.

We require that rules can only be applied when they add new formulas
on every created branch (the notion of \emph{regularity} of a proof is
required~\cite{Fitting96a}). For example, this means that \Split{} can
only be applied to proof goals that contain neither
$\Filter(\Transition) = 0$ nor $\Filter(\Transition) > 0$, and can
never be applied to split on the same term twice on the same branch.
This suggests a proof strategy where you \Propagate{} when you can, \Split{}
when you must, and \Subsume{} when neither is possible anymore.

\begin{table}
\begin{tabular}{@{}l>{$}c<{$}p{3cm}@{}}\toprule
  Name & \text{Rule} & Side conditions\\
  \midrule

  % EXPAND
  \Expand & 
    \inferrule
  {\Connected(\Automaton, \Filter) ,~ \FlowEq(\Automaton, \Filter) ,~ \MonoidElement = \sum_{\Transition \in \Transitions_\Automaton} \Filter(\Transition) \cdot \Map(\Transition), \SomeInequalities, \SomeClause}
  {\SinglePredicateInstance, \SomeInequalities, \SomeClause} & 
  None \\[4ex]

  % SPLIT
  \Split & 
  \inferrule{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause, \Filter(\Transition) = 0 \mid \Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause, \Filter(\Transition) > 0}{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause} &
  if $\Transition \in \Transitions_\Automaton$ \\[4ex]

  % PROPAGATE
  \Propagate &
  \inferrule{\Connected(\Automaton, \Filter), \Set{\Filter(\Transition') = 0 \SuchThat \Transition' \in C}, \SomeInequalities, \SomeClause, \Filter(\Transition) = 0}{\Connected(\Automaton, \Filter), \Set{\Filter(\Transition') = 0 \SuchThat \Transition' \in C}, \SomeInequalities, \SomeClause} &
  if $t = \FromLabelTo{\State}{}{\State'} \in \Transitions_\Automaton, \SeparatingCut(C, \Automaton, \State)$ \\[4ex]

  % SUBSUME
  \Subsume &
  \inferrule{\SomeInequalities,\SomeClause}{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause} &
  \Split{} and \Propagate{} cannot be applied \\
  \bottomrule
  \end{tabular}
  \caption{Derivation rules for one automaton.}\label{tbl:rules:single}
\end{table}
We use the symbolic function $\FlowEq(\Automaton, \Filter)$ that generates a set
of existentially quantified linear inequalities with the following definition,
where we assign fresh, existentially quantified variables to
$\FinalStateVar_\State, \Filter(\Transition)$ for every $\State \in
\AcceptingStates, \Transition \in \Transitions$:
\[
\begin{aligned}
  & \FlowEq(\Automaton, \Filter) = \sum\limits_{\State \in \AcceptingStates} \FinalStateVar_\State = 1 \land \AndComp{\State \in \States}{\In(\State, \Filter) - \Out(\State, \Filter)} = \Sink(\State) \land
  \AndComp{\State \in \AcceptingStates}{\FinalStateVar_\State \geq 0}\\
  & \Sink(\State) = 0 \text{ if } \State \not\in \AcceptingStates, \FinalStateVar_\State \text{ otherwise.} \\
  & \In(\State, \Filter) = \StartFlow(\State) + \sum_{\Transition \in \FromLabelTo{\State'}{}{\State}} \Filter(\Transition)\\
  & \StartFlow(\State)  = 1 \text{ if } \State = \InitialState, \text{ otherwise $0$.} \\
  & \Out(\State, \Filter) = \sum_{\Transition \in \FromLabelTo{\State}{}{\State'}} \Filter(\Transition)
\end{aligned}
\]

In addition to \cref{tbl:rules:single}, we assume the existence of a rule
\PresburgerClose{}, corresponding to a sound and complete solver for Presburger
formulae, and for the elements of~$\Monoid$.

The $\Propagate{}$ rule allows us to propagate (dis-)connectedness across
$\Automaton$. It states that we are only allowed to use transitions attached to
a reachable state, and is necessary to ensure connectedness in the presence of
cycles in~$\Automaton$.

\textsc{Expand} expands the predicate into its most basic rules; one set of
linear equations synchronising the transitions mentioned by~$\Filter$ to the
corresponding Monoid element~$\MonoidElement$, and the linear flow equations of
the standard Parikh image formulation, as described by~\FlowEq. Since
$\Connected$ and $\Image$ are partially redundant and the difference is covered
by~$\FlowEq$, we can remove the instance of~$\Image$ when applying~$\Expand$. In
this sense, we split the semantics of the $\Image$~predicate into its counting
aspect (covered by $\FlowEq$) and its connectedness aspect (covered by
$\Connected$).

Finally, $\Split{}$ allows us to branch the proof tree by trying to exclude a
contested transition from a potential solution before concluding that it must be
included. Intuitively, this is what guarantees our ability to make forward
progress by eliminating paths through~$\Automaton$.

A decision procedure for our predicate in a tableau-based automated theorem
prover would start by expanding the predicate using the $\Expand{}$~rule. A
theorem prover would perform algebraic substitution on the underlying constants
of~$\Filter$, boiling them down to choices of branches, which depend on one
single variable, and loop transitions. This logic corresponds to the placement
of counters for optimally edge-profiling the CFG of a program, making up a
minimum-spanning tree of the automaton~\cite{path-profiling}.

In order to make the examples below tractable, we will assume the existence of a
rule \EquationReasoning{} that allows us to perform standard algebraic reasoning
on linear inequalities. This rule is not necessary for correctness or
completeness, but shortens the examples considerably.

\subsection{An Example}

Starting with~$\AcaOrBc{}$, where $\Map$ is the length function, in effect
$\Transition \mapsto 1$ for transitions, and the constraints that the length is
odd using the same trick as in \cref{sec:motivation}, we have the
definitions in \cref{fig:example:single:equivalences} (omitting existential
quantifiers and $x \geq 0$ for every variable to avoid clutter).

\begin{figure}[ht]
  We define $\Filter$ and $\FlowEq(\AcaOrBc{}, \Filter)$ as in the following two
  equations, and then apply \EquationReasoning{} (under the implicit assumption
  that every RHS is $\geq 0$) to obtain the equivalent third definition :
  \begin{minipage}[b]{0.3\linewidth}
    \begin{equation*}
      \begin{aligned}
        & \Filter(\FromLabelTo{S}{a}{A}) & = \TransitionVar_1 \\
        & \Filter(\FromLabelTo{S}{b}{B}) & = \TransitionVar_2 \\
        & \Filter(\FromLabelTo{A}{c}{A})  & = \TransitionVar_3  \\
        & \Filter(\FromLabelTo{B}{b}{S}) & = \TransitionVar_4 \\
        & \Filter(\FromLabelTo{A}{a}{F}) & = \TransitionVar_5 \\
        & \Filter(\FromLabelTo{B}{c}{F}) & = \TransitionVar_6 \\
      \end{aligned}
    \end{equation*}    
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}[b]{0.3\linewidth}
    \begin{equation*}
      \begin{aligned}
        % S
        &  \TransitionVar_4 = \TransitionVar_5 + \TransitionVar_2 - 1 \\
        % A
        & \TransitionVar_1 = \TransitionVar_5 \\
        % B
        & \TransitionVar_2 = \TransitionVar_4 + \TransitionVar_6 \\
        % F
        & \TransitionVar_5 + \TransitionVar_6 = \FinalStateVar_1 \\
        & \FinalStateVar_1 = 1 \\
      \end{aligned}  
    \end{equation*}    
  \end{minipage}
  \begin{minipage}[b]{0.3\linewidth}
    \begin{equation*}
      \begin{aligned}
        & \Filter(\FromLabelTo{S}{a}{A}) & = 1 - \TransitionVar_6 \\
        & \Filter(\FromLabelTo{S}{b}{B}) & = \TransitionVar_4 + \TransitionVar_6 \\
        & \Filter(\FromLabelTo{A}{c}{A})  & = \TransitionVar_3  \\
        & \Filter(\FromLabelTo{B}{b}{S}) & = 2\TransitionVar_6 + \TransitionVar_4 \\
        & \Filter(\FromLabelTo{A}{a}{F}) & = 1 - \TransitionVar_6 \\
        & \Filter(\FromLabelTo{B}{c}{F}) & = \TransitionVar_6 \\
      \end{aligned}
    \end{equation*}    
  \end{minipage}
  \caption{Equivalences defining $\Filter$ and $\FlowEq(\AcaOrBc{}, \Filter)$
  respectively.}\label{fig:example:single:equivalences}
  \end{figure}

\begin{figure}
  \centering
\begin{prooftree}
  \hypo[]{
    \begin{matrix}
      1 - \TransitionVar_6 = 0 \land \\
      \TransitionVar_6 = 1 \land \\
      \TransitionVar_4 + 1 > 0 \land \\
      \TransitionVar_3 = 0 \land \\
      2 + \TransitionVar_4 > 0 \land \\
      k = 1 + \TransitionVar_4 
    \end{matrix}
  }
  \infer1[\Subsume{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 = 0 \land \\
      \TransitionVar_6 = 1 \land \\
      \TransitionVar_4 + 1 > 0 \land \\
      \TransitionVar_3 = 0 \land \\
      2 + \TransitionVar_4 > 0 \land \\
      k = 1 + \TransitionVar_4 
    \end{matrix}
  }
  \infer1[\Propagate]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 = 0 \land \\
      \TransitionVar_6 = 1 \land \\
      \TransitionVar_4 + 1 > 0 \land \\
      \TransitionVar_3 \geq 0 \land \\
      2 + \TransitionVar_4 \geq 0 \land \\
      2k = 2 + 2\TransitionVar_4 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\EquationReasoning{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 = 0 \land \\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3 + \TransitionVar_6
    \end{matrix}
  }
  % BRANCH:  = 0'
  \hypo{
    \begin{matrix}
      \TransitionVar_3 > 0 \land \\
      \TransitionVar_6 = 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\Subsume{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \TransitionVar_3 > = 0 \land \\
      \TransitionVar_6 = 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\EquationReasoning{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \TransitionVar_6 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0\\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\Propagate{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \TransitionVar_6 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\EquationReasoning{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3 + \TransitionVar_6
    \end{matrix} % BRANCH: > 0
  } % SPLIT
  \infer2[\Split{} $1 - \TransitionVar_6$]{ \Connected(\AcaOrBc{}, \Filter) \land 2k = 1 + 2\TransitionVar_4 + \TransitionVar_3 + \TransitionVar_6 }
  \infer1[\EquationReasoning{}]{
    \Connected(\AcaOrBc{}, \Filter) \land
    2k + 1 =
    \sum\limits_{\Transition \in \Transitions} (\Transition \mapsto 1)(\Label) \cdot \Filter(\Transition)
  }
  \infer1[\Expand{}]{\Image{}_{\AcaOrBc{}, \Transition \mapsto 1}(\Filter, 2k + 1)}
\end{prooftree}
\caption{A derivation for \Calculus{} computing odd lengths in $\AcaOrBc{}$.}\label{fig:derivation:single}
\end{figure}

In \cref{fig:derivation:single}, we see how we start by expanding the predicate
using the (simplified) flow equations. We can also see how interleaving
reasoning on the corresponding linear equations helps the deduction. In both
branches the reasoning is similar: we conclude that when choosing either path at
the starting state we must avoid the other one, perform propagation based on
that fact, use some algebraic reasoning to derive the expected forms of the
upper and lower bounds to propagate the now disconnected transitions, and
finally subsume, removing the $\Connected{}$ predicate when we are unable to use
either of the rules, leaving only a set of linear equations. To obtain a solution from the
proof, one would need to perform standard model generation on the remaining
constraints to obtain values for the sought variable $\MonoidElement$. The full
Parikh image is the disjunction of the constraints left at the two
leaves after quantifier elimination.

Following the values of $\Filter$, we can read out the path corresponding to our solution in the product, and with that a word. In our case, for the right branch we have $\Filter(\FromLabelTo{S}{b}{B}) = 0,
\Filter(\FromLabelTo{A}{c}{A})  > 0,
\Filter(\FromLabelTo{B}{b}{S}) = 0,
\Filter(\FromLabelTo{S}{a}{A}) = 1 ,
\Filter(\FromLabelTo{A}{a}{F}) = 1 ,
\Filter(\FromLabelTo{B}{c}{F}) = 0$. We can read out a word along it: aca, accca, etc, and the corresponding constraint: $\exists k : \TransitionVar_3 = 2k -1 \land \TransitionVar_3 > 0 \land k \geq 0$.

\subsection{Correctness of \Calculus{}}\label{sec:single:correct}

Our correctness proof of \Calculus{} consists of two main parts: we
first show that the construction of a proof always terminates, and
then that each of the proof rules in \cref{tbl:rules:single} is
an equivalence transformation, i.e., does not change the set of
satisfying assignments of a formula. In combination, those two results
immediately imply that \Calculus{} gives rise to a decision procedure.

\subsubsection{\Calculus{} terminates}
\begin{lemma}\label{lma:single-terminates}
  Suppose $\SomeClause{}$ is a set of formulas in which the predicates
  $\Image$ and $\Connected$ only occur positively. There is no
  infinite sequence of proofs~$P_0, P_1, P_2, \ldots$ in which $P_0$
  has $\SomeClause{}$ as root, and each $P_{i+1}$ is derived from
  $P_i$ by applying one of the rules in \cref{tbl:rules:single}.
\end{lemma}

\begin{proof}
  The rule~\Expand{} can only be applied finitely often, since each
  application removes one $\Image$ predicate, and none of the rules
  introduce new instances of the predicate. The rule~\Subsume{} can
  only be applied finitely often, since it strictly decreases the
  combined number of $\Image$ and $\Connected$ predicates in sets of
  formulas, and none of the rules increases that number.

  To show termination of \Split{} and \Propagate{}, observe that the
  $\Filter$ in a predicate~$\Connected(\Automaton, \Filter)$ is never
  updated on a proof branch, which means that the set of terms
  $\Filter(t)$ for $t \in \Automaton$ on every branch is finite. Each
  application of \Split{} and \Propagate{} adds a new
  formula~$\Filter(\Transition) = 0$ or $\Filter(\Transition) > 0$
  to a proof goal, which can only happen finitely often.
\end{proof}

\subsubsection{The rules in \cref{tbl:rules:single} are solution-preserving}

\begin{lemma}\label{lma:single-correct}
  Consider an application of one of the rules in
  \cref{tbl:rules:single}, with
  premises~$\SomeClause_1, \ldots, \SomeClause_k$ and
  conclusion~$\SomeClause$. An assignment~$\beta$ satisfies the
  conclusion~$\SomeClause$ if and only if it satisfies one of the
  premises~$\SomeClause_i$.
\end{lemma}

\begin{proof}
  This property has to be shown by analysing the possible applications
  of each proof rule.

  \Expand{} unfolds the definition of the $\Image$ predicate. To show
  that the rule is solution-preserving we prove the equivalence of the
  upper and lower sets of formulas:
  \begin{itemize}
  \item Assume that $\beta$ satisfies the conclusion, which means that
    there is some accepting path
    $\Path = \PathEnumeration \in \Accepting{\Paths(\Automaton)}$ with
    $\val_\beta(\Filter(\Transition)) = \TransitionCount(\Transition,
    \Path)$ and $\val_\beta(\MonoidElement) =
    \Map(\WordOf(\Path))$. Since immediately implies that $\beta$
    satisfies $\Connected(\Automaton, \Filter)$, since a path is
    connected, and $\FlowEq(\Automaton, \Filter)$ since an accepting
    path satisfies the flow equations. The
    equation~$\MonoidElement = \sum_{\Transition \in
      \Transitions_\Automaton}\Filter(\Transition) \cdot
    \Map(\Transition)$ holds because of
    $\val_\beta(\Filter(\Transition)) = \TransitionCount(\Transition,
    \Path)$.
  \item Assume that $\beta$ satisfies the premise, which implies that
    $\val_\beta(\Filter)$ describes a consistent, connected flow of
    the automaton. By the same argument as in
    \cite{generate-parikh-image} (\cref{sec:verma}), this flow
    can be mapped to an accepting path~$\Path$ of $\Automaton$ such
    that each transition~$\Transition$ occurs on $\Path$ exactly
    $\val_\beta(\Filter(\Transition))$ times. Together with the equation
    $\val_\beta(\Filter(\Transition)) = \TransitionCount(\Transition,
    \Path)$, this implies that $\beta$ satisfies $\SinglePredicateInstance$.
  \end{itemize}

  In \Split{}, we make use of the fact that $\Filter(\Transition)$ is
  $\Naturals$-valued by definition. For any $\beta$, clearly exactly
  one of $\Filter(\Transition) = 0$ or $\Filter(\Transition) > 0$ will
  be satisfied, implying the property.

  For \Propagate{}, suppose that
  $\SeparatingCut(C, \Automaton, \State)$, which means that every
  accepting path visiting~$q$ contains at least one of the transitions
  in $C$. For a $\beta$ satisfying $\Connected(\Automaton, \Filter)$
  and $\Filter(\Transition') = 0$ for $\Transition' \in C$, this means
  that also $\val_\beta(\Filter(\Transition)) = 0$ has to hold.

  Finally, for \Subsume{}, observe that if \Split{} cannot be applied,
  then a goal must contain~$\Filter(\Transition) = 0$ or
  $\Filter(\Transition) > 0$ for every $\Transition$. In case the
  formulas in $\SomeInequalities$ are inconsistent, an application of
  \Subsume{} is trivially solution-preserving; therefore assume that
  $\SomeInequalities$ is consistent, which means that it contains
  exactly one of $\Filter(\Transition) = 0$ or
  $\Filter(\Transition) > 0$ for each $\Transition$. Since
  \Propagate{} is not applicable, the transitions~$\Transition$ with
  $\Filter(\Transition) > 0$ must form a connect sub-graph of the
  automaton; this means that $\Connected(\Automaton, \Filter)$ is
  redundant as it is implied by $\SomeInequalities$.
\end{proof}


\section{Parikh Images from Products of Automata}\label{sec:multiple}

We now generalise our calculus to natively work with intersections of
regular languages, or equivalently products of automata. For this
extension, we change the main predicate~$\Image$ to be indexed by a
vector of automata~$\Tuple{\Automaton_1, \ldots, \Automaton_k}$.  For
simplicity, we assume that the sets of states of the $k$ automata (and
therefore also the transition sets) are pairwise disjoint. 
\begin{definition}\label{def:multiple}
  Suppose $\Automaton_1, \ldots, \Automaton_k$ are automata,
  $\Map : \Alphabet^* \to \Monoid$ is a map to a commutative monoid
  $\Monoid$,
  $\Filter: \bigcup_{i=1}^k \Transitions_{\Automaton_i} \to \Naturals$
  is a transition selection function, and
  $\MonoidElement \in \Monoid$. The predicate
  $\ImagePredicate{\Tuple{\Automaton_1, \ldots,
      \Automaton_k}}{\Map}{\Filter}{\MonoidElement}$ is true exactly
  when there are accepting paths~$\Path_1, \ldots, \Path_k$ of the
  respective automata, such that for each $i \in \{1, \ldots, k\}$ and
  $\Transition \in \Transitions_{\Automaton_i}$ it holds that
  \begin{itemize}
  \item the multiplicity of $\Transition$ on $\Path_i$ is consistent with
    $\Filter$, that is,
    $\Filter(\Transition) = \TransitionCount(\Transition, \Path_i)$,
  \item the automata all accept the same
    word~$\WordOf(\Path_i) = \WordOf(\Path_1)$, and
  \item the accepted word is mapped to
    $\MonoidElement = \Map(\WordOf(\Path_i)) =
    \Map(\WordOf(\Path_1))$.
  \end{itemize}
\end{definition}

  \begin{table}[t]
    \begin{tabular}{@{}l>{$}c<{$}p{3cm}@{}}\toprule
      Name & \text{Rule} & Side conditions\\
      \midrule
    
      % EXPAND
      \ExpandM & 
      \inferrule
      {
       {\begin{array}{c}
           \left\{
           \FlowEq(\Automaton_i, \Filter),~~
           \Connected(\Automaton_i, \Filter),~~
           \MonoidElement = \sum_{\Transition \in \Transitions_{\Automaton_i}} \Filter(\Transition) \cdot \Map(\Transition)
           \right\}_{i=1}^k,
           \\[4ex]
          \ImagePredicate{\Tuple{\Automaton_1, \ldots, \Automaton_k}}{\Map}{\Filter}{\MonoidElement}, ~
          \SomeInequalities, \SomeClause
        \end{array}}
        }
      {\ImagePredicate{\Tuple{\Automaton_1, \ldots, \Automaton_k}}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause} & 
      None \\[5ex]
      \Materialise &
      \inferrule
      {{
         \begin{array}{c}
           \FlowEq(\Automaton', \Filter'),~
           \Connected(\Automaton', \Filter'),~
           \BindingSum(\Automaton_i, \Automaton_j, \Filter')
           \\\ImagePredicate{
                     \Tuple{\Automaton_1, \Automaton_{i-1}, \Automaton_{i+1}, \ldots,
                     \Automaton_{j-1}, \Automaton_{j+1}, \ldots, \Automaton_k,
                     \Automaton'},~ \SomeInequalities, \SomeClause
           }{\Map}{\Filter'}{\MonoidElement}
           \end{array}}
           }
      {\ImagePredicate{\Tuple{\Automaton_1, \ldots, \Automaton_i, \ldots, \Automaton_j, \ldots, \Automaton_k}}{\Map}{\Filter}{\MonoidElement},~ \SomeInequalities, \SomeClause} &
      $\begin{array}{@{}l@{}}
         \Filter' = \Extend(\Automaton_i, \Automaton_j,\Filter),\\
         1 \leq i < j \leq k, \\
      \Automaton' = \Automaton_i \times \Automaton_j,\\
      \end{array}$ \\
      \bottomrule
      \end{tabular}
      \caption{Additional derivation rules for products of arbitrarily many automata.}\label{tbl:rules:multi}
    \end{table}

    For the calculus (\cref{tbl:rules:multi}), we first extend
    $\Expand$ to generate flow equations and instances of $\Connected$
    for each automaton, resulting in a new rule $\ExpandM$.  Unlike
    \Expand{}, \ExpandM{} does not remove the $\Image$~predicate,
    since it is needed to keep track of the currently considered
    partial products.

The rule~$\Materialise$ introduces the product of two individual
automata~$\Automaton_i, \Automaton_j$; this step eliminates
$\Automaton_i, \Automaton_j$ as index of the $\Image$ predicate, and
instead adds the product~$\Automaton_i \times \Automaton_j$, while
also introducing the flow equations and the $\Connected$ predicate.

The rule~$\Materialise$ also has to connect the newly introduced
product~$\Automaton_i \times \Automaton_j$ to the previous automata
$\Automaton_i, \Automaton_j$. This is done by extending the
selection function~$\Filter$ to $\Filter'$, mapping the transitions of the product
to fresh variables~$x_{\Transition}$:
\begin{equation*}
  \Extend(\Automaton_i, \Automaton_j,\sigma)(t)
  ~~=~~
  \begin{cases}
    x_{\Transition} & t \in \Transitions_{\Automaton_i \times \Automaton_j}
    \\
    \Filter(t) & \text{otherwise}
  \end{cases}
\end{equation*}

The multiplicity of transitions in the product then has to be related
to the multiplicities in the individual automata, modelled using the
$\BindingSum$ predicate. The predicate expresses that the multiplicity
of a transition~$\Transition \in \Transitions_{\Automaton_i}$ in
$\Automaton_i$ has to coincide with the sum of the multiplicities of
transitions in $\Automaton_i \times \Automaton_j$ derived from $t$,
and similarly for $\Automaton_j$:
%
  $$
  \BindingSum(\Automaton_i, \Automaton_j, \Filter') ~~=~~
  \begin{aligned}
  & \Set{ 
    \Filter'(\Transition)  =  \sum\limits_{\Transition' = \FromLabelTo{\Tuple{\State, \State_R}}{\Label}{\Tuple{\State', \State_R}}} \Filter'(\Transition')
  \SuchThat \Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions_{\Automaton_i} } \cup\mbox{} \\ 
  & \Set{
    \Filter'(\Transition)  =  \sum\limits_{\Transition' = \FromLabelTo{\Tuple{\State_L, \State}}{\Label}{\Tuple{\State_L, \State'}}} \Filter'(\Transition') \SuchThat \Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions_{\Automaton_j}
  }
  \end{aligned}
$$

\iffalse
Note that this definition implies that $\Filter(\Transition) =
\Filter(\Transition')$ whenever two transitions $\Transition \in
\Transitions_{\Automaton_1}, \Transition' \in \Transitions_{\Automaton_2}$
produces a product transition $\Transition'' \in \Transitions_{\Automaton_1
\times \Automaton_2}$. This corresponds to our intuition that the terms of the
product must agree on the value they accept. As before, we implicitly map
$\Filter$ to fresh terms for each transition in the product.
\fi

For instances of precisely one automaton, neither rule applies and we
perform the calculus as before.

\subsection{An Example}\label{sec:multiple:example}

We return again to our example in \cref{sec:introduction:parikh}, where we
compute the whole Parikh image of the product of $\AcaOrBc{}$ and
$\SomethingCSomething$ under the constraint that there are more instances of
letters a than b. This time our monoid $\Monoid$ is 3-dimensional vectors with
element-wise addition, and $\Map$ that maps each transition to the corresponding
increment vector, e.g $\Map(\FromLabelTo{S}{a}{A}) = \VectorLiteral{1,0,0}$.


In the interest of space, we refer back to
\cref{fig:example:single:equivalences} for the definitions of $\Filter$ for
$\AcaOrBc$ and only define $\Filter$ for $\SomethingCSomething{}$ after
substitutions as follows. Note the expansion of the $\Sigma$ labels:
    \begin{equation*}
      \begin{aligned}
        % & \Filter(\FromLabelTo{S}{a}{A}) & = 1 - \TransitionVar_6 \\
        % & \Filter(\FromLabelTo{S}{b}{B}) & = \TransitionVar_4 + \TransitionVar_6 \\
        % & \Filter(\FromLabelTo{A}{c}{A})  & = \TransitionVar_3  \\
        % & \Filter(\FromLabelTo{B}{b}{S}) & = 2\TransitionVar_6 + \TransitionVar_4 \\
        % & \Filter(\FromLabelTo{A}{a}{F}) & = 1 - \TransitionVar_6 \\
        % & \Filter(\FromLabelTo{B}{c}{F}) & = \TransitionVar_6 \\
        % Other automaton
        & \Filter(\FromLabelTo{S}{\Sigma}{S}) & = \VectorLiteral{\TransitionVar_{7a}, \TransitionVar_{7b}, \TransitionVar_{7c}} \\
        & \Filter(\FromLabelTo{S}{c}{F}) & = 1 \\
        & \Filter(\FromLabelTo{F}{\Sigma}{F}) & = \VectorLiteral{\TransitionVar_{9a}, \TransitionVar_{9b}, \TransitionVar_{9c}} \\
      \end{aligned}
    \end{equation*}
    
In the derivation tree of \cref{fig:derivation:multi} we start as before, but
with the product version of the $\Image$ predicate. The only possible rule here
is $\ExpandM$, which we use to add the corresponding constraints on each
automata of the product as we would have had in the single-automaton version. We
perform algebraic reasoning on those equations, along with the constraint on
$\MonoidElement$ to determine bounds on transition variables that will enable us
to \Subsume{} and remove one of the $\Connected{}$ predicates. When we have
finished doing so, we use \Materialise{} to compute the product of the now
filtered automata. Further algebraic reasoning allows us to remove the final
instance of $\Connected$. We then perform \Expand{} to get rid of the final
$\Image$ instance, and are immediately able to $\Subsume$ the resulting
$\Connected{}$ predicate. This, again, leaves us with a set of linear
inequalities at the leaf that we can use to obtain a model of our final
$\MonoidElement$ values (now a vector). This time we can read out aca, or
$\VectorLiteral{2, 0, 1}$.

\begin{figure}
  \centering
\begin{prooftree}
  \hypo{
    \begin{matrix}
      1 = \TransitionVar_{10} \land 
      \TransitionVar_3 = \TransitionVar_{11} \land \\
      1 = \TransitionVar_{12} \land 
      1 = \TransitionVar_{11} \land 
      2 > 0
    \end{matrix}  
  }
  \infer1[\Subsume{}, \Expand{}, \Subsume{}]{
    \begin{matrix}
      1 = \TransitionVar_{10} \land 
      \TransitionVar_3 = \TransitionVar_{11} \land  \\
      1 = \TransitionVar_{12} \land 
      1 = \TransitionVar_{11} \land \\
      \Image{}_{\Automaton', \Map}(\Filter, 
      \VectorLiteral{2, 0, 1}) \land 
      \Connected(\SomethingCSomething{}, \Filter) \land
        2 > 0
    \end{matrix}  
  }
  \infer1[\EquationReasoning{}]{
    \begin{matrix}
      1 = \TransitionVar_{10} \land
      \TransitionVar_3 = \TransitionVar_{11} 
      1 = \TransitionVar_{12} \land
      1 = \TransitionVar_{11} \\
      \Image{}_{\Automaton', \Map}(\Filter, 
      \VectorLiteral{
        \TransitionVar_{7a} + \TransitionVar_{9a},
        0,
        \TransitionVar_{7c} + \TransitionVar_{9c} + 1}) \land \\
        \TransitionVar_6 + \TransitionVar_4 = 0 \land 
        1 - \TransitionVar_6 > 0 \land
        \TransitionVar_6 = 0 \land 
        \TransitionVar_4 = 0 \land
        \TransitionVar_3 > 0 \land
        2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
          \VectorLiteral{
        \TransitionVar_{7a} + \TransitionVar_{9a},
        0,
        \TransitionVar_{7c} + \TransitionVar_{9c} + 1}
        = \VectorLiteral{
          2,
          0,
          \TransitionVar_3} \land \\
      \Connected(\SomethingCSomething{}, \Filter) \land
      \land \TransitionVar_{7a} + \TransitionVar_{9a} > 0
    \end{matrix}  
  }
  \infer1[\Materialise]{
    \begin{matrix}
      \TransitionVar_6 + \TransitionVar_4 = 0 \land 
      1 - \TransitionVar_6 > 0 \land
      \TransitionVar_6 = 0 \land 
      \TransitionVar_4 = 0 \land
      \TransitionVar_3 > 0 \land
      2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
      \VectorLiteral{
        \TransitionVar_{7a} + \TransitionVar_{9a},
        0,
        \TransitionVar_{7c} + \TransitionVar_{9c} + 1}
        = \VectorLiteral{
          2,
          0,
          \TransitionVar_3} \land \\
      \Connected(\SomethingCSomething{}, \Filter) \land \\
      \Image{}_{\Tuple{\AcaOrBc{},\SomethingCSomething{}}, \Map}(\Filter, 
      \VectorLiteral{\TransitionVar_{7a} + \TransitionVar_{9a}, 0, \TransitionVar_{7c} + \TransitionVar_{9c} + 1}) \land \TransitionVar_{7a} + \TransitionVar_{9a} > 0
    \end{matrix}
  }
  \infer1[\Subsume{}]{
  \begin{matrix}
    \TransitionVar_6 + \TransitionVar_4 = 0 \land 
    1 - \TransitionVar_6 > 0 \land
    \TransitionVar_6 = 0 \land 
    \TransitionVar_4 = 0 \land
    \TransitionVar_3 > 0 \land
    2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
    \VectorLiteral{
      \TransitionVar_{7a} + \TransitionVar_{9a},
      0,
      \TransitionVar_{7c} + \TransitionVar_{9c} + 1}
      = \VectorLiteral{
        2,
        0,
        \TransitionVar_3} \land \\
    \Connected(\AcaOrBc{}, \Filter) \land 
    \Connected(\SomethingCSomething{}, \Filter) \land \\
    \Image{}_{\Tuple{\AcaOrBc{},\SomethingCSomething{}}, \Map}(\Filter, 
    \VectorLiteral{\TransitionVar_{7a} + \TransitionVar_{9a}, 0, \TransitionVar_{7c} + \TransitionVar_{9c} + 1}) \land \TransitionVar_{7a} + \TransitionVar_{9a} > 0
  \end{matrix}
  }
  \infer1[\EquationReasoning]{
    \begin{matrix}
      \VectorLiteral{a, b, c} = \VectorLiteral{
          2 - 2\TransitionVar_6,
          2\TransitionVar_4 + 3\TransitionVar_6,
          \TransitionVar_3 + \TransitionVar_6
        } \land
        \\
        \VectorLiteral{a, b, c} = \VectorLiteral{
            \TransitionVar_{7a} + \TransitionVar_{9a},
            \TransitionVar_{7b} + \TransitionVar_{9b},
            \TransitionVar_{7c} + \TransitionVar_{9c} + 1
          } \land \\
      \Connected(\AcaOrBc{}, \Filter) \land 
      \Connected(\SomethingCSomething{}, \Filter) \land \\
      \Image{}_{\Tuple{\AcaOrBc{},\SomethingCSomething{}}, \Map}(\Filter, 
      \VectorLiteral{a, b, c}) \land a > b
    \end{matrix}
  }
  \infer1[\ExpandM]{\Image{}_{\Tuple{\AcaOrBc{},\SomethingCSomething{}}, \Map}(\Filter, \VectorLiteral{a, b, c}) \land a > b}
\end{prooftree}
\caption{A derivation for \Calculus{} on the Parikh image of strings with more a's than b's in the product of $\AcaOrBc{}$ and $\SomethingCSomething{}$.}\label{fig:derivation:multi}
\end{figure}

\subsection{Correctness of \Calculus{} for Products of Automata}

Since \cref{tbl:rules:multi} only extends the existing rules of
\cref{tbl:rules:single}, we focus on the differences compared
to the calculus for a single automaton.

%\subsubsection{\Calculus{} for products of automata terminates}
\begin{lemma}\label{lma:multi-terminates}
  Suppose $\SomeClause{}$ is a set of formulas in which the product
  version of $\Image$ only occurs positively. There is no
  infinite sequence of proofs~$P_0, P_1, P_2, \ldots$ in which $P_0$
  has $\SomeClause{}$ as root, and each $P_{i+1}$ is derived from
  $P_i$ by applying one of the rules in \cref{tbl:rules:multi}.
\end{lemma}

\begin{proof}
  The rule \Materialise{} can similarly only be used finitely many times, as
  each application reduces the number of automata in the product of $\Image$ by
  one automaton, until only one remains and \cref{lma:single-terminates} for
  single-automaton instances applies.
  
  This implies that also the rule~\ExpandM{} can only be applied
  finitely often since its side condition only allows applying it once
  per instance of an $\Image$ predicate containing a product, and only finitely
  many instances of $\Image$ can be introduced on each branch.
\end{proof}

%\subsubsection{The rules in \cref{tbl:rules:multi} are solution-preserving}

Since our calculus now includes a rule introducing new variables, the
\Materialise{} rule, we have to slightly generalise the notion of
solution-preservation:
%
\begin{lemma}\label{lma:multi-correct}
  Consider an application of one of the rules in
  \cref{tbl:rules:multi}, with
  premises~$\SomeClause_1, \ldots, \SomeClause_k$ and
  conclusion~$\SomeClause$. An assignment~$\beta$ (over the symbols in
  $\SomeClause$) satisfies the conclusion~$\SomeClause$ if and only if
  there is an extension~$\beta' of \beta$ satisfying one of the
  premises~$\SomeClause_i$.
\end{lemma}

\begin{proof}
  We have to consider the two new rules in \cref{tbl:rules:multi}. The
  result is immediate for \ExpandM{}, since this rule does not remove the
  $\Image$ predicate from a proof goal, and the newly introduced formulas
  are all implied by the $\Image$ predicate.

  For \Materialise{}, observe that the existence of an accepting path
  in $\Automaton_i \times \Automaton_j$ is equivalent to the existence
  of individual paths in $\Automaton_i, \Automaton_j$ accepting the
  same word. The path in the product will satisfy the flow equations
  and connectedness, and it will be related to the individual paths as
  stipulated by the \BindingSum{} predicate.
\end{proof}


\section{Extensions}\label{sec:extensions}

This section describes a number of possible extensions to the calculus. Note
that some are partially or fully implemented in \Catra{} already. In particular,
we have some level of support for symbolic transitions over Unicode alphabets to
keep practical automata under a reasonable size, though we do not allow a full Boolean algebra over symbols as described e.g. in~\cite{symbolic-automata}.

\subsection{Backjumping and Learning No-Goods}\label{sec:ext:backjumping}

\Calculus{} can be accelerated for some instances by adding rules for
backjumping. In particular, central connectedness constraints for an automaton
can be learnt. In that case, when discovering that a state $\State$ of an
automaton $\Automaton$ under a certain transition variable $\Filter$ has become
unreachable, we can learn the clause $\Connected(\Automaton, \Filter) \land
\sum_{\Transition \in \SeparatingCut(\Automaton, \State)} \Filter(\Transition) =
0 \implies \sum_{\Transition' = \FromLabelTo{\State}{}{} \in
\Transitions_{\Automaton}} = 0$. A similar rule can be used when a state becomes
backwards-unreachable (but with a corresponding backwards cut). At the moment,
only forward-cut learning is implemented in \Catra, and has provided a slight
improvement in performance on some instances.

The other source of clauses to learn is the \Materialise{} rule. Whenever an
attempt to materialise a product of two ($\Filter$-filtered) automata
$\Automaton_1 \times \Automaton_2$ produces an empty product, we can determine
the cause of the failure with respect to the automata and their respective
transition variables, and learn no-good combinations that can never be part of a
model. In order to be able to do this, we need a few semantic predicates to
record the state of the calculation, as well as a system of disambiguating
automata, since it is possible to arrive at the same automaton by multiple
combinations of decisions and products. In \Catra{}, we use a number of
additional predicates to record the status of the materialisation of products,
to separate instances of our main predicate, and to register the mapping between
automata and their respective transition $\Filter$ terms, described in
\cref{sec:implementation}. However, at this point only rudimentary no-good
learning is implemented.

\subsection{Symbolic Automata}\label{sec:ext:symbolic}

Extending \Calculus{} to support fully symbolic automata is possible within the
framework, depending on your choice of $\Map$. The difficulty consists in
handling the mapping of the homomorphism $\Map$ over symbolic labels, assuming
it maps to finitely many monoid elements. This is not always straightforward, as
seen in the example of \cref{sec:multiple:example}. This complexity is inherent
in computing the full Parikh image, and stems from the fact that we need to
differentiate between the possible interpretations of the $\Sigma$ transitions
without knowing ahead of times which ones will actually be materialied in the
product. If, on the other hand, if our $\Map$ had been length-counting, which
does not differentiate between values, it would have required no adaptation at
all. With a somewhat liberal interpretation of what we are allowed to map to
(e.g. fresh terms), it is possible to use $\Map$ to represent choice operations
like the ones in a range label. In \Catra{}, we frontload this problem by
requiring the user to encode their input as a Parikh automaton. For an example of how automata can be encoded, see \cref{sec:parikh-automata}.

\subsection{Transducers}\label{sec:ext:transducers}

Another interesting application is applying \Calculus{} to automata with
multiple tracks, e.g. transducers. One application of such a calculus would be
to represent replace operations and other functions on regular languages, and to
be able to answer questions like "does this operation change the length of the
string". The difficulty in implementing it for transducers is first to perform
the mapping on the labels, which for both the length and Parikh cases is
straightforward; just do the same thing in two dimensions. The more complicated
operation is defining the product of transducers. In some cases it would
probably be desirable to perform synchronisation (e.g. requiring overlapping
transitions) on only some transitions, for example the first track.

Implementing such a calculus is straightforward in \Calculus, since the
definition of products was left out of the definition. All that would be needed
is an appropriate update of the definitions of products. Simiarly, \Catra was
written with modularity in mind, and it should be straightforward to extend both
the input grammar and automata implementations to accommodate multi-track
automata with arbitrary synchronisation.

\subsection{Finding the Presburger representation of a homomorphic image}\label{sec:finding-the-image}

To find the Presburger form of the homomorphic image efficiently, we adapt the
quantifier elimination approach of~\cite{qe} to our problem domain. The core
method is the same: we incrementally use \Calculus{} to find models,
\Generalise{} the models we find into a quantifier-free Presburger formula with
the $\MonoidElement$ as the only free variable (algorithm~\ref{alg:generalise}), add the
negated formula as a constraint, and continue enumerating models until none
remain. The disjunction of the generalised models we enumerated is now our
image.

\begin{algorithm}
  \caption{$\Generalise{}(\Automaton, \Filter, a)$ will generalise a final product $\Automaton$ and a model $a$ under the homomorphism $\Map$.}\label{alg:generalise}
  \KwData{$\Automaton$, a product from \Calculus{}, a model $a$ assigning counts to the terms that $\Filter$ associate with each transition of $\Automaton$, and our homomorphism $\Map$ that we want to compute the image modulo.}
  \KwResult{a quantifier-free Presburger formula $P$ representing a partial $\Map$-homomorphic image}
  \SetKwFunction{EliminateQuantifiers}{eliminateQuantifiers}

  $\Automaton' \gets \Tuple{\Automaton_\States, \Automaton_{\InitialState}, \Automaton_\AcceptingStates, \Set{\Transition \in \Automaton_\Transitions \SuchThat a(\Filter(\Transition)) \neq 0}}$

  \KwRet{\EliminateQuantifiers{$\Map(\ParikhMap(\Automaton'))$}}
  \end{algorithm}


  \begin{algorithm}
    \DontPrintSemicolon
    \caption{$\FindImage{}(\Automaton_1 \times \ldots \times \Automaton_k, \Map)$ will find the Presburger form for the product $\Automaton_1 \times \ldots \times \Automaton_k$ modulo a homomorphism $\Map$ where the only free variable is/are the one(s) representing the monoid element of $\Map$.}\label{alg:find-image}
    \KwData{$\Automaton$, a product from \Calculus{}, a model $a$ assigning counts to the terms that $\Filter$ associate with each transition of $\Automaton$, and our homomorphism $\Map$ that we want to compute the image modulo.}
    \KwResult{a quantifier-free Presburger formula $P$ representing a partial $\Map$-homomorphic image}
    \SetKwFunction{NewTheoremProver}{newTheoremProver}
    \SetKwFunction{EliminateQuantifiers}{eliminateQuantifiers}
    \SetKwFunction{FreshVariable}{freshVariable}
    \SetKwFunction{Assert}{assert}
    \SetKwFunction{GetModel}{getModel}
    \SetKwData{ImageVar}{image}
  
$p \gets \NewTheoremProver{}$\;
$\Filter(\Transition) := \FreshVariable{p}$ for every $\Transition \in \Transitions_{\Automaton}$\;
$\MonoidElement \gets$ \FreshVariable{$p$}\;
\Assert{$p, \exists \MonoidElement, \Filter(\Transition) \text{ for every } \Transition \in \Transitions_\Automaton \HoldsThat \ImagePredicate{\Automaton}{\Map}{\Filter}{\MonoidElement} \land \AndComp{\Transition \in \Transitions}{\Filter(\Transition) \geq 0}$}\;
$\ImageVar \gets \bot$\;
\While{$p$ has more models}{
  $\Tuple{\Automaton, a} \gets \GetModel(p)$\;
  $G \gets \Generalise{}(\Automaton, \Filter, a)$\;
  $\ImageVar \gets G \lor \Image$\;
  \Assert{$p, \lnot G$}\;
  }
    \KwRet{\ImageVar}
    \end{algorithm}

    The \GetModel{} function is nonstandard in that it returns both the model
    and its associated automaton. Since algorithm~\ref{alg:find-image} is
    essentially a form of quantifier elimination and therefore an internal affair
    to the theorem prover, this should be considered fair game.
    
    A preliminary implementation of this approach is available as part
    of~\Catra, but has been neither optimised nor fully tested.

\section{Implementation}\label{sec:implementation}

We implement \Calculus{} for Parikh automata as described in
\cref{sec:parikh-automata}. The artefact submitted along with this paper is a
program that reads an instance file with one or more products of one or more
Parikh automaton with transition labels defined as ranges of Unicode characters,
along with a set of constraints on the final values of their registers expressed
as Presburger arithmetic in a C-like syntax. We call this program
\Catra.\footnote{If you really must read it as an acronym, please read it as
CAtegory Theory on Register Automata, or if you object to the somewhat
nonstandard use of register automata and category theory, as Check Assignments
of The Registers Afterwards. Or alternatively if you find it all to be too much
of a theoretical exercise, as Can Anyone Think of A Real Application.}

\Catra{} is written in Scala, with the calculus described in this paper
implemented as a theory plug-in for the \Princess{} automated theorem
prover~\cite{princess}, which also performs the Presburger reasoning. For
comparison, we also provide an implementation of the baseline method
from~\cite{generate-parikh-image}, a direct translation that uses the~\Nuxmv{}
symbolic model checker~\cite{nuxmv} to solve our constraints, and the
approximation described in~\cite{approximate-parikh} on top of the standard
baseline back-end. An example of an input file corresponding to our running
example introduced in \cref{sec:motivation} can be seen in
\cref{lst:input-example}.

\Catra{} uses symbolic labels for automata. A symbolic label is defined as a
finite range of Unicode code points. This allows succinct representation of many
regular expression patterns such as \lstinline{(a-z).*} which would have
otherwise required $27$~transitions. The transition for the range would be
written \lstinline{a -> b [97, 122]}.

In satisfaction mode, supported by all backends, \Catra{} tries to satisfy the
constraints expressed by the input file, reporting \Sat{} with register
assignments or \Unsat{} much like traditional~SAT- or SMT solvers would.
Additionally, baseline and \Calculus{} also support generation of the Presburger
formula describing the constraints of the input file. Baseline uses standard
quantifier elimination, and \Calculus{} uses the method described in
\cref{sec:finding-the-image}.

Since \Princess{} does not support multiple-arity predicates like the ones we
use in \Calculus{}, we have implemented variable-length arguments using
additional helper predicates. These are $\Unused{}(\Automaton)$, which marks an
automaton as unused in any product, and $\TransitionMask{}(\Automaton,
\Transition, \Filter(\Transition))$ which associates a transition $\Transition$
and automaton $\Automaton$ with its corresponding transition variable.
Additionally, we associate each of our predicates with an instance variable in
order to differentiate instances of the predicates.

\subsection{Implementing the Baseline}\label{sec:implementing-baseline}

We baseline using the same Presburger solver (\Princess{}), input file parser,
and automaton implementation as \Catra. We do this in order to better analyse
the impact of the calculus rules themselves. Using the formula of
\eqref{eq:generate-parikh}, we produce quantified Presburger formulae for each
successive term and add them to \Princess. We compute the product incrementally
term by term, checking satisfiability at each step. We use a priority queue to
select automata for each step, and order it by the number of transitions as a
heuristic for the size of the automata. We use this heuristic to avoid computing
large (and therefore slow) products until we have to, banking our hopes on
computing an empty intermittent product early. The pseudocode for our
implementation can be seen in \cref{alg:baseline}.

\begin{algorithm}
  \caption{How we implement the baseline approach}\label{alg:baseline}
  \KwData{$\Automaton_1, \ldots, \Automaton_n$ automata, other constraints $\SomeClause$}
  \KwResult{\textsc{Sat} or \textsc{Unsat}}
  \SetKwFunction{NewTheoremProver}{newTheoremProver}
  \SetKwFunction{NewPriorityQueue}{newPriorityQueue}
  \SetKwFunction{Dequeue}{dequeue}
  \SetKwFunction{Enqueue}{enqueue}
  \SetKwFunction{Assert}{assert}

  $p \gets \NewTheoremProver{}$

  \Assert{$p$, $\SomeClause$}

  \ForEach{$\Automaton_i$}{
    \Assert{$p, \ParikhMap(\Automaton_i)$}

    \If{$p$ is \textsc{Unsat}}{break}

  }

  $q \gets \NewPriorityQueue{}$


  \While{$p$ not \textsc{Unsat} and $|q| > 1$}{
    $\Automaton, \Automaton' \gets \Dequeue{q}$ 
    
    \Assert{$p, \ParikhMap(\Automaton \times \Automaton')$}

    \Enqueue{$q, \Automaton \times \Automaton'$}
  }
  
  \KwRet{$p$'s SAT status}

  \end{algorithm}

As an optimisation, our automata (including intermittent products) are created
forward- and backward- reachable-minimal. Any automaton we produce only contains
states that are both reachable from the initial state and has a path to an
accepting state. We never perform any other minimisation on the automata for
either backend. More complex minimisation was left out since performing
minimisation on automata with counters is non-trivial, and minimising symbolic
automata risks exponential blowup \cite{minimising-symbolic}.

\subsection{Heuristics and search strategies}

There are a number of choices left unspecified in \Calculus{} as described in
\cref{sec:calculus,sec:multiple}. For example, the order of materialisation of
intermediate products and the order of splitting. In this section we describe
additional implementation details and techniques used to enhance \Catra.

\subsubsection{Splitting, Materialisation, and Propagation}

We order our rule applications so that we first propagate connectedness if
possible, then perform materialisation if tractable as defined below, then
finally resort to splitting if we must.

In addition to applying \Split{} as described in \cref{tbl:rules:single} to
randomly selected transitions, we prefer splitting to sever a strongly connected
from the initial state. We randomly select an automaton where we can compute a
cut between an SCC and the initial state, that is where the SCC does not contain
the initial state and where the sum of the transition variables of the
transitions in the cut is not known to be positive. If there are multiple such
strongly connected components we choose one randomly. We then proceed to split
on the sum of the transition variables of the cut as if it were a regular
transition, e.g. its sum being zero or nonzero. In this way we drive \Calculus{}
towards applying \Propagate{}.

The implementation of the connectedness constraint is opportunistic and straightforward. We compute a set of dead states by performing forward and backwards reachability computations on an automaton, where we disregard any transition whose associated variable is known to be zero. After that we add clauses ensuring any transition variable associated with a transition starting in a dead state is zero.

Product materialisation is the final piece of the puzzle. In the current
implementation we put off computing intermediate products until at least all but
two transition variables of one of the automata is known to be either present or
absent. The number was chosen experimentally, and we observe a consistent trend
towards lower numbers being better. The other automaton for the product is
selected randomly.

\subsubsection{Clause Learning}

\Catra{} enables clause learning by default when using our backend, as it has
been experimentally shown to increase the performance in aggregate (though not
strictly). We do not currently implement all the proposed features of
\cref{sec:ext:backjumping}, but we do implement forward-reachability cut
learning. No sophisticated clause learning for products has been implemented.

\subsubsection{Random Restarts}

Finally, we perform restarts scaled by the Luby series~\cite{luby}. Experimental
results have shown this to have a large improvement in performance, which is
unsurprising given how many random choices we make during solving and how
tail-heavy our problem is.

\section{Evaluation}\label{sec:experiments}

We evaluate the performance of \Catra{} on~\NrBenchmarks{} instances of Parikh
automata intersection problems generated by the \OstrichPlus{} string constraint
solver when solving the PyEx benchmarks~\cite{pyex}. After generating an
initial~\InitialNrBenchmarks{},we remove~\NrTrivial{} instances solved in under
five seconds by baseline. We also attempted to benchmark instances generated by \Ostrich{} solving the kaluza-len (\numprint{38227} instances) and pyex-len (\numprint{791} instances) benchmark suites, but had to discard them since they were all trivial. For instance, on a laptop, \Calculus{} could solve every instance from kaluza-len in under five seconds, mean about 0.1s. The benchmarks are run on commit~\texttt{\commit}.

The benchmarks are executed in parallel using GNU Parallel~\cite{parallel},
since they are mostly single-threaded and initial results showed negligible
interference on performance, on \BenchmarkRig{}. We compiled the code using
Scala~\ScalaVersion{}, and executed the experiments on~\JvmVersion{} with a
maximum heap of~\MaxHeapSize{}. We used \Nuxmv{} version~\NuxmvVersion{} invoked
as a subprocess for each instance. Instances were executed in batches of
\BatchSize{}, each given a fresh JVM. We believe this represents a realistic use
case where \Calculus{} is used to support e.g a string solver. Experiments were
executed in random order for all backends. Each instance got a time budget
of~\RuntimeTimeout.

All runtimes are measured in wall-clock time as observed by the JVM when
executing the instance, and exclude time spent parsing (usually far below
\numprint{0.1}s).

\subsection{Execution Time and Ability to Solve Instances}\label{sec:runtime}

In \cref{fig:solve-division} we show how many of the~\NrBenchmarks{} instances left after excluding the trivial instances, among them the \numprint{38227} and \numprint{791} instances generated from kaluza-len and pyex-len respectively,
the respective back-ends could solve and with which status. A full summary of
their outcomes is also available in \cref{tab:solve-status}. Note that we lack
ground truth for all instances. We see that \Calculus{} generally outperforms
\Nuxmv{} on determining unsatisfiability, while being worse at satisfiable
instances. Both \Nuxmv{} and \Calculus{} strictly outperform baseline on every
kind of instance, but most of all on satisfiable instances, and \Calculus{}
outperforms \Nuxmv{}. \Nuxmv{} and \Calculus{} disagree on 38 instances, where
\Nuxmv{} returns \Sat{} and \Calculus{} returns \Unsat{}. We believe this to be
caused by a non-deterministically triggered bug in the clause learning of
\Calculus{}, but have not had time to further investigate it. We have however
validated all \numprint{3908} satisfying assignments from \Calculus{} with
\Nuxmv{}. \Cref{tab:solve-status} also shows a small number of crashing bugs in
our implementation.

Baseline performs worse on satisfiable instances because it executes a heuristic
meant to detect unsatisfiability early at a heavy penalty to satisfiable
instances. The heuristic is enabled since it is unrealistic to assume that we
know in advance whether an instance is satisfiable or not, but we do know that
unsatisfiable instances are more common.

\begin{table}
  \centering
  \input{graphs/solved_pivot_table.tex}
  \caption{The result of running the respective back-ends with a timeout of
  \RuntimeTimeout. Instances solved by no backend within the timeout are omitted
  from the table. }\label{tab:solve-status}
\end{table}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-by-solver.pdf}
    \caption{The division of statuses per backend.}
    \label{fig:solve-division}
    \Description[A bar chart showing three bars, one per backend, illustrating how many instances they could solve]{The bars are divided by satisfiable and unsatisfiable instances. Baseline could seemingly only solve unsatisfiable instances, lazy could solve a few satisfiable and mostly unsatisfiable, and nuxmv could solve about twice as many unsatisfiable as satisfiable instances, and slightly less in total than lazy.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-time-boxplot.pdf}
    \caption{The distribution of runtimes for solved instances per backend. Note that the number of instances solved differs between backends.}
    \label{fig:runtime-boxplot}
    \Description[A box plot showing the distribution of runtime over the three backends]{The middle box plot shows a tiny box centered around 0 seconds for the lazy backend, the rightmost box shows a bigger box between five seconds and 30 seconds for nuxmv, and the leftmost box shows a smaller box between 20 and 30 seconds for baseline. The whiskers of nuxmv span between 0 and the timeout, 60 seconds, while they are much tighter for lazy. On the other hand, lazy has  a lot of outliers.}
  \end{subfigure}
  \vfill
  \begin{subfigure}[b]{0.75\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-cactus.pdf}
    \caption{The number of instances solved as the time budget increases, simulated from one \RuntimeTimeout-timeout run.}
    \label{fig:cactus}
    \Description[A cactus plot comparing the performance of nuxmv, lazy and baseline]{The line for baseline is strictly lower than the two others, and does not head upwards from zero until after 10 seconds of timeout, while the number of instances for nuxmv increases sharply until ca 10 seconds and then linearly after that. Lazy solves much more instances than nuxmv.}
  \end{subfigure}
\end{figure}

\subsubsection{Scalability}\label{sec:scaling}

A cactus plot showing the number of instances solved within a given timeout for
each backend can be seen in \cref{fig:cactus}. Here, we see clearly that
\Calculus{} outperforms \Nuxmv{} in general, but that \Nuxmv{} might have better
long-term scaling properties.

\iffalse
\subsection{Finding a Presburger Formula}\label{sec:evaluation:finding-image}

For baseline and \Calculus{}, \Catra{} offers the ability to find the equivalent Presburger formula representing a given instance. For baseline, we use the built-in quantifier elimination facilities of the underlying \Princess{} theorem prover, while for \Catra{} we use the specially tailored approach described in \cref{sec:finding-the-image}. For this experiment, we use only the~\NrKnownSat{} instances known to be satisfiable from the previous experiment detailed in \cref{sec:scaling,sec:runtime}. 

To make sure baseline puts up as much competition as possible, we disable
checking intermittent satisfiability and configure \Catra{} to run in the
maximally eager mode where the product is first computed before any
satisifiability check is performed. We run the experiments with a timeout
of~\ImageTimeout{}. The results of the experiment is summarised in
\cref{fig:cactus:image} and \cref{tab:image-results}. \Fudge{We see here that
something happens}.

\begin{figure}[ht]
  \caption{The number of instances \Catra{} was able to find the Presburger form of the image for within a given number of seconds per backend.}
  \label{fig:cactus:image}
\end{figure}
\fi

\subsection{Threats to Validity}

The most obvious threats to validity would be poor benchmarking or poor
implementation, e.g. if the method described in \cref{sec:calculus,sec:multiple}
deviate from what is actually benchmarked in section \cref{sec:experiments}, or
if the methods used for benchmarking would be unsound.  To increase the
probability that our results are representative both in the sense of
representing  on expected inputs and in the statistical sense for a given run of
\Catra{} despite the use of randomness in our implementation we execute many
experiments. Moreover, to address the issue of correctness we have validated all
reported solutions made by \Calculus{} with \Nuxmv{} to ensure that \Calculus{}
is indeed sound.

Another threat to validity would be if the competition (baseline and \Nuxmv)
would be disadvantaged in our comparison. For \Nuxmv{} we use the default
configuration which we believe should be performant (or it should not be the
default). Additionally, tweaking our invocation of \Nuxmv{} is explicitly made
easy for artefact reviewers. For baseline, our best argument is that the
solution we use is close to the one found in the \Ostrich{} string solver and
that it therefore should be realistic.

The greatest threat to validity is our choice of implementation platform and
automata library. There are some signs that product computation is inefficient,
notably the good performance of low-threshold automata materialisation that
prioritises computing smaller products. This means that \Calculus{} makes less
heavy use of product computation than baseline does, instead relying more on
\Princess{}'. This situation would unfairly advantage \Calculus{} if our
automata library was the bottleneck due to our automata library being
unnaturally poor. We believe this is unlikely since similar performance issues
have been reported for string solvers. Additionally, profiling suggests that
both \Princess{}-based backends spend most of their time in \Princess{},
suggesting that the automaton implementation is not the bottleneck.

\section{Conclusion}

In this paper we have introduced a calculus to compute commutating operations on
intersections of regular languages that we call \Calculus{}. We have evaluated
it on \NrBenchmarks{} Parikh automata intersection problems generated by the
\OstrichPlus{} string solver \cite{ostrich-plus} solving the PyEx benchmark
suite \cite{pyex} using our Parikh automata solver \Catra{}.

Within \Catra{}, \Calculus{} shows astonishing performance in terms of memory
usage and solve-time compared to the baseline approach laid out in
\cite{generate-parikh-image} when implemented on the same underlying automated
theorem prover (\Princess{}, \cite{princess}). It is also competitive with the
\Nuxmv{} model checker \cite{nuxmv}, outperforming it on unsatisfiable instances
and generally outperforming it for timeouts under 30 seconds with its advantage
increasing drastically for even shorter timeouts. 30 seconds would generally be
considered a long timeout for our intended use as supporting infrastructure to a
string constraint solver.

Future investigations involve two tracks. The first one is integration into
existing string solvers (wich \Ostrich{} being a particularly promising
candidate due to its shared use of \Princess{}), and further adaptation to that
use case. Closer inspection of the instances where we currently time out should
be useful to further improve our heuristics.

The second track for future improvements is the extension into other problem
domains, including other logics, model checking problems, as well as to more
powerful automata such as transducers. In principle, we are also already able to
express stronger constraints than Parikh automata, due to our use of a full
automated theorem prover which allows adding arbitrary constraints in addition
to the expected Presburger formulae.

%% Acknowledgments
% \begin{acks}                            %% acks environment is optional
%                                         %% contents suppressed with 'anonymous'
%   %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%   %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%   %% acknowledge financial support and will be used by metadata
%   %% extraction tools.
%   This material is based upon work supported by the
%   \grantsponsor{GS100000001}{National Science
%     Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%   No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%   No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%   conclusions or recommendations expressed in this material are those
%   of the author and do not necessarily reflect the views of the
%   National Science Foundation.
% \end{acks}


%% Bibliography
\bibliography{bibliography}


%% Appendix
\appendix
\section{Appendix}
\subsection{Per-instance comparison between backends}
\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-duels-lazy-baseline-scatter.pdf}
    \caption{Per-instance runtime between baseline and \Calculus{}}
    \label{fig:duels:baseline}
    \Description[A scatter plot showing individual instances as dots compared to \Calculus{} and baseline]{The plot shows most dots at the bottom of the plot, with some plots collected at the right-hand side.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-duels-lazy-nuxmv-scatter.pdf}
    \caption{Per-instance runtime between baseline and \Calculus{}}
    \label{fig:duels:nuxmv}
    \Description[A scatter plot showing individual instances as dots compared to \Calculus{} and \Nuxmv{}]{The plot shows flames of dots between the bottom and top, with a line of dots near the right-hand side corresponding to timeouts.}
  \end{subfigure}
\end{figure}
\subsection{Example instance for \Catra{}}
\begin{lstlisting}[caption={An example input file for \Catra{} for the problem introduced in \cref{sec:motivation}, illustrating every major syntax element. From beginning to end: synchronised (product) automata using the keyword \texttt{synchronised} (automata A and B), labels (except those with ranges), register increments, and constraints on the final values of their counters.}, label=lst:input-example]
  // So far we only support integer counters. Note that we have individual
  // counters for each automaton to avoid surprises for product construction.
  counter int l_a, l_b, l_c, r_c;

  synchronised {
  automaton aca_or_bc {
    init S;
  
    // We use ASCII values here, this is for lowercase a
    S -> A [97] { l_a += 1 };
    S -> B [98] { l_b += 1 };
  
    B -> S  [98] { l_b += 1 };
    B -> final [99] { l_c += 1 };
  
    A -> A [99] { l_c += 1 };
    A -> final [97] { l_a += 1 };
  
    accepting final;
  };
  
  automaton something_c_something {
      init S;
  
      // Special short-hand value for the whole Unicode alphabet.
      S -> S [any] ;
      S -> F [99] { r_c += 1 };
  
      F -> F [any];
  
      accepting F;
  };
  };
  
  // Constrain the number of a:s to be larger than the number of c:s. Since the
  // automata are synchronised on every transition and counters are guaranteed
  // to be consistent these constraints are sufficient without also constraining
  // r_c.
  constraint l_a > l_c;
\end{lstlisting}

% Text of appendix \ldots

\end{document}
