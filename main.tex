%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[acmsmall,review,anonymous,screen]{acmart}\settopmatter{printfolios=true,printccs=true,printacmref=true}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[acmsmall]{acmart}\settopmatter{}


%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{POPL} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2023}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format must update the '\documentclass' and
%% topmatter commands above; see 'acmart-sigplanproc-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption



\usepackage{amsmath,empheq,fancybox}
\usepackage{paralist}
\usepackage{url}
\usepackage{color}
\usepackage{textcomp,listings}
\usepackage{array}
\usepackage{mymacros}
\usepackage{microtype}
\usepackage{listings}
\usepackage{csquotes}
\usepackage{proof}
\usepackage[capitalise]{cleveref}
\usepackage{algorithm2e}      
\usepackage{multirow}
\usepackage{mathpartir}
\usepackage{amsthm}
\usepackage{numprint}
\usepackage{ebproof}
\usepackage[makeroom]{cancel}

\usepackage{mathtools} % Bonus

\usepackage{enumitem}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\newlist{constraints}{enumerate}{3}
\setlist[constraints,1]{%
    label=\sffamily{(\roman*):},
    ref=\normalfont{Constraint (\roman*)},
    wide,itemsep=0pt,topsep=0pt
}    
\crefname{constraintsi}{}{}
\Crefname{constraintsi}{}{}


\lstset{
    columns=fullflexible,
    showspaces=false,
    showtabs=false,
    breaklines=true,
    showstringspaces=false,
    breakatwhitespace=true,
    escapeinside={(*@}{@*)},
    commentstyle=\color{greencomments},
    keywordstyle=\color{bluekeywords},
    stringstyle=\color{redstrings},
    numberstyle=\color{graynumbers},
    basicstyle=\ttfamily\small,
    framesep=12pt,
    xleftmargin=12pt,
    tabsize=4,
    captionpos=b
}


\newif\ifcomments
\commentstrue
\newif\ifoutline
\outlinetrue

\newcommand{\contents}[1]{\ifoutline{\color{blue}
    \begin{itemize}
    #1
    \end{itemize}
  }\fi}

\allowdisplaybreaks[1]


\begin{document}

%% Title information
\title{A Constraint Solving Approach to Parikh Images of Regular Languages}
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}                   %% \country is recommended
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
  A common problem in string constraint solvers is computing the Parikh image, a
   set of linear equations that describe all possible combinations of character
   counts in strings of a given language. Automata-based string solvers
   frequently need to compute the Parikh image of large products (intersections)
   of nondeterministic automata, which in many operations is both prohibitively
   slow and memory-intensive. We contribute a novel understanding of how Parikh
   maps can be tackled as a constraint solving problem to solve real-world
   constraints stemming from functions on regular languages, most notably the
   length constraint. Furthermore, we show how this formulation can be
   efficiently implemented as a calculus, \Calculus{}, in an automated theorem
   prover supporting Presburger logic.  We implement \Calculus{} in a tool
   called \Catra{}, and evaluate it on constraints produced by the
   \OstrichPlus{} string constraint solver when solving Parikh automata
   intersection problems produced when solving standard string constraint
   benchmarks involving string length constraints. We show that our solution
   strictly outperforms the standard approach described
   in~\citeauthor{generate-parikh-image} as well as the over-approximating
   method recently described by~\citeauthor{approximate-parikh} by a wide
   marigin, and for realistic timeouts for constraint solving also the~\Nuxmv{}
   model checker.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>10003752.10003766.10003773.10003775</concept_id>
  <concept_desc>Theory of computation~Quantitative automata</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10003752.10003766.10003776</concept_id>
  <concept_desc>Theory of computation~Regular languages</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10003752.10003790.10003794</concept_id>
  <concept_desc>Theory of computation~Automated reasoning</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10003752.10010124.10010138.10010142</concept_id>
  <concept_desc>Theory of computation~Program verification</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>10003752.10003790.10011192</concept_id>
  <concept_desc>Theory of computation~Verification by model checking</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  </ccs2012>  
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Quantitative automata}
\ccsdesc[300]{Theory of computation~Regular languages}
\ccsdesc[500]{Theory of computation~Automated reasoning}
\ccsdesc[300]{Theory of computation~Program verification}
\ccsdesc[300]{Theory of computation~Verification by model checking}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{Parikh images, string solvers, model checking}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}\label{sec:introduction}
\input{introduction.tex}

\section{An Intuition for Our Approach}\label{sec:intuition}
\input{example.tex}

\section{Preliminaries}\label{sec:preliminaries}
\input{preliminaries.tex}

\section{Projections on Parikh Images}\label{sec:generalised}
\input{generalised.tex}

\section{A Calculus for Projections on Parikh Images}\label{sec:calculus}
\input{calculus.tex}

\section{Parikh Images from Products of Automata}\label{sec:multiple}

We now generalise our calculus to natively work with intersections of
regular languages, or equivalently products of automata. For this
extension, we change the main predicate~$\Image$ to be indexed by a
vector of automata~$\Tuple{\Automaton_1, \ldots, \Automaton_k}$.  For
simplicity, we assume that the sets of states of the $k$ automata (and
therefore also the transition sets) are pairwise disjoint. 
\begin{definition}\label{def:multiple}
  Suppose $\Automaton_1, \ldots, \Automaton_k$ are automata,
  $\Map : \Alphabet^* \to \Monoid$ is a map to a commutative monoid
  $\Monoid$,
  $\Filter: \bigcup_{i=1}^k \Transitions_{\Automaton_i} \to \Naturals$
  is a transition selection function, and
  $\MonoidElement \in \Monoid$. The predicate
  $\ImagePredicate{\Tuple{\Automaton_1, \ldots,
      \Automaton_k}}{\Map}{\Filter}{\MonoidElement}$ is true exactly
  when there are accepting paths~$\Path_1, \ldots, \Path_k$ of the
  respective automata, such that for each $i \in \{1, \ldots, k\}$ and
  $\Transition \in \Transitions_{\Automaton_i}$ it holds that
  \begin{itemize}
  \item the multiplicity of $\Transition$ on $\Path_i$ is consistent with
    $\Filter$, that is,
    $\Filter(\Transition) = \TransitionCount(\Transition, \Path_i)$,
  \item the automata all accept the same
    word~$\WordOf(\Path_i) = \WordOf(\Path_1)$, and
  \item the accepted word is mapped to
    $\MonoidElement = \Map(\WordOf(\Path_i)) =
    \Map(\WordOf(\Path_1))$.
  \end{itemize}
\end{definition}

  \begin{table}[t]
    \begin{tabular}{@{}l>{$}c<{$}p{3cm}@{}}\toprule
      Name & \text{Rule} & Side conditions\\
      \midrule
    
      % EXPAND
      \ExpandM & 
      \inferrule
      {
       {\begin{array}{c}
           \left\{
           \FlowEq(\Automaton_i, \Filter),~~
           \Connected(\Automaton_i, \Filter),~~
           \MonoidElement = \sum_{\Transition \in \Transitions_{\Automaton_i}} \Filter(\Transition) \cdot \Map(\Transition)
           \right\}_{i=1}^k,
           \\[4ex]
          \ImagePredicate{\Tuple{\Automaton_1, \ldots, \Automaton_k}}{\Map}{\Filter}{\MonoidElement}, ~
          \SomeInequalities, \SomeClause
        \end{array}}
        }
      {\ImagePredicate{\Tuple{\Automaton_1, \ldots, \Automaton_k}}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause} & 
      None \\[5ex]
      \Materialise &
      \inferrule
      {{
         \begin{array}{c}
           \FlowEq(\Automaton', \Filter'),~
           \Connected(\Automaton', \Filter'),~
           \BindingSum(\Automaton_i, \Automaton_j, \Filter')
           \\\ImagePredicate{
                     \Tuple{\Automaton_1, \Automaton_{i-1}, \Automaton_{i+1}, \ldots,
                     \Automaton_{j-1}, \Automaton_{j+1}, \ldots, \Automaton_k,
                     \Automaton'},~ \SomeInequalities, \SomeClause
           }{\Map}{\Filter'}{\MonoidElement}
           \end{array}}
           }
      {\ImagePredicate{\Tuple{\Automaton_1, \ldots, \Automaton_i, \ldots, \Automaton_j, \ldots, \Automaton_k}}{\Map}{\Filter}{\MonoidElement},~ \SomeInequalities, \SomeClause} &
      $\begin{array}{@{}l@{}}
         \Filter' = \Extend(\Automaton_i, \Automaton_j,\Filter),\\
         1 \leq i < j \leq k, \\
      \Automaton' = \Automaton_i \times \Automaton_j,\\
      \end{array}$ \\
      \bottomrule
      \end{tabular}
      \caption{Additional derivation rules for products of arbitrarily many automata.}\label{tbl:rules:multi}
    \end{table}

    For the calculus (\cref{tbl:rules:multi}), we first extend
    $\Expand$ to generate flow equations and instances of $\Connected$
    for each automaton, resulting in a new rule $\ExpandM$.  Unlike
    \Expand{}, \ExpandM{} does not remove the $\Image$~predicate,
    since it is needed to keep track of the currently considered
    partial products.

The rule~$\Materialise$ introduces the product of two individual
automata~$\Automaton_i, \Automaton_j$; this step eliminates
$\Automaton_i, \Automaton_j$ as index of the $\Image$ predicate, and
instead adds the product~$\Automaton_i \times \Automaton_j$, while
also introducing the flow equations and the $\Connected$ predicate.

The rule~$\Materialise$ also has to connect the newly introduced
product~$\Automaton_i \times \Automaton_j$ to the previous automata
$\Automaton_i, \Automaton_j$. This is done by extending the
selection function~$\Filter$ to $\Filter'$, mapping the transitions of the product
to fresh variables~$x_{\Transition}$:
\begin{equation*}
  \Extend(\Automaton_i, \Automaton_j,\sigma)(t)
  ~~=~~
  \begin{cases}
    x_{\Transition} & t \in \Transitions_{\Automaton_i \times \Automaton_j}
    \\
    \Filter(t) & \text{otherwise}
  \end{cases}
\end{equation*}

The multiplicity of transitions in the product then has to be related
to the multiplicities in the individual automata, modelled using the
$\BindingSum$ predicate. The predicate expresses that the multiplicity
of a transition~$\Transition \in \Transitions_{\Automaton_i}$ in
$\Automaton_i$ has to coincide with the sum of the multiplicities of
transitions in $\Automaton_i \times \Automaton_j$ derived from $t$,
and similarly for $\Automaton_j$:
%
  $$
  \BindingSum(\Automaton_i, \Automaton_j, \Filter') ~~=~~
  \begin{aligned}
  & \Set{ 
    \Filter'(\Transition)  =  \sum\limits_{\Transition' = \FromLabelTo{\Tuple{\State, \State_R}}{\Label}{\Tuple{\State', \State_R}}} \Filter'(\Transition')
  \SuchThat \Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions_{\Automaton_i} } \cup\mbox{} \\ 
  & \Set{
    \Filter'(\Transition)  =  \sum\limits_{\Transition' = \FromLabelTo{\Tuple{\State_L, \State}}{\Label}{\Tuple{\State_L, \State'}}} \Filter'(\Transition') \SuchThat \Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions_{\Automaton_j}
  }
  \end{aligned}
$$

\iffalse
Note that this definition implies that $\Filter(\Transition) =
\Filter(\Transition')$ whenever two transitions $\Transition \in
\Transitions_{\Automaton_1}, \Transition' \in \Transitions_{\Automaton_2}$
produces a product transition $\Transition'' \in \Transitions_{\Automaton_1
\times \Automaton_2}$. This corresponds to our intuition that the terms of the
product must agree on the value they accept. As before, we implicitly map
$\Filter$ to fresh terms for each transition in the product.
\fi

For instances of precisely one automaton, neither rule applies and we
perform the calculus as before.

\subsection{An Example}\label{sec:multiple:example}

We return again to our example in \cref{sec:introduction:parikh}, where we
compute the whole Parikh image of the product of $\AcaOrBc{}$ and
$\SomethingCSomething$ under the constraint that there are more instances of
letters a than b. This time our monoid $\Monoid$ is 3-dimensional vectors with
element-wise addition, and $\Map$ that maps each transition to the corresponding
increment vector, e.g $\Map(\FromLabelTo{S}{a}{A}) = \VectorLiteral{1,0,0}$.


In the interest of space, we refer back to
\cref{fig:example:single:equivalences} for the definitions of $\Filter$ for
$\AcaOrBc$ and only define $\Filter$ for $\SomethingCSomething{}$ after
substitutions as follows. Note the expansion of the $\Sigma$ labels:
    \begin{equation*}
      \begin{aligned}
        % & \Filter(\FromLabelTo{S}{a}{A}) & = 1 - \TransitionVar_6 \\
        % & \Filter(\FromLabelTo{S}{b}{B}) & = \TransitionVar_4 + \TransitionVar_6 \\
        % & \Filter(\FromLabelTo{A}{c}{A})  & = \TransitionVar_3  \\
        % & \Filter(\FromLabelTo{B}{b}{S}) & = 2\TransitionVar_6 + \TransitionVar_4 \\
        % & \Filter(\FromLabelTo{A}{a}{F}) & = 1 - \TransitionVar_6 \\
        % & \Filter(\FromLabelTo{B}{c}{F}) & = \TransitionVar_6 \\
        % Other automaton
        \Filter(\FromLabelTo{S}{\Sigma}{S}) & = \VectorLiteral{\TransitionVar_{7a}, \TransitionVar_{7b}, \TransitionVar_{7c}} \\
        \Filter(\FromLabelTo{S}{c}{F}) & = 1 \\
        \Filter(\FromLabelTo{F}{\Sigma}{F}) & = \VectorLiteral{\TransitionVar_{9a}, \TransitionVar_{9b}, \TransitionVar_{9c}} \\
      \end{aligned}
    \end{equation*}
    
In the derivation tree of \cref{fig:derivation:multi} we start as before, but
with the product version of the $\Image$ predicate. The only possible rule here
is $\ExpandM$, which we use to add the corresponding constraints on each
automata of the product as we would have had in the single-automaton version. We
perform algebraic reasoning on those equations, along with the constraint on
$\MonoidElement$ to determine bounds on transition variables that will enable us
to \Subsume{} and remove one of the $\Connected{}$ predicates. When we have
finished doing so, we use \Materialise{} to compute the product of the now
filtered automata. Further algebraic reasoning allows us to remove the final
instance of $\Connected$. We then perform \Expand{} to get rid of the final
$\Image$ instance, and are immediately able to $\Subsume$ the resulting
$\Connected{}$ predicate. This, again, leaves us with a set of linear
inequalities at the leaf that we can use to obtain a model of our final
$\MonoidElement$ values (now a vector). This time we can read out aca, or
$\VectorLiteral{2, 0, 1}$.

\begin{figure}
  \centering
\begin{prooftree}
  \hypo{
    {\begin{array}{lll}
      1 = \TransitionVar_{10} & \land \TransitionVar_3 = \TransitionVar_{11} & \land 1 = \TransitionVar{12} \\ 
      \land 1 = \TransitionVar_{11} &  & \land 2 > 0
    \end{array}}  
  }
  \infer1[\Subsume{}, \Expand{}, \Subsume{}]{
    {\begin{array}{ll}
      2 > 0 \land 1 = \TransitionVar_{10} & \land \TransitionVar_3 = \TransitionVar_{11} \\
      \land 1 = \TransitionVar_{12} & \land 1 = \TransitionVar_{11} \\
      \land \Image{}_{\Automaton', \Map}(\Filter, \VectorLiteral{2, 0, 1}) & \land \Connected(\SomethingCSomething{}, \Filter) \\
    \end{array}}  
  }
  \infer1[\EquationReasoning{}]{
  {  \begin{array}{ll}
      1 = \TransitionVar_{10} \land \TransitionVar_3 = \TransitionVar_{11} & 
      \land 1 = \TransitionVar_{12} \land 1 = \TransitionVar_{11} \\
      \land \Image{}_{\Automaton', \Map}(\Filter, 
      \VectorLiteral{
        \TransitionVar_{7a} + \TransitionVar_{9a},
        0,
        \TransitionVar_{7c} + \TransitionVar_{9c} + 1}) & \land 
        \TransitionVar_6 + \TransitionVar_4 = 0 \\
        \land 1 - \TransitionVar_6 > 0 \land \TransitionVar_6 = 0 &
        \land \TransitionVar_4 = 0 \land  \TransitionVar_3 > 0 \\
        \land 2\TransitionVar_6 + \TransitionVar_4 = 0 & \land 
          \VectorLiteral{
        \TransitionVar_{7a} + \TransitionVar_{9a},
        0,
        \TransitionVar_{7c} + \TransitionVar_{9c} + 1} 
        = \VectorLiteral{
          2,
          0,
          \TransitionVar_3} \\ \land 
      \Connected(\SomethingCSomething{}, \Filter) &
      \land \TransitionVar_{7a} + \TransitionVar_{9a} > 0
    \end{array} } 
  }
  \infer1[\Materialise]{
    {\begin{array}{ll}
      \TransitionVar_6 + \TransitionVar_4 = 0 \land 1 - \TransitionVar_6 > 0 &
      \land \TransitionVar_6 = 0  \land \TransitionVar_4 = 0 \\
      \land \TransitionVar_3 > 0 & \land 2\TransitionVar_6 + \TransitionVar_4 = 0 \\
      \land 
      \VectorLiteral{
        \TransitionVar_{7a} + \TransitionVar_{9a},
        0,
        \TransitionVar_{7c} + \TransitionVar_{9c} + 1}
        = \VectorLiteral{
          2,
          0,
          \TransitionVar_3} & \land
      \Connected(\SomethingCSomething{}, \Filter) \\
      \land 
      \Image{}_{\Tuple{\AcaOrBc{},\SomethingCSomething{}}, \Map}(\Filter, 
      \VectorLiteral{\TransitionVar_{7a} + \TransitionVar_{9a}, 0, \TransitionVar_{7c} + \TransitionVar_{9c} + 1}) & \land \TransitionVar_{7a} + \TransitionVar_{9a} > 0
    \end{array}}
  }
  \infer1[\Subsume{}]{
  {\begin{array}{ll}
    \TransitionVar_6 + \TransitionVar_4 = 0 \land 
    1 - \TransitionVar_6 > 0 &
    \land
    \TransitionVar_6 = 0 \land 
    \TransitionVar_4 = 0 \\
    \land
    \TransitionVar_3 > 0 & \land
    2\TransitionVar_6 + \TransitionVar_4 = 0 \\ 
    \land
    \VectorLiteral{
      \TransitionVar_{7a} + \TransitionVar_{9a},
      0,
      \TransitionVar_{7c} + \TransitionVar_{9c} + 1}
      = \VectorLiteral{
        2,
        0,
        \TransitionVar_3} & \land 
    \Connected(\AcaOrBc{}, \Filter) \\
    \land 
    \Connected(\SomethingCSomething{}, \Filter) \\
    \land 
    \Image{}_{\Tuple{\AcaOrBc{},\SomethingCSomething{}}, \Map}(\Filter, 
    \VectorLiteral{\TransitionVar_{7a} + \TransitionVar_{9a}, 0, \TransitionVar_{7c} + \TransitionVar_{9c} + 1}) & \land \TransitionVar_{7a} + \TransitionVar_{9a} > 0
  \end{array}}
  }
  \infer1[\EquationReasoning]{
    \begin{aligned}
      \VectorLiteral{a, b, c} = \VectorLiteral{
          2 - 2\TransitionVar_6,
          2\TransitionVar_4 + 3\TransitionVar_6,
          \TransitionVar_3 + \TransitionVar_6
        } \land
        \\
        \VectorLiteral{a, b, c} = \VectorLiteral{
            \TransitionVar_{7a} + \TransitionVar_{9a},
            \TransitionVar_{7b} + \TransitionVar_{9b},
            \TransitionVar_{7c} + \TransitionVar_{9c} + 1
          } \land \\
      \Connected(\AcaOrBc{}, \Filter) \land 
      \Connected(\SomethingCSomething{}, \Filter) \land \\
      \Image{}_{\Tuple{\AcaOrBc{},\SomethingCSomething{}}, \Map}(\Filter, 
      \VectorLiteral{a, b, c}) \land a > b
    \end{aligned}
  }
  \infer1[\ExpandM]{\Image{}_{\Tuple{\AcaOrBc{},\SomethingCSomething{}}, \Map}(\Filter, \VectorLiteral{a, b, c}) \land a > b}
\end{prooftree}
\caption{A derivation for \Calculus{} on the Parikh image of strings with more a's than b's in the product of $\AcaOrBc{}$ and $\SomethingCSomething{}$.}\label{fig:derivation:multi}
\end{figure}

\subsection{Correctness of \Calculus{} for Products of Automata}

Since \cref{tbl:rules:multi} only extends the existing rules of
\cref{tbl:rules:single}, we focus on the differences compared
to the calculus for a single automaton.

%\subsubsection{\Calculus{} for products of automata terminates}
\begin{lemma}\label{lma:multi-terminates}
  Suppose $\SomeClause{}$ is a set of formulas in which the product
  version of $\Image$ only occurs positively. There is no
  infinite sequence of proofs~$P_0, P_1, P_2, \ldots$ in which $P_0$
  has $\SomeClause{}$ as root, and each $P_{i+1}$ is derived from
  $P_i$ by applying one of the rules in \cref{tbl:rules:multi}.
\end{lemma}

\begin{proof}
  The rule \Materialise{} can similarly only be used finitely many times, as
  each application reduces the number of automata in the product of $\Image$ by
  one automaton, until only one remains and \cref{lma:single-terminates} for
  single-automaton instances applies.
  
  This implies that also the rule~\ExpandM{} can only be applied
  finitely often since its side condition only allows applying it once
  per instance of an $\Image$ predicate containing a product, and only finitely
  many instances of $\Image$ can be introduced on each branch.
\end{proof}

%\subsubsection{The rules in \cref{tbl:rules:multi} are solution-preserving}

Since our calculus now includes a rule introducing new variables, the
\Materialise{} rule, we have to slightly generalise the notion of
solution-preservation:
%
\begin{lemma}\label{lma:multi-correct}
  Consider an application of one of the rules in
  \cref{tbl:rules:multi}, with
  premises~$\SomeClause_1, \ldots, \SomeClause_k$ and
  conclusion~$\SomeClause$. An assignment~$\beta$ (over the symbols in
  $\SomeClause$) satisfies the conclusion~$\SomeClause$ if and only if
  there is an extension~$\beta' of \beta$ satisfying one of the
  premises~$\SomeClause_i$.
\end{lemma}

\begin{proof}
  We have to consider the two new rules in \cref{tbl:rules:multi}. The
  result is immediate for \ExpandM{}, since this rule does not remove the
  $\Image$ predicate from a proof goal, and the newly introduced formulas
  are all implied by the $\Image$ predicate.

  For \Materialise{}, observe that the existence of an accepting path
  in $\Automaton_i \times \Automaton_j$ is equivalent to the existence
  of individual paths in $\Automaton_i, \Automaton_j$ accepting the
  same word. The path in the product will satisfy the flow equations
  and connectedness, and it will be related to the individual paths as
  stipulated by the \BindingSum{} predicate.
\end{proof}


\section{Extensions}\label{sec:extensions}
\input{extensions.tex}

\section{Implementation}\label{sec:implementation}

We implement \Calculus{} for Parikh automata as described in
\cref{sec:parikh-automata}. The artefact submitted along with this paper is a
program that reads an instance file with one or more products of one or more
Parikh automaton with transition labels defined as ranges of Unicode characters,
along with a set of constraints on the final values of their registers expressed
as Presburger arithmetic in a C-like syntax. We call this program
\Catra.
% \footnote{If you really must read it as an acronym, please read it as
%CAtegory Theory on Register Automata, or if you object to the somewhat
%nonstandard use of register automata and category theory, as Check Assignments
%of The Registers Afterwards. Or alternatively if you find it all to be too much
%of a theoretical exercise, as Can Anyone Think of A Real Application.}

\Catra{} is written in Scala, with the calculus described in this paper
implemented as a theory plug-in for the \Princess{} automated theorem
prover~\cite{princess}, which also performs the Presburger reasoning. For
comparison, we also provide an implementation of the baseline method
from~\cite{generate-parikh-image}, a direct translation that uses the~\Nuxmv{}
symbolic model checker~\cite{nuxmv} to solve our constraints, and the
approximation described in~\cite{approximate-parikh} on top of the standard
baseline back-end. An example of an input file corresponding to our running
example introduced in \cref{sec:motivation} can be seen in
\cref{lst:input-example}.

\Catra{} uses symbolic labels for automata. A symbolic label is defined as a
finite range of Unicode code points. This allows succinct representation of many
regular expression patterns such as \lstinline{(a-z).*} which would have
otherwise required $27$~transitions. The transition for the range would be
written \lstinline{a -> b [97, 122]}.

In satisfaction mode, supported by all backends, \Catra{} tries to satisfy the
constraints expressed by the input file, reporting \Sat{} with register
assignments or \Unsat{} much like traditional~SAT- or SMT solvers would.
Additionally, baseline and \Calculus{} also support generation of the Presburger
formula describing the constraints of the input file. Baseline uses standard
quantifier elimination, and \Calculus{} uses the method described in
\cref{sec:finding-the-image}.

Since \Princess{} does not support multiple-arity predicates like the ones we
use in \Calculus{}, we have implemented variable-length arguments using
additional helper predicates. These are $\Unused{}(\Automaton)$, which marks an
automaton as unused in any product, and $\TransitionMask{}(\Automaton,
\Transition, \Filter(\Transition))$ which associates a transition $\Transition$
and automaton $\Automaton$ with its corresponding transition variable.
Additionally, we associate each of our predicates with an instance variable in
order to differentiate instances of the predicates.

\subsection{Implementing the Baseline}\label{sec:implementing-baseline}

We baseline using the same Presburger solver (\Princess{}), input file parser,
and automaton implementation as \Catra. We do this in order to better analyse
the impact of the calculus rules themselves. Using the formula of
\eqref{eq:generate-parikh}, we produce quantified Presburger formulae for each
successive term and add them to \Princess. We compute the product incrementally
term by term, checking satisfiability at each step. We use a priority queue to
select automata for each step, and order it by the number of transitions as a
heuristic for the size of the automata. We use this heuristic to avoid computing
large (and therefore slow) products until we have to, banking our hopes on
computing an empty intermittent product early.

\begin{algorithm}
  \caption{How we implement the baseline approach}\label{alg:baseline}
  \KwData{$\Automaton_1, \ldots, \Automaton_n$ automata, other constraints $\SomeClause$}
  \KwResult{\textsc{Sat} or \textsc{Unsat}}
  \SetKwFunction{NewTheoremProver}{newTheoremProver}
  \SetKwFunction{NewPriorityQueue}{newPriorityQueue}
  \SetKwFunction{Dequeue}{dequeue}
  \SetKwFunction{Enqueue}{enqueue}
  \SetKwFunction{Assert}{assert}

  $p \gets \NewTheoremProver{}$

  \Assert{$p$, $\SomeClause$}

  \ForEach{$\Automaton_i$}{
    \Assert{$p, \ParikhMap(\Automaton_i)$}

    \If{$p$ is \textsc{Unsat}}{break}

  }

  $q \gets \NewPriorityQueue{}$


  \While{$p$ not \textsc{Unsat} and $|q| > 1$}{
    $\Automaton, \Automaton' \gets \Dequeue{q}$ 
    
    \Assert{$p, \ParikhMap(\Automaton \times \Automaton')$}

    \Enqueue{$q, \Automaton \times \Automaton'$}
  }
  
  \KwRet{$p$'s SAT status}

  \end{algorithm}

As an optimisation, our automata (including intermittent products) are created
forward- and backward- reachable-minimal. Any automaton we produce only contains
states that are both reachable from the initial state and has a path to an
accepting state. We never perform any other minimisation on the automata for
either backend. More complex minimisation was left out since performing
minimisation on automata with counters is non-trivial, and minimising symbolic
automata risks exponential blowup \cite{minimising-symbolic}.

\subsection{Heuristics and search strategies}

There are a number of choices left unspecified in \Calculus{} as described in
\cref{sec:calculus,sec:multiple}. For example, the order of materialisation of
intermediate products and the order of splitting. In this section we describe
additional implementation details and techniques used to enhance \Catra.

\subsubsection{Splitting, Materialisation, and Propagation}

We order our rule applications so that we first propagate connectedness if
possible, then perform materialisation if tractable as defined below, then
finally resort to splitting if we must.

In addition to applying \Split{} as described in \cref{tbl:rules:single} to
randomly selected transitions, we prefer splitting to sever a strongly connected
from the initial state. We randomly select an automaton where we can compute a
cut between an SCC and the initial state, that is where the SCC does not contain
the initial state and where the sum of the transition variables of the
transitions in the cut is not known to be positive. If there are multiple such
strongly connected components we choose one randomly. We then proceed to split
on the sum of the transition variables of the cut as if it were a regular
transition, e.g. its sum being zero or nonzero. In this way we drive \Calculus{}
towards applying \Propagate{}.

The implementation of the connectedness constraint is opportunistic and straightforward. We compute a set of dead states by performing forward and backwards reachability computations on an automaton, where we disregard any transition whose associated variable is known to be zero. After that we add clauses ensuring any transition variable associated with a transition starting in a dead state is zero.

Product materialisation is the final piece of the puzzle. In the current
implementation we put off computing intermediate products until at least all but
two transition variables of one of the automata is known to be either present or
absent. The number was chosen experimentally, and we observe a consistent trend
towards lower numbers being better. The other automaton for the product is
selected randomly.

\subsubsection{Clause Learning}

\Catra{} enables clause learning by default when using our backend, as it has
been experimentally shown to increase the performance in aggregate (though not
strictly). We do not currently implement all the proposed features of
\cref{sec:ext:backjumping}, but we do implement forward-reachability cut
learning. No sophisticated clause learning for products has been implemented.

\subsubsection{Random Restarts}

Finally, we perform restarts scaled by the Luby series~\cite{luby}. Experimental
results have shown this to have a large improvement in performance, which is
unsurprising given how many random choices we make during solving and how
tail-heavy our problem is.

\section{Evaluation}\label{sec:experiments}

We evaluate the performance of \Catra{} on~\NrBenchmarks{} instances of Parikh
automata intersection problems generated by the \OstrichPlus{} string constraint
solver when solving the PyEx benchmarks~\cite{pyex}. After generating an
initial~\InitialNrBenchmarks{},we remove~\NrTrivial{} instances solved in under
five seconds by baseline. We also attempted to benchmark instances generated by \Ostrich{} solving the kaluza-len (\numprint{38227} instances) and pyex-len (\numprint{791} instances) benchmark suites, but had to discard them since they were all trivial. For instance, on a laptop, \Calculus{} could solve every instance from kaluza-len in under five seconds, mean about 0.1s. The benchmarks are run on commit~\texttt{\commit}.

The benchmarks are executed in parallel using GNU Parallel~\cite{parallel},
since they are mostly single-threaded and initial results showed negligible
interference on performance, on \BenchmarkRig{}. We compiled the code using
Scala~\ScalaVersion{}, and executed the experiments on~\JvmVersion{} with a
maximum heap of~\MaxHeapSize{}. We used \Nuxmv{} version~\NuxmvVersion{} invoked
as a subprocess for each instance. Instances were executed in batches of
\BatchSize{}, each given a fresh JVM. We believe this represents a realistic use
case where \Calculus{} is used to support e.g a string solver. Experiments were
executed in random order for all backends. Each instance got a time budget
of~\RuntimeTimeout.

All runtimes are measured in wall-clock time as observed by the JVM when
executing the instance, and exclude time spent parsing (usually far below
\numprint{0.1}s).

\subsection{Execution Time and Ability to Solve Instances}\label{sec:runtime}

In \cref{fig:solve-division} we show how many of the~\NrBenchmarks{} instances left after excluding the trivial instances, among them the \numprint{38227} and \numprint{791} instances generated from kaluza-len and pyex-len respectively,
the respective back-ends could solve and with which status. A full summary of
their outcomes is also available in \cref{tab:solve-status}. Note that we lack
ground truth for all instances. We see that \Calculus{} generally outperforms
\Nuxmv{} on determining unsatisfiability, while being worse at satisfiable
instances. Both \Nuxmv{} and \Calculus{} strictly outperform baseline on every
kind of instance, but most of all on satisfiable instances, and \Calculus{}
outperforms \Nuxmv{}. \Nuxmv{} and \Calculus{} disagree on 38 instances, where
\Nuxmv{} returns \Sat{} and \Calculus{} returns \Unsat{}. We believe this to be
caused by a non-deterministically triggered bug in the clause learning of
\Calculus{}, but have not had time to further investigate it. We have however
validated all \numprint{3908} satisfying assignments from \Calculus{} with
\Nuxmv{}. \Cref{tab:solve-status} also shows a small number of crashing bugs in
our implementation.

Baseline performs worse on satisfiable instances because it executes a heuristic
meant to detect unsatisfiability early at a heavy penalty to satisfiable
instances. The heuristic is enabled since it is unrealistic to assume that we
know in advance whether an instance is satisfiable or not, but we do know that
unsatisfiable instances are more common.

\begin{table}
  \centering
  \input{graphs/solved_pivot_table.tex}
  \caption{The result of running the respective back-ends with a timeout of
  \RuntimeTimeout. Instances solved by no backend within the timeout are omitted
  from the table. }\label{tab:solve-status}
\end{table}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-by-solver.pdf}
    \caption{The division of statuses per backend.}
    \label{fig:solve-division}
    \Description[A bar chart showing three bars, one per backend, illustrating how many instances they could solve]{The bars are divided by satisfiable and unsatisfiable instances. Baseline could seemingly only solve unsatisfiable instances, lazy could solve a few satisfiable and mostly unsatisfiable, and nuxmv could solve about twice as many unsatisfiable as satisfiable instances, and slightly less in total than lazy.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-time-boxplot.pdf}
    \caption{The distribution of runtimes for solved instances per backend. Note that the number of instances solved differs between backends.}
    \label{fig:runtime-boxplot}
    \Description[A box plot showing the distribution of runtime over the three backends]{The middle box plot shows a tiny box centered around 0 seconds for the lazy backend, the rightmost box shows a bigger box between five seconds and 30 seconds for nuxmv, and the leftmost box shows a smaller box between 20 and 30 seconds for baseline. The whiskers of nuxmv span between 0 and the timeout, 60 seconds, while they are much tighter for lazy. On the other hand, lazy has  a lot of outliers.}
  \end{subfigure}
  \vfill
  \begin{subfigure}[b]{0.75\textwidth}
    \includegraphics[width=\textwidth]{graphs/\commit-cactus.pdf}
    \caption{The number of instances solved as the time budget increases, simulated from one \RuntimeTimeout-timeout run.}
    \label{fig:cactus}
    \Description[A cactus plot comparing the performance of nuxmv, lazy and baseline]{The line for baseline is strictly lower than the two others, and does not head upwards from zero until after 10 seconds of timeout, while the number of instances for nuxmv increases sharply until ca 10 seconds and then linearly after that. Lazy solves much more instances than nuxmv.}
  \end{subfigure}
\end{figure}

\subsubsection{Scalability}\label{sec:scaling}

A cactus plot showing the number of instances solved within a given timeout for
each backend can be seen in \cref{fig:cactus}. Here, we see clearly that
\Calculus{} outperforms \Nuxmv{} in general, but that \Nuxmv{} might have better
long-term scaling properties.

\iffalse
\subsection{Finding a Presburger Formula}\label{sec:evaluation:finding-image}

For baseline and \Calculus{}, \Catra{} offers the ability to find the equivalent Presburger formula representing a given instance. For baseline, we use the built-in quantifier elimination facilities of the underlying \Princess{} theorem prover, while for \Catra{} we use the specially tailored approach described in \cref{sec:finding-the-image}. For this experiment, we use only the~\NrKnownSat{} instances known to be satisfiable from the previous experiment detailed in \cref{sec:scaling,sec:runtime}. 

To make sure baseline puts up as much competition as possible, we disable
checking intermittent satisfiability and configure \Catra{} to run in the
maximally eager mode where the product is first computed before any
satisifiability check is performed. We run the experiments with a timeout
of~\ImageTimeout{}. The results of the experiment is summarised in
\cref{fig:cactus:image} and \cref{tab:image-results}. \Fudge{We see here that
something happens}.

\begin{figure}[ht]
  \caption{The number of instances \Catra{} was able to find the Presburger form of the image for within a given number of seconds per backend.}
  \label{fig:cactus:image}
\end{figure}
\fi

\subsection{Threats to Validity}

The most obvious threats to validity would be poor benchmarking or poor
implementation, e.g. if the method described in \cref{sec:calculus,sec:multiple}
deviate from what is actually benchmarked in section \cref{sec:experiments}, or
if the methods used for benchmarking would be unsound.  To increase the
probability that our results are representative both in the sense of
representing  on expected inputs and in the statistical sense for a given run of
\Catra{} despite the use of randomness in our implementation we execute many
experiments. Moreover, to address the issue of correctness we have validated all
reported solutions made by \Calculus{} with \Nuxmv{} to ensure that \Calculus{}
is indeed sound.

Another threat to validity would be if the competition (baseline and \Nuxmv)
would be disadvantaged in our comparison. For \Nuxmv{} we use the default
configuration which we believe should be performant (or it should not be the
default). Additionally, tweaking our invocation of \Nuxmv{} is explicitly made
easy for artefact reviewers. For baseline, our best argument is that the
solution we use is close to the one found in the \Ostrich{} string solver and
that it therefore should be realistic.

The greatest threat to validity is our choice of implementation platform and
automata library. There are some signs that product computation is inefficient,
notably the good performance of low-threshold automata materialisation that
prioritises computing smaller products. This means that \Calculus{} makes less
heavy use of product computation than baseline does, instead relying more on
\Princess{}. This situation would unfairly advantage \Calculus{} if our
automata library was the bottleneck due to our automata library being
unnaturally poor. We believe this is unlikely since similar performance issues
have been reported for string solvers. Additionally, profiling suggests that
both \Princess{}-based backends spend most of their time in \Princess{},
suggesting that the automaton implementation is not the bottleneck.

\section{Conclusion}

In this paper we have introduced a calculus to compute commutating operations on
intersections of regular languages that we call \Calculus{}. We have evaluated
it on \NrBenchmarks{} Parikh automata intersection problems generated by the
\OstrichPlus{} string solver \cite{ostrich-plus} solving the PyEx benchmark
suite \cite{pyex} using our Parikh automata solver \Catra{}.

Within \Catra{}, \Calculus{} shows astonishing performance in terms of memory
usage and solve-time compared to the baseline approach laid out in
\cite{generate-parikh-image} when implemented on the same underlying automated
theorem prover (\Princess{}, \cite{princess}). It is also competitive with the
\Nuxmv{} model checker \cite{nuxmv}, outperforming it on unsatisfiable instances
and generally outperforming it for timeouts under 30 seconds with its advantage
increasing drastically for even shorter timeouts. 30 seconds would generally be
considered a long timeout for our intended use as supporting infrastructure to a
string constraint solver.

Future investigations involve two tracks. The first one is integration into
existing string solvers (wich \Ostrich{} being a particularly promising
candidate due to its shared use of \Princess{}), and further adaptation to that
use case. Closer inspection of the instances where we currently time out should
be useful to further improve our heuristics.

The second track for future improvements is the extension into other problem
domains, including other logics, model checking problems, as well as to more
powerful automata such as transducers. In principle, we are also already able to
express stronger constraints than Parikh automata, due to our use of a full
automated theorem prover which allows adding arbitrary constraints in addition
to the expected Presburger formulae.

%% Acknowledgments
% \begin{acks}                            %% acks environment is optional
%                                         %% contents suppressed with 'anonymous'
%   %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%   %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%   %% acknowledge financial support and will be used by metadata
%   %% extraction tools.
%   This material is based upon work supported by the
%   \grantsponsor{GS100000001}{National Science
%     Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%   No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%   No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%   conclusions or recommendations expressed in this material are those
%   of the author and do not necessarily reflect the views of the
%   National Science Foundation.
% \end{acks}


%% Bibliography
\bibliography{bibliography}


%% Appendix
\appendix
\section{Appendix}
\input{appendix.tex}
\end{document}
