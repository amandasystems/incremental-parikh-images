%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=true}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[acmsmall]{acmart}\settopmatter{}


%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{POPL} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2023}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format must update the '\documentclass' and
%% topmatter commands above; see 'acmart-sigplanproc-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption



\usepackage{amsthm}
\usepackage{amsmath,mathtools,empheq,fancybox}
\usepackage{paralist}
\usepackage{url}
\usepackage{color}
\usepackage{textcomp,listings}
\usepackage{array}
\usepackage{mymacros}
\usepackage{microtype}
\usepackage{listings}
\usepackage{csquotes}
\usepackage{proof}
\usepackage{algorithm2e}      

\usepackage{mathpartir}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\lstset{
    columns=fullflexible,
    showspaces=false,
    showtabs=false,
    breaklines=true,
    showstringspaces=false,
    breakatwhitespace=true,
    escapeinside={(*@}{@*)},
    commentstyle=\color{greencomments},
    keywordstyle=\color{bluekeywords},
    stringstyle=\color{redstrings},
    numberstyle=\color{graynumbers},
    basicstyle=\ttfamily\small,
    framesep=12pt,
    xleftmargin=12pt,
    tabsize=4,
    captionpos=b
}


\newif\ifcomments
\commentstrue
\newif\ifoutline
\outlinetrue

\newcommand{\contents}[1]{\ifoutline{\color{blue}
    \begin{itemize}
    #1
    \end{itemize}
  }\fi}

\allowdisplaybreaks[1]


\begin{document}

%% Title information
\title{An Efficient Calculus for Generalised Parikh Images of Regular Languages}
                                        %% when present, will be used in
                                        %% header instead of Full Title.
\titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'

%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}                   %% \country is recommended
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
  The image of the Parikh map is important in automata theory, offering a
  compact characterisation of an automaton. We contribute a novel understanding
  of how Parikh maps can be combined with arbitrary commutative monoid morphisms
  to efficiently represent real-world constraints found in String solvers.
  Furthermore, we show how this formulation can be efficiently implemented as a
  calculus that we call \Calculus{} in a theorem prover supporting Presburger
  logic. In particular, our calculus allows us to solve the common problem in
  string solvers of computing the Parikh image of products of arbitrarily many
  automata. We demonstrate this by implementing a tool called \Catra{}, which we
  evaluate on constraints produced by \Fudge{the PyEx benchmarks} when solved by
  the \Ostrich{} string constraint solver. We show that \Calculus{} strictly
  outperforms the standard eager approach described
  in~\citeauthor{generate-parikh-image} as well as the improved
  over-approximating method recently described
  by~\citeauthor{approximate-parikh}. \Calculus{} typically also outperforms
  the~\Nuxmv{} model checker, in particularly on unsatisfiable instances.
  Finally, \Fudge{\Calculus{} and therefore \Catra{} provides an efficient
  method for deriving the Presburger formula of a given Parikh image that
  outperforms any previously published method.}
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{parikh images, string solvers, model checking}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}

The Parikh map is a characterisation of a context-free language as a vector of
its character counts, famously a semilinear set~\cite{parikh-theorem}. It
appears naturally as part of many operations in model checking and the solving
of string constraints in automata-based string solvers such as \Ostrich{},
notably in representing constraints on string lengths.

It is possible to compute the Parikh image of any context-free language using
the method described in~\cite{generate-parikh-image}. Later improvements have
produced a construction taking at most linear time to
produce~\cite{muscholl-linear}. However, the resulting existentially quantified
clauses are costly to eliminate, in practice making many real-world problems
intractable.

In addition to the costly elimination of the existentially quantified variables,
another problem arises for the common case of solving length constraints in
string solvers. Conjunctions of string constraints lead to the computation of
Parikh images of products of regular languages. Applying the approach
in~\cite{generate-parikh-image} would require the computation of the product
before its Parikh image. In several instances we have observed while solving
real-world string constraints, the computation the product of automata exhausts
the memory of any machine due to the exponential blow-up in size of the product,
quickly becoming intractable as the number of terms in the product increases.

Addressing these concerns, we have developed a calculus for Parikh images of
products of regular languages that we call \Calculus{}. It allows us to
interleave the computation of arbitrarily deep products of automata with the
product's Parikh image. This enables us to let both calculations inform each
other, eliminating unnecessary work, and pruning the size of the partial
products considered in the computation for a smaller memory footprint. Moreover,
the method can be used iteratively to tackle smaller chunks of the product
incrementally, thereby avoiding running out of memory. We implement this
calculus, along with the approximate method of~\cite{approximate-parikh}, its
fall-back variant adapted from~\cite{generate-parikh-image}, and an adapter for
the \Nuxmv{} model checker, in a tool that we call \Catra{}. \Catra{} allows
solving constraints over groups of synchronised (products of) Parikh automata,
that is symbolic automata with transitions that increment and decrement integer
registers and that allows checking the constraints on these registers after
executing automata.

Additionally, \Catra{} is able to efficiently generate a quantifier-free
Presburger formula representing such an instance of constraints and automata,
something beyond the reach of general model checkers like~\Nuxmv.

In summary, we contribute:
\begin{itemize}
\item The \Calculus{} calculus to efficiently compute the Parikh image of products of Parikh automata.
\item A specialised method for quantifier elimination that blends well with \Calculus{} to efficiently compute quantifier-free Presburger representations of a given Parikh image.
\item Experiments illustrating the performance of \Calculus{} on real-world examples from string solving, including \NrBenchmarks{} instances in a standardised format made available for future studies.
\item The \Catra{} tool for solving such instances, containing an implementation of \Calculus{}, the over-approximation described in~\cite{approximate-parikh}, and an adapter for the~\Nuxmv{} model checker~\cite{nuxmv}.
\item Suggestions for how to efficiently implement \Calculus{} in a modern automated theorem prover, including strategies for case splitting, clause learning, and constraint propagation for connectedness.
\item Finally, a published description of the well-known bug in~\cite{generate-parikh-image} previously only described in folklore.
\end{itemize}

\subsection{A demonstration of our approach}

Consider the two automata $\SomethingCSomething{}$
(Figure~\ref{fig:something-c-something}) and $\AcaOrBc{}$
(Figure~\ref{fig:aca-or-bc}). Assume we want to find a value in the Parikh image
of their product where $\TransitionCount{a} > \TransitionCount{b}$, that is find
a count of characters in a string accepted by both automata where there are more
a:s than b:s.

The na\"ive approach would be to first compute the product of the automata and
then compute the Parikh image of its product. However, we can take advantage of
the constraint to prune the automata before computing the product and help
reduce the blow-up. To give an intuition for the core idea of \Calculus{}, we
will annotate the transitions of $\SomethingCSomething$ and $\AcaOrBc$ with
variables illustrating how many times they are taken, and perform informal
reasoning on them corresponding to the reasoning performed by our calculus.

\begin{figure}[ht]
  \begin{minipage}[b]{0.75\linewidth}
  \centering 
    \includegraphics[width=\textwidth]{aca_or_bc}
    \caption{An automaton recognising the regular expression
    $\mathtt{ac^*a|b(bb)^*c}$.}\label{fig:aca-or-bc}

    \includegraphics[width=\textwidth]{something_c_something}
    \caption{An automaton recognising the regular expression
    $\mathtt{.^*c.^*}$.}\label{fig:something-c-something}
  \end{minipage}
  \end{figure}


Note that we have already performed some reasoning in the starting figures -- we
use the same symbol for the number of times each of the upper transitions of
$\AcaOrBc$ are taken, since we know that they must be taken the same number of
times, and we introduced the constant $1$ for the middle transition in
$\SomethingCSomething$, since it must always be taken precisely once to reach
the accepting state. We set the initial flow into the starting state to 1 and
use linear equations to enforce balanced flow throughout the automaton to
determine the relations between the counting variables on each transition (e.g.
$1 + {l_b}' = l_a + l_b$, or $l_a + l_c' = 1$). Using standard linear equation
reasoning loses additional variables and gives us the annotations in
Figure~\ref{fig:example-simplify-1}. 


\begin{figure}[h]
  \begin{minipage}[b]{0.75\linewidth}
  \centering 
    \includegraphics[width=\textwidth]{aca_or_bc_simplified}
    \caption{$\AcaOrBc{}$ with its symbolic transition counters after
    simplification of their equations.}\label{fig:example-simplify-1}
  \end{minipage}
  \end{figure}

  We now need to relate the number of times a transition was taken to the
  character counts we are really interested in. Note that this also puts the two
  automata into contact with each other, before any product has been computed.
  Some consideration leads us to the definition that the number of occurrence of
  a each character corresponds to the sum of usages of transitions containing
  it, e.g. relations of the kind $\TransitionCount{a} = 2 - 2l_c$, and
  $\TransitionCount{b} = 2 l_b' + l_c'$.\footnote{For the attentive reader: in this
  example, we leave the catch-all transitions of $\SomethingCSomething$
  unconstrained since they will be taken care of by the product and would
  complicate the reasoning since we cannot yet know which character they will
  correspond to in the final product. In other words, this is a safe
  under-approximation that only limits our ability to reason before
  computing the product.}

This relation can be combined with the constraint that $\TransitionCount{a} >
\TransitionCount{b}$ to obtain $2 - 2l_c > 2 l_b' + l_c'$. Combining this
equation with the requirement from $\SomethingCSomething$ that
$\TransitionCount{c} \geq 1$ and $\TransitionCount{c} = l_c + l_c'$ from
$\AcaOrBc$ gives $0 \geq l_b'$, and since we cannot use a transition a negative
number of times $l_b' = 0$, which gives the automaton in
Figure~\ref{fig:example-simplify-2}.

\begin{figure}[h]
  \begin{minipage}[b]{0.75\linewidth}
  \centering 
    \includegraphics[width=\textwidth]{aca_or_bc_simplified_2}
    \caption{$\AcaOrBc{}$ after removing the now unused b-transition.}\label{fig:example-simplify-2}
  \end{minipage}
  \end{figure}

  Plugging in the same inequality again on the same principle, we get $2(1 -
  l_c') > l_c' \equiv 1 > l_c'$ since we are working with integers, and
  therefore that $l_c' = 0$. This gives us the even smaller automaton of
  Figure~\ref{fig:example-simplify-3}.

  \begin{figure}[h]
    \begin{minipage}[b]{0.75\linewidth}
    \centering 
      \includegraphics[width=\textwidth]{aca_or_bc_simplified_3}
      \caption{$\AcaOrBc{}$ after further constraint propagation.}\label{fig:example-simplify-3}
    \end{minipage}
    \end{figure}

This will produce a straightforward product since both automata now have the
same shape, where we essentially only need to expand the self-loops of
$\SomethingCSomething$ and reduce its range labels to only capture a:s. Note
that we got this result by only performing linear inequality reasoning on the
automata of the product versus the number of times each transition would be
used.

\section{Preliminaries}

\subsection{Finite-state Automata and their Products}
We define a non-deterministic automaton~$\Automaton$ with alphabet~$\Alphabet$
as $\Automaton = \AutomatonTuple$ where $\Transitions = \States \times \Alphabet
\times \States$, $\States$ is its states, $\InitialState$ is w.l.o.g. assumed to
be the single initial state, and $\AcceptingStates$ is its set of accepting
states.  We describe a transition $\Transition$ from state $\State$ to state
$\State'$ with label $\Label \in \Alphabet$ as $\Transition =
\FromLabelTo{\State}{\Label}{\State'}$. When one or more of the states and
labels of a transition are uninteresting we will omit it.

We will let variables $\Transition, \Transition', \Transition_1, \ldots, \Transition_n$ etc describe transitions, $\State, \ldots, \State_n$ states, and $\Automaton, \ldots, \Automaton_n$ automata, and use subscript indexing ($\Transitions_\Automaton$) to refer to the transitions, states, etc of a given automaton.

By a \emph{product} of two automata $\Automaton_1, \Automaton_2$, written
$\Automaton_1 \times \Automaton_2$, we mean an automaton constructed to run
$\Automaton_1$ and $\Automaton_2$ in parallel on an input and only accept the
input if both automata would do so.

We refer to the resulting product states as tuples, $\Tuple{\State, \State'}$,
which represent the state of the product automaton where $\Automaton_1$ would be
in $\State$ and $\Automaton_2$ would be in $\State'$. Note that since we use
ordered tuples the product is technically (but w.l.o.g) not commutative; the
left-hand-side must come from the left-hand term. The sole purpose of this
matching is to allow us to speak with precision about the origins of components
in a product.

\subsection{The Parikh Map and its Image}
Formally, the \textit{Parikh map} over a context-free language $\Strings=
\left\{a_1, \ldots, a_k \right\}$ is defined as in \cite{kozen}:

$$
\begin{aligned}
& \ParikhMap: \MapFromTo{\Strings}{\natural^k} \\
& \ParikhMap(s) = \VectorLiteral{\#a_1(s), \#a_2(s), \ldots, \#a_k(s)}
\end{aligned}
$$

That is, $\ParikhMap(s)$ is a vector of the number of occurrences of each
character in the language for a given string $s$. For example, for  $\Strings =
\Set{a, b}$, we would have $\ParikhMap(abb) = \VectorLiteral{1, 2}$.

We define the image of this map, the \textit{Parikh image}, of some subset of
the language $\Language \subseteq \Strings$ as:

\[
\ParikhMap(\Language) = \Set{\ParikhMap(x) \SuchThat x \in \Language}
\]

Thus, we would have $\ParikhMap(\left\{ab, abb\right\}) = \left\{\left[1,
1\right], \left[1, 2\right]\right\}$.

Parikh's theorem states that ignoring order, any context-free grammar has a
letter-equivalent regular language (c.f.~\cite{construction} for a construction of
such automata from context-free grammars and~\cite{bounds} for bounds on its size). However, there are languages that are
not context-free that also have semilinear images under~$\ParikhMap$ (e.g.
$\ParikhMap(\Set{a^nb^nc^n \SuchThat n \geq 0}) = \ParikhMap((abc)^*) = \#a =
\#b = \#c \land \#a \geq 0$).

\subsection{The Parikh image of a regular language expressed in Presburger arithmetic}

Since Parikh images are semilinear, they can be fully expressed by Presburger
arithmetic. This means that any Parikh image can be written as a set of linear
equation. Following~\cite{generate-parikh-image}, we define the Parikh Image of
a regular language recognised by a DFA $\Automaton = \AutomatonTuple$ as:

\begin{equation}
\begin{aligned}
\ParikhMap(\Automaton) := 
& \AndComp{\Letter \in \Alphabet}{\LetterVar_{\Letter} = \sum_{\Transition = \FromLabelTo{}{\Letter}{} \in \Transitions} \TransitionVar_{\Transition}
}\\
& \AndComp{\State \in \AcceptingStates}{\FinalStateVar_{\State} > 0 \implies \StateVar_{\State} > 0}\\
& \AndComp{\State \in \States}{
  \left(\FinalStateVar_{\State} \text{ if } \State \in \AcceptingStates \text{ otherwise } 0 \right) +
  \sum_{\Transition = \FromLabelTo{\State}{}{}} - \sum_{\Transition = \FromLabelTo{}{}{\State}}
= \begin{cases}
    1 \text{  if $\State = \InitialState$} \\
    0 \text{ otherwise}
  \end{cases}
}\\
& \AndComp{\Transition = \FromLabelTo{}{}{\State} \in \Transitions}{
  \TransitionVar_{\Transition} > 0 \implies \StateVar_{\State}
} \\
& \AndComp{\State \in \States}{
  \StateVar_{\State} > 0 \implies
  \left(\FinalStateVar_{\State} = 1 \text{ if } \State \in \AcceptingStates\right) \OrComp{\Transition = \FromLabelTo{\State}{}{\State'} \in \Transitions}{
    \StateVar_{\State} = \StateVar_{\State'} + 1 \land 
    \TransitionVar_{\Transition} \geq 1 \land
  \StateVar_{\State'} \geq 1
    }
}
\end{aligned}
\label{eq:generate-parikh}
\end{equation}

where all variables $\TransitionVar_i, \StateVar_i, \FinalStateVar_i$ are
existentially quantified and the free variables $\LetterVar_\Letter$ make up the
actual image. Performing standard quantifier elimination on $\TransitionVar_i,
\StateVar_i, \FinalStateVar_i$ with $\LetterVar_\Letter$ would produce the
Parikh image in its Presburger form. Note that compared to the method described
in~\cite{generate-parikh-image}, ours is backwards. Intuitively the approaches
are equivalent since the Parikh image, by definition, disregards order. The
approach can be described as either following paths through the automaton
forwards from an (in our case, \emph{the}) initial state, or conversely as in
the case here, choose an accepting state and trace a path backwards to the
initial state. 

\contents{
  \item mention the bug here!
}

In this paper we refer to this model as the baseline approach, though we also
apply some optimisations that allow us to detect unsatisfiability early,
described in Section~\ref{sec:implementing-baseline}. Notably, the calculus
introduced in this paper can at its core be described as lazily enforcing the
connectedness constraint of this method (the final two clauses) while also
interleaving the (similarly lazy) computation of products of automata.

\subsubsection{An Example}
We apply Equation~\ref{eq:generate-parikh} to the automaton of Figure~\ref{fig:len-branch}. We then get the quantified Presburger formula seen in Equation~\ref{eq:parikh-formula}. As before, we omit the implicit existential quantifiers for brevity, along with the constranints requiring all variables to be nonnegative. We also omit the $\land$-clauses between each line. In other words, in this example, only the variables $\LetterVar_a, \LetterVar_b, \LetterVar_c, \LetterVar_d, \LetterVar_e$, and $\LetterVar_f$ are free, and corresponds to the counts of the respective subscript letter. Note that since they here uniquely identify a transition due to the one-to-one mapping between character labels and transitions in the example, the first clause can be trivially eliminated by substituting $\LetterVar_a$ for $\TransitionVar_{\FromLabelTo{0}{a}{1}}$, etc. This is not generally the case.

\begin{equation}
  \begin{aligned}
  &\LetterVar_{a} = \TransitionVar_{\FromLabelTo{0}{a}{1}} \land \LetterVar_{b} = \TransitionVar_{\FromLabelTo{0}{b}{2}} \land \LetterVar_{c} = \TransitionVar_{\FromLabelTo{1}{c}{1}} \land \LetterVar_{d} = \TransitionVar_{\FromLabelTo{2}{d}{3}} \land \LetterVar_{e} = \TransitionVar_{\FromLabelTo{2}{e}{2}} \land \LetterVar_{f} = \TransitionVar_{\FromLabelTo{1}{f}{3}}\\
  &\FinalStateVar_{3} > 0 \implies \StateVar_{3} > 0\\
  & \TransitionVar_{\FromLabelTo{0}{b}{2}} + \TransitionVar_{\FromLabelTo{0}{a}{1}} = 1 \land  \TransitionVar_{\FromLabelTo{1}{f}{3}} - \TransitionVar_{\FromLabelTo{0}{a}{1}} = 0\\
  & \TransitionVar_{\FromLabelTo{2}{d}{3}} - \TransitionVar_{\FromLabelTo{0}{b}{2}} = 0 \land \FinalStateVar_{3} + 0 - \TransitionVar_{\FromLabelTo{2}{d}{3}} - \TransitionVar_{\FromLabelTo{1}{f}{3}} = 0\\
  &\TransitionVar_{\FromLabelTo{0}{a}{1}} > 0 \implies \StateVar_{1} > 0 \land \TransitionVar_{\FromLabelTo{0}{b}{2}} > 0 \implies \StateVar_{2} > 0\\
  &\TransitionVar_{\FromLabelTo{1}{c}{1}} > 0 \implies \StateVar_{1} > 0 \land \TransitionVar_{\FromLabelTo{2}{e}{2}} > 0 \implies \StateVar_{2} > 0\\
  &\TransitionVar_{\FromLabelTo{1}{f}{3}} > 0 \implies \StateVar_{3} > 0 \land \TransitionVar_{\FromLabelTo{2}{d}{3}} > 0 \implies \StateVar_{3} > 0\\
  &\StateVar_{0} > 0 \implies \left(\StateVar_{0} = \StateVar_{1} + 1 \land \TransitionVar_{\FromLabelTo{0}{a}{1}} \geq 1 \land \StateVar_{1} \geq 1\right) \lor \left(\StateVar_{0} = \StateVar_{2} + 1 \land \TransitionVar_{\FromLabelTo{0}{b}{2}} \geq 1 \land \StateVar_{2} \geq 1\right)\\
  &\StateVar_{1} > 0 \implies \left(\StateVar_{1} = \StateVar_{1} + 1 \land \TransitionVar_{\FromLabelTo{1}{c}{1}} \geq 1 \land \StateVar_{1} \geq 1\right) \lor \left(\StateVar_{1} = \StateVar_{3} + 1 \land \TransitionVar_{\FromLabelTo{1}{f}{3}} \geq 1 \land \StateVar_{3} \geq 1\right)\\
  &\StateVar_{2} > 0 \implies \left(\StateVar_{2} = \StateVar_{2} + 1 \land \TransitionVar_{\FromLabelTo{2}{e}{2}} \geq 1 \land \StateVar_{2} \geq 1\right) \lor \left(\StateVar_{2} = \StateVar_{3} + 1 \land \TransitionVar_{\FromLabelTo{2}{d}{3}} \geq 1 \land \StateVar_{3} \geq 1\right)\\
  &\StateVar_{3} > 0 \implies \FinalStateVar_{3} = 1
  \end{aligned}
  \label{eq:parikh-formula}
  \end{equation}

  We see here that the formula encodes the choice of path at the start of the
  automaton ($\TransitionVar_{\FromLabelTo{0}{b}{2}} +
  \TransitionVar_{\FromLabelTo{0}{a}{1}} = 1$), and the dependency between that
  choice and the following outgoing transition, e.g. in
  $\TransitionVar_{\FromLabelTo{2}{d}{3}} -
  \TransitionVar_{\FromLabelTo{0}{b}{2}} = 0$. Here we only have one choice of
  incoming flow; from $\FinalStateVar_3$, corresponding to the only accepting
  state. Simplifying the equation using standard algebra gives us this smaller
  set of clauses:

  \begin{equation}
    \begin{aligned}
    &\FinalStateVar_{3} > 0 \implies \StateVar_{3} > 0\\
    & \LetterVar_{d} + \LetterVar_{a} = 1\\
    & \FinalStateVar_{3}  > \LetterVar_{d} \implies \StateVar_{1} > 0 \land \LetterVar_{d} > 0 \implies \StateVar_{2} > 0\\
    &\LetterVar_{c} > 0 \implies \StateVar_{1} > 0 \land \LetterVar_{e} > 0 \implies \StateVar_{2} > 0\\
    &\FinalStateVar_{3}  > \LetterVar_{d} \implies \StateVar_{3} > 0 \land \LetterVar_{d} > 0 \implies \StateVar_{3} > 0\\
    &\StateVar_{0} > 0 \implies \left(\StateVar_{0} = \StateVar_{1} + 1 \land (\FinalStateVar_{3}  - \LetterVar_{d}) \geq 1 \land \StateVar_{1} \geq 1\right) \lor \left(\StateVar_{0} = \StateVar_{2} + 1 \land \LetterVar_{d} \geq 1 \land \StateVar_{2} \geq 1\right)\\
    &\StateVar_{1} > 0 \implies \left(\StateVar_{1} = \StateVar_{3} + 1 \land (\FinalStateVar_{3}  - \LetterVar_{d}) \geq 1 \land \StateVar_{3} \geq 1\right)\\
    &\StateVar_{2} > 0 \implies \left(\StateVar_{2} = \StateVar_{3} + 1 \land \LetterVar_{d} \geq 1 \land \StateVar_{3} \geq 1\right)\\
    &\StateVar_{3} > 0 \implies \FinalStateVar_{3} = 1
    \end{aligned}
    \label{eq:parikh-formula:reduced}
    \end{equation}
  
Note that some of the or-clauses disappear since they contain false
inequalities of the type $x = x + 1$, arising from cycles. These clauses encode
the distance in a spanning tree from an accepting state, and the
unsatisfiability is by design, essentially ensuring that two states in a cycle
cannot vouch for each other's connectedness.

Assume we are really interested in occurrences with the letter e, i.e. we solve
for $\LetterVar_e > 0$. This implies that $\StateVar_{2} > 0$ (essentially, that
the state our transition of interest started in is reachable), in turn implying
that $\StateVar_{2} = \StateVar_{3} + 1 \land \LetterVar_{d} \geq 1 \land
\StateVar_{3} \geq 1$. Armed with this knowledge, we now know that $\StateVar_3
> 0$, which means that $\FinalStateVar_3 = 1$. Since we also know that
$\LetterVar_d \geq 1$, we know that $\LetterVar_a = 0$. This corresponds to
choosing the bottom path through the automaton. Applying this to the equation
and performing standard boolean logic propagation gives the following:

\begin{equation}
  \begin{aligned}
  &\LetterVar_{d} = 1 \land \LetterVar_{a} = 0 \\
  & \StateVar_{3} > -1 \\
  &\LetterVar_{c} = 0\\
  &\StateVar_{0} > 0 \implies \StateVar_{0} = \StateVar_{3} + 2\\
  \end{aligned}
  \label{eq:parikh-formula:choose-a}
  \end{equation}

  Since the final implication holds (we know $\StateVar_3 \geq 0$, and therefore
  $\StateVar_3 + 2 > 0$), we can disregard it. Thereby we have eliminated all
  quantified variables and are left with a formula containing only the free
  ones. Note that all constraints on $\LetterVar_e$ have disappeared after we
  asserted that it was strictly positive. This is no coincidence: since it is a
  loop, it can take any positive value.

  This example, in addition to showing how the encoding works, has hopefully
  also illustrated one of the key insights behind the design of \Calculus: the
  fact that even few additional constraints on the Parikh image can dramatically
  reduce the difficulty of computing it, as in this extreme case where only one
  additional choice in practice fixes the values of all other symbols in the image.

\section{Overview of our generalised Parikh images}

\subsection{Generalised Parikh Images}\label{sec:generalised}

To generalise the logic over Parikh images, we introdouce a morphism $\Map:\: \Sigma
\to \Monoid$ where $\Monoid = \left(X;+_{\Monoid};0_{\Monoid}\right)$ is a commutative monoid where the following holds:
\begin{itemize}
  \item $\Map(\epsilon) = 0_{\Monoid}$
  \item $\Map(a \Concat b) = \Map(a) +_{\Monoid} \Map(b)$, where $\Concat$ is
  the string concatenation operation
\end{itemize}

Many other generalisations of the Parikh map have been studied, most of them
less plain than this one. Prominent examples include generalising the Parikh map
to segments of a fixed length \cite{KARHUMAKI1980155} and the more general
Parikh matrix, which gives more information about a word than the standard
Parikh image \cite{parikh-matrix}. Another notable generalisation is the
p-vector, introduced in~\cite{infinite-words}, which denotes the position of
each letter in the word rather than the number of their occurrences and allows
for generalisations into infinite alphabets. All of these, in some sense, add
something which is not already there in the Parikh map. By contrast, the main
utility of the formulation introduced here is that it allows us to \emph{remove}
something, thereby obtaining an easier problem.

\subsection{Applications}

\subsubsection{Example~1: String length}

A useful example of such a simplifying morphism can express string length, the
problem that originally motivated our study of the Parikh map. We let $\Monoid =
\left(\mathbb{Z};+;0\right)$ and let $\Map(x) = 1$. Then the length of a string
$s = s_1 \RepeatSum{\Concat}  s_n$ is given by $\Map(s) = \sum \Map(s_i) = 1
\RepeatSum{+} 1$. Note that if we substitute vectors and vector addition for
this monoid, we obtain the regular Parikh map.

Intuitively, this produces a simpler problem than computing the Parikh image, as
we now ignore not just the order of characters but also their differences. This
means that the branching automaton of Figure~\ref{fig:len-branch} becomes
equivalent to the non-branching automaton of Figure~\ref{fig:len-branch-free}.
Note that the elimination of the choice of paths also effectively eliminates the
possibility of disconnecting one of the self-loops. This, in turn, means that we
can eliminate all of the connectedness implications of the quantified formula,
which would in turn simplify it (and the quantifier elimination of it)
considerably.

\begin{figure}
  \includegraphics[width=0.5\linewidth]{choice}
  \caption{An example automaton with a choice of two paths: either we start with an a or a b.}
  \label{fig:len-branch}
\end{figure}

\begin{figure}
  \includegraphics[width=0.5\linewidth]{no-choice}
  \caption{An equivalent automaton to the one in Figure~\ref{fig:len-branch}
  when treating any letter the same as would be the case with a length
  constraint.}\label{fig:len-branch-free}
\end{figure}


\contents{
  \item Formally analyse the complexity of how much easier the length problem is
}


\subsubsection{Example~2: Parikh Automata}

Another application for this generalisation is to express Parikh
automata~\cite{parikh-automata}, and this is indeed what the implementation
described in Section~\ref{sec:implementation} does.


\contents{
 \item what is parikh automata
 \item Give an actual example of a parikh automata
}


\section{A Calculus for Generalised Parikh Images}\label{sec:calculus}

We assume an NFA $\Automaton = \AutomatonTuple$ with $\NrTransitions$
transitions $\Transitions =
\Set{\EllipsisSequence{\Transitions}{\NrTransitions}}$. For convenience, we then introduce the following supporting notations:

\begin{definition}
  A \textit{path} $\Path = \PathEnumeration$ of an automaton~$\Automaton$ with
  transitions $\Transitions$ represents a path through $\Automaton$ using
  transitions in $\Transitions$ (i.e. $\Transitions(\State_k, \Label_{k+1},
  \State_{k+1})$ holds for every $k$), passing zero or more labels $\Label_1,
  \ldots, \Label_n$. The path must begin in the initial state, i.e.~$\State_0 =
  \InitialState$. However, the ending state, $\State_n$, is not necessarily an
  accepting state.
  \end{definition}

\begin{definition}
  Moreover, we also talk about the \textit{set of paths} of an automaton,
  $\Paths(\Automaton)$, a possibly infinite (if $\Automaton$ has loops) set of
  valid paths through $\Automaton$. Additionally, we use the
  notation~$\Paths(\Automaton, \State)$ to mean all paths ending in
  state~$\State$.
\end{definition}

\begin{definition}
  The \textit{word} of a path $\WordOf(\Path) = \Label_1 \RepeatSum{\Concat} \Label_n$ is
  the word read out on its labels.
\end{definition}

\begin{definition}
  The \textit{states} of a path $\StatesOf(\Path) = \Set{\State \SuchThat
  \FromLabelTo{\State}{}{\State'} \in \Path \text{ or }
  \FromLabelTo{\State'}{}{\State} \in \Path}$ are the states visited along a
  path. Note that $\InitialState \in \StatesOf(\Path)$ for every path since all
  paths start in the initial state.
\end{definition}

\begin{definition}
  A \textit{cut}, $C$ of an automaton~$\Automaton = \AutomatonTuple$,
  $\SeparatingCut(C, \Automaton, \State)$, is a minimal set of transitions whose
  removal from~$\Transitions$ would cause~$\State \in \States$ to be unreachable
  from the initial state~$\InitialState$. $\SeparatingCut(\Automaton,
  \InitialState)$ is crucially undefined since it cannot be made unreachable
  from itself by definition.
\end{definition}

\begin{definition}
 The \textit{transition count}, $\TransitionCount(\Transition, \Path)$ is the
 number of times a transition $\Transition =
 \FromLabelTo{\State_1}{\Label}{\State_2} \in \Transitions$ appears in a path
 $p$.
\end{definition}

We then introduce the two predicates into our calculus with the following
definitions:

\begin{definition}
  The Parikh predicate, $\SinglePredicateInstance$, for some automaton
  $\Automaton = \AutomatonTuple$, modulo some map $\Map$ to a commutative monoid
  $\Monoid$ as described in Section~\ref{sec:generalised} and with a transition
  selection function $\Filter:\: \Transitions \to \Naturals$ holds when
  $\MonoidElement \in \Monoid$ is the Parikh image of $\Automaton$ modulo
  $\Map$, or more formally when there is an accepting path $\Path =
  \PathEnumeration \in \Accepting{\Paths(\Automaton)}$ such that:
  \begin{itemize}
    \item $\Filter(\Transition) = \TransitionCount(\Transition, \Path)$
    \item $\MonoidElement = \Map(\WordOf(\Path))$
  \end{itemize}
\end{definition}

\begin{definition}
  $\Connected(\Automaton, \Filter)$ for some automaton $\Automaton =
  \AutomatonTuple$ holds when for every $\Transition =
  \FromLabelTo{\State}{}{\State'} \in \Transitions$, $\Filter(\Transition) > 0
  \implies \exists \Path \in \Paths(\Automaton)$ $\Filter(\Transition_i) > 0$
  for $\Transition_i \in p$, and $\State \in \StatesOf(p)$, or in words that
  there exists some $\Filter$-selected valid path that reaches $\Transition$'s
  starting state, $\State$. Intuitively, it represents the condition that
  $\Automaton$ is connected with respect to the selection function $\Filter$ for
  every transition. It is redundant to $\Image$ by design.
\end{definition}

Finally, we present the rules of our calculus for one automaton in Table~\ref{tbl:rules:single}. Note that we
operate on sets of symbols (terms, clauses). Additionally, we also use the
convention of splitting the formulas into linear inequalities ($\SomeInequalities$)
and any other clauses ($\SomeClause$).

We use the shorthand notation~$\Transitions_\Automaton$ to refer to the
transitions of an automaton~$\Automaton$. Additionally, for an
automaton~$\Automaton = \AutomatonTuple$ we allow mapping the selection function
like so: $\Filter(\Automaton) = \Tuple{\States, \InitialState, \AcceptingStates,
\Set{\Transition \in \Transitions \SuchThat \Filter(\Transition) > 0}}$, i.e.
$\Automaton$~with only the transitions for which~$\Filter$ is positive. In this
instance, the basis for the matched linear inequalities is
implicitly~$\SomeInequalities$.

These rules are meant to be executed in an automated theorem prover, and are
read bottom-up. Since a large part of our rules operate by adding and literally
matching linear (in)equalities in a proof goal, we use the shorthand of listing
the matched inequalities as antecedents. An example of this can be seen in
the~\Propagate{} rule.

Please note that the filtering function~$\Filter$ is evaluated symbolically in
these rules, and can in practice be read as a symbolic function from transitions
to terms (e.g.~\texttt{t} or~\texttt{t+1}), not dissimilar to~$\FlowEq$ below.
In our implementation, $\Filter$~is a vector of fresh unknown constants with the
same size as~$\Transitions$.

We assume that every rule only fires when it would add a new clause. For
example, this means that we cannot use~\Split{} to split on the same term twice.
This suggests a proof strategy where you \Propagate{} when you can, \Split{}
when you must, and \Subsume{} when neither is possible anymore.

\begin{table}[h]
\begin{tabular}{@{}l>{$}c<{$}p{3cm}@{}}\toprule
  Name & Rule & Prerequisites\\
  \midrule

  % EXPAND
  \Expand & 
    \inferrule
  {\Connected(\Automaton, \Filter) \land \FlowEq(\Automaton, \Filter) \land \MonoidElement = \sum_{\Transition \in \Transitions_\Automaton} \Filter(\Transition) \cdot \Map(\Transition), \SomeInequalities, \SomeClause}
  {\SinglePredicateInstance, \SomeInequalities, \SomeClause} & 
  None \\

  % SPLIT
  \Split & 
  \inferrule{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause, \Filter(\Transition) = 0 \mid \Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause, \Filter(\Transition) > 0}{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause} &
  if $\Transition \in \Transitions_\Automaton$ \\

  % PROPAGATE
  \Propagate &
  \inferrule{\Connected(\Automaton, \Filter), \Set{\Filter(\Transition') = 0 \SuchThat \Transition' \in C}, \SomeInequalities, \SomeClause, \Filter(\Transition) = 0}{\Connected(\Automaton, \Filter), \Set{\Filter(\Transition') = 0 \SuchThat \Transition' \in C}, \SomeInequalities, \SomeClause} &
  if $t = \FromLabelTo{\State}{}{\State'} \in \Transitions_\Automaton, \SeparatingCut(C, \Automaton, \State)$\\

  % SUBSUME
  \Subsume &
  \inferrule{\SomeInequalities,\SomeClause}{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause} &
  \Split{} and \Propagate{} cannot be applied \\
  \bottomrule
  \end{tabular}
  \caption{Derivation rules for one automaton.}\label{tbl:rules:single}
\end{table}

We use the symbolic function $\FlowEq(\Automaton, \Filter)$ that generates a set
of existentially quantified linear inequalities with the following definition:
$$
\begin{aligned}
  & \FlowEq(\Automaton, \Filter) = \AndComp{\State \in \States}{\In(\State, \Filter) - \Out(\State, \Filter)}
\text{ $\geq 0$ if $\State \in \AcceptingStates$, $= 0$ otherwise}\\
  & \In(\State, \Filter) = \left(\text{$1$ if $\State = \InitialState$, otherwise $0$}\right) + \sum_{\Transition = \FromLabelTo{\State_0}{}{\State} \in \Transitions} \Filter(\Transition)\\
  & \Out(\State, \Filter) = \sum_{\Transition = \FromLabelTo{\State}{}{\State'} \in \Transitions} \Filter(\Transition)
\end{aligned}
$$
Where we transparently assign fresh variables to each state $\State \in \States$
and existentially quantify them in the whole clause. 

Additionally, we assume the existence of a rule \PresburgerClose{},
corresponding to a sound and complete solver for Presburger formulae, modulo the
monoid~$\Monoid$.

The $\Propagate{}$ rule allows us to propagate (dis-)connectedness across
$\Automaton$. It states that we are only allowed to use transitions attached to
a reachable state, and is necessary to ensure connectedness in the presence of
cycles in~$\Automaton$.

\textsc{Expand} expands the predicate into its most basic rules; one set of
linear equations synchronising the transitions mentioned by~$\Filter$ to the
corresponding Monoid element~$\MonoidElement$, and the linear flow equations of
the standard Parikh image formulation, as described by~\FlowEq. Since
$\Connected$ and $\Image$ are partially redundant and the difference is covered
by~$\FlowEq$, we can remove the instance of~$\Image$ when applying~$\Expand$. In
this sense, we split the semantics of the $\Image$~predicate into its counting
aspect (covered by $\FlowEq$) and its connectedness aspect (covered by
$\Connected$).

Finally, $\Split{}$ allows us to branch the proof tree by trying to exclude a
contested transition from a potential solution before concluding that it must be
included. Intuitively, this is what guarantees our ability to make forward
progress by eliminating paths through~$\Automaton$.

A decision procedure for our predicate in a tableau-based automated theorem
prover would start by expanding the predicate using the $\Expand{}$~rule. A
modestly clever theorem prover would perform algebraic substitution on the
underlying constants of~$\Filter$, boiling them down to choices of branches,
which depend on one single variable, and loop transitions. This logic
corresponds to the placement of counters for optimally edge-profiling the CFG of
a program, making up a MST of the automaton~\cite{path-profiling}.

\subsection{An Example}

\subsection{The calculus is correct}

To be as clear as possible about what it means for our calculus to be correct,
we use more exact definitions than the traditional soundness and completeness.

\subsubsection{The calculus terminates}
\subsubsection{The calculus allows no values outside the Parikh image}
\subsubsection{The calculus excludes no values in the Parikh image}

\subsection{Finding the Presburger representation of a Parikh Image}
\contents{
  \item lazy qe
}

\section{Parikh Images from Products of Automata}

To generalise the calculus to calculations on products of automata, we change the main predicate to take arbitrarily many automata:
\begin{definition}
  $\ImagePredicate{\Automaton_1\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}$
  is true exactly when the single-automaton version of the predicate would hold
  for the automaton.
\end{definition}

  For the calculus, we first extend $\Expand$ to generate flow equations and instances of $\Connected$ for each automaton.


  \begin{table}[h]
    \begin{tabular}{@{}l>{$}c<{$}p{3cm}@{}}\toprule
      Name & Rule & Prerequisites\\
      \midrule
    
      % EXPAND
      \ExpandM & 
      \inferrule
      {\Set{\FlowEq(\Automaton_i), \Connected(\Automaton_i) \SuchThat \Automaton_i \in \Automaton_1,\ldots,\Automaton_k}, \ImagePredicate{\Automaton_1\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause}
      {\ImagePredicate{\Automaton_1\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause} & 
      None \\
      \Materialise &
      \inferrule
      {    \BindingSum(\Automaton', \Filter),\ImagePredicate{\Automaton'\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause}
      {\ImagePredicate{\Automaton_1\times\Automaton_2\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause} &
      $\Automaton' = \Automaton_1\times\Automaton_2$ \\
      \bottomrule
      \end{tabular}
      \caption{Additional derivation rules for products of arbitrarily many automata.}\label{tbl:rules:multi}
    \end{table}

\ExpandM{} must be applied before any other rule, like \Expand{}, but unlike \Expand{}, \ExpandM{} does not remove the $\Image$~predicate since it is needed to keep track of the product.

Before we can begin to define our final rule, we need to talk about product states.

Then we introduce the rule $\Materialise$, used to compute a partial product between two terms:

  With the following helper symbolic function:

  $$
  \BindingSum(\Automaton_1 \times \Automaton_2, \Filter) = \bigcup
  \begin{aligned}
  & \Set{
    \Filter(\Transition)  =  \sum\limits_{\Transition' = \FromLabelTo{\Tuple{\State, \State_R}}{\Label}{\Tuple{\State', \State_R}}} \SuchThat \Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions_{\Automaton_1}
  }, \\ 
  & \Set{
    \Filter(\Transition)  =  \sum\limits_{\Transition' = \FromLabelTo{\Tuple{\State_L, \State}}{\Label}{\Tuple{\State_L, \State'}}} \SuchThat \Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions_{\Automaton_2}
  }
  \end{aligned}
$$

Note that this definition implies that $\Filter(\Transition) =
\Filter(\Transition')$ whenever two transitions $\Transition \in
\Transitions_{\Automaton_1}, \Transition' \in \Transitions_{\Automaton_2}$
produces a product transition $\Transition'' \in \Transitions_{\Automaton_1
\times \Automaton_2}$. This corresponds to our intuition that the terms of the
product must agree on the value they accept. As before, we implicitly map
$\Filter$ to fresh terms for each transition in the product.

Finally, for instances of precisely one automaton, neither rule applies and we
perform the calculus as before.

\subsection{An Example}


\subsection{The extended calculus is also correct}
\subsubsection{The calculus terminates}
\subsubsection{The calculus allows no values outside the Parikh image}
\subsubsection{The calculus excludes no values in the Parikh image}


\section{Extensions}
\subsection{Backjumping and Learning No-Goods}
\subsubsection{An Example}
\subsection{Symbolic Automata}\label{sec:ext:symbolic}


\section{Implementation and Experiments}\label{sec:implementation}

We implement a calculus for Parikh automata as described in
Section~\ref{sec:parikh-automata}. The artefact submitted along with this paper
is a program that reads one or more products of one or more DFA, the register
incrementations performed along the edges of their transitions, and the labels
of the transitions as ranges of Unicode characters, along with a set of
constraints on the final values of their registers expressed as Presburger
arithmetic. We call this program \Catra{}.\footnote{If you really must read it
as an acronym, please read it as CAtegory Theory on Register Automata, or if you
object to the somewhat nonstandard use of register automata and category theory,
as Check Assignments of The Registers Afterwards, or alternatively if you find it
all to be too much of a theoretical exercise as Can Anyone Think of A Real
Application.}

\Catra{} is written in Scala, with the calculus described in this paper
implemented as a theory plug-in for the \Princess{} automated theorem
prover~\cite{princess}, which also performs the Presburger reasoning. For
comparison, we also provide an implementation of the baseline method
from~\cite{generate-parikh-image}, a direct translation that uses the~\Nuxmv{}
symbolic model checker~\cite{nuxmv} to solve our constraints, and \Fudge{the
approximation described in~\cite{approximate-parikh} on top of the standard
baseline back-end}. An example of an input file \Fudge{illustrating every feature
of the file format} can be seen in Listing~\ref{lst:input-example}.

\begin{lstlisting}[caption={An example input file for \Catra{}, illustrating every major syntax element. From beginning to end: synchronised (product) automata using the keyword \texttt{synchronised} (automata A and B), range labels, and their single-character shorthand syntax, register increments, and constraints on the final values of their counters.}, label=lst:input-example]
  /* Declare counters. Initially only integers, but other data-types
 could be considered as well. All counters start at zero. */
counter int x, y, z, length;

// List automata. Automata transitions are labelled with intervals,
// and can specify counter increments/decrements. A counter can only
// be used in at most one automaton.

synchronised {
    automaton A {
    init s0;

    s0 -> s1 [0, 10] { x += 1};

    accepting s1, s2;
    };


    automaton B {
    init t1;

    t1 -> t2 [11, 22] { y += 1};


    accepting t1, t2;
    };
};

automaton C {
    init s0;

    s0 -> s1 [15] { z += 1 };
    s1 -> s1 [15] { z += 1};

    accepting s1;
};


// Specify a constraint on the final values of counters; check whether
// there is a word accepted by all automata for which the final counter
// values satisfy this constraints.
constraint z = 2x + 2y && !(x = 0);
\end{lstlisting}

Please note that \Catra{} uses symbolic labels for automata. A symbolic label is
defined as a range of matching single Unicode code points. This allows succinct
representation of many regular expression patterns such as \lstinline{(a-z).*}
which would have otherwise required $27$~nearly identical transitions. In this
instance, the transition would be defined as \lstinline{a -> b [97, 122]}, as the
interval is closed from both edges.

In satisfaction mode, supported by all back-ends, \Catra{} tries to satisfy the
constraints expressed by the input file, reporting satisfiable with register
assignments or unsatisfiable much like traditional~SAT- or SMT solvers would.
Additionally, the baseline and our own back-end also support generating
the Presburger formula describing the constraints of the input file \Fudge{using
the method described in Section~\ref{sec:presburger-representation}}.

Note that \Catra{} can solve equations of Parikh images. An equation like
$\ParikhMap(\Automaton_1 \times \Automaton_2) = \ParikhMap(\Automaton_3 \times
\Automaton_4)$ would be expressed as one \lstinline{synchronised} block per side of
the equation containing their respective terms followed by a
\lstinline{constraint} requiring all their counters to agree (e.g
\lstinline|Aa = Ba && Ab = Bb ...|). Note that the product construction will implicitly require the counters of each term to agree with each other (and the product), so equalities between all counters of all automata are not necessary.

\subsection{Implementing the Baseline}\label{sec:implementing-baseline}

We implement the baseline approach using the same Presburger solver
(\Princess{}), input file parser, and automaton implementation as \Catra{}. We
do this in order to make the comparison between methods as fair as possible, and
give us the ability to compare the efficiency of the calculus rules themselves.
Using the formula of Equation~\ref{eq:generate-parikh}, we produce quantified
Presburger formulae for each successive term and add them to Princess. In this
fashion we compute the product incrementally term by term, checking
satisfiability in each step. We use \Fudge{a priority queue} to select which
term to use for the next step of the product, and order it by \Fudge{the number
of transitions} as a heuristic for the size of the term. We use this heuristic
to avoid computing large (and therefore slow) products until we have to, banking
our hopes on finding a source of unsatisfiability earlier. The pseudocode for
our implementation can be seen in Algorithm~\ref{alg:baseline}.

\begin{algorithm}
  \caption{An algorithm with caption}\label{alg:baseline}
  \KwData{$\Automaton_1, \ldots, \Automaton_n$ automata, other constraints $\SomeClause$}
  \KwResult{\textsc{Sat} or \textsc{Unsat}}
  \SetKwFunction{NewTheoremProver}{newTheoremProver}
  \SetKwFunction{NewPriorityQueue}{newPriorityQueue}
  \SetKwFunction{Dequeue}{dequeue}
  \SetKwFunction{Enqueue}{enqueue}
  \SetKwFunction{Assert}{assert}

  $p \gets \NewTheoremProver{}$

  \Assert{$p$, $\SomeClause$}

  \ForEach{$\Automaton_i$}{
    \Assert{$p, \ParikhMap(\Automaton_i)$}

    \If{$p$ is \textsc{Unsat}}{break}

  }

  $q \gets \NewPriorityQueue{}$


  \While{$p$ not \textsc{Unsat} and $|q| > 1$}{
    $\Automaton, \Automaton' \gets \Dequeue{q}$ 
    
    \Assert{$p, \ParikhMap(\Automaton \times \Automaton')$}

    \Enqueue{$q, \Automaton \times \Automaton'$}
  }
  
  \KwRet{$p$'s SAT status}

  \end{algorithm}

It should be noted that our automata (including the successive products) are
always by construction forward- and backward- reachable-minimal. We avoid
computing fully minimal automata since it is unclear when an automaton with
registers can be safely minimised. This guarantees that any automaton we produce
only contains states that are both reachable from the initial state and has a
path to an accepting state.

\subsection{Heuristics and search strategies}
\contents{
  \item Optimisations and tricks
}


\subsection{\Catra{} is Versatile: Encoding Epistemic Logic Problems As Parikh Automata}

\contents{
\item Model-checking examples e.g. \cite{epistemic-logic}
}

\subsection{\Catra{} is Faster Than The Competition: Benchmarks and Their Results}\label{sec:experiments}

We evaluate the performance of \Catra{} on~\NrBenchmarks{} instances generated by the \Ostrich{} string constraint solver when solving the \Fudge{pyex benchmarks}. \Fudge{The instances are all enormous and have some on average 900 products of 400000 automata with 97 counters each and 9999 constraints}. After generating an initial~\InitialNrBenchmarks{}, we remove~\NrBroken{} misgenerated instances that did not parse, as well as~\NrTrivial{} instances solved in under five seconds by the baseline backend.

The benchmarks are executed in parallel, since they are mostly single-threaded and initial results showed negligible interference on performance, on \BenchmarkRig{}. We compiled the code using Scala~\ScalaVersion{}, and executed the experiments on~\JvmVersion{} with a maximum heap of~\MaxHeapSize{}. We used \Nuxmv{} version~\NuxmvVersion{} invoked as a subprocess for each instance. Experiments were executed on a fresh JVM per backend, but the JVM was kept active for all benchmarks, meaning that JIT compilation would make subsequent executions faster. We believe this represents a realistic use case where \Calculus{} is integrated into a string solver or other system and be called repeatedly. To mitigate systemic impacts of this decision on our results, experiments were executed in random order for all backends. We run all  with a timeout of~\RuntimeTimeout{}.

As a way of gauging the differences in overhead between the native backends and \Nuxmv, we executed a small experiment where we ran the same trivial instance 30 times and observed the runtimes for all solvers. All backends saw an improvement from JIT compilation within the first three runs, but subsequent runs only improved the native backends. The final measured overhead for the fully warm JVMs was less than~\OverheadSeconds. From this we draw the conclusion that major differences in runtime between the native backends and \Nuxmv{} is more likely to stem from innate differences rather than unfair overhead resulting from our particular configuration.

All runtimes are measured in wall-clock time as observed by the JVM when executing the instance, and exclude time spent parsing, which in all observed cases was minimal in comparison to the runtime, usually far below 0.1 second.

It should be noted that despite running in the standard, deterministic configuration, we observed some degree of nondeterminism in \Nuxmv. Instances that were solved or unsolved would sometimes be solved or timed out in subsequent runs. This observation was made even without randomised execution order, suggesting that the nondeterminism likely stemmed either from our encoding of the instances into \Nuxmv's format, or the execution environment. Similarly, \Calculus{} showed similar signs of nondeterminism, but only for satisfiable instances. To address this, we increased the runtime from our initial experiments to give us a wider marigin. By contrast, the baseline implementation showed no signs of nondeterminism. It should be noted that for both backends exhibiting nondeterminism, the nondeterminism did not change the general trend of the experiments, even for experiments with few instances. Therefore, we believe that our large set of benchmarks will protect us from bias since any backend is unlikely to be neither consistently lucky nor consistently unlucky.

\subsubsection{Execution Time and Ability to Solve Instances}\label{sec:runtime}

In Figures~\ref{fig:solve-nr} and~\ref{fig:solve-division} we show how many of the~\NrBenchmarks{} the respective back-ends could solve and with which status. A full summary of their outcomes is also available in Table~\ref{tab:solve-status}. We see here \Fudge{that some solver is better than the others}.

\begin{figure}
  \caption{The number of solved instances per backend.}
  \label{fig:solve-nr}
\end{figure}

\begin{figure}
  \caption{The division of statuses per backend.}
  \label{fig:solve-division}
\end{figure}

\subsubsection{Scalability}\label{sec:scaling}

A cactus plot showing the number of instances solved within a given timeout for each backend can be seen in Figure~\ref{fig:cactus}. \Fudge{We see here that some solver is fast and some other is not}.

\begin{figure}
  \caption{The number of instances solved within a given number of seconds per backend.}
  \label{fig:cactus}
\end{figure}

\subsubsection{Finding a Presburger Formula}

For baseline and \Calculus{}, \Catra{} offers the ability to find the equivalent Presburger formula representing a given instance. For baseline, we use the built-in quantifier elimination facilities of the underlying \Princess{} theorem prover, while for \Catra{} \Fudge{we use the specially tailored approach described in some section}. For this experiment, we use only the~\NrKnownSat{} instances known to be satisfiable from the previous experiment detailed in Sections~\ref{sec:scaling} and~\ref{sec:runtime}. 

To make sure baseline puts up as much competition as possible, we disable
checking intermittent satisfiability and configure \Catra{} to run in the
maximally eager mode where the product is first computed before any
satisifiability check is performed. We run the experiments with a timeout of~\ImageTimeout{}. The results of the experiment is summarised
in Figure~\ref{fig:cactus:image} and Table~\ref{tab:image-results}. \Fudge{We see here that something happens}.

\begin{figure}
  \caption{The number of instances solved within a given number of seconds per backend.}
  \label{fig:cactus:image}
\end{figure}


\subsection{Threats to Validity}
\contents{
  \item There might be better implementations
  \item We might have over-biased
  \item Our benchmarks might be biased
  \item The underlying automata implementation might be bad and therefore skew
  the results.
  
}

\section{Conclusions}

\contents{
  \item Future extensions: more logics
  \item Loop invariants?
  \item Integration into string solvers?
  \item We have shown fastest
  \item We have shown versatile
  \item We have shown concretely useful
}

\section{Discussions}

\contents{
\item cf Parikh Images of Grammars: Complexity and Applications -- what is the best complexity we can do? https://ieeexplore.ieee.org/abstract/document/5571050

}

%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
  This material is based upon work supported by the
  \grantsponsor{GS100000001}{National Science
    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  conclusions or recommendations expressed in this material are those
  of the author and do not necessarily reflect the views of the
  National Science Foundation.
\end{acks}




%% Bibliography
\bibliography{bibliography}


%% Appendix
\appendix
% \section{Appendix}

% Text of appendix \ldots

\end{document}
