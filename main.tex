%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[acmsmall,review,anonymous,screen]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=true}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[acmsmall]{acmart}\settopmatter{}


%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{POPL} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2023}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format must update the '\documentclass' and
%% topmatter commands above; see 'acmart-sigplanproc-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption



\usepackage{amsmath,empheq,fancybox}
\usepackage{paralist}
\usepackage{url}
\usepackage{color}
\usepackage{textcomp,listings}
\usepackage{array}
\usepackage{mymacros}
\usepackage{microtype}
\usepackage{listings}
\usepackage{csquotes}
\usepackage{proof}
\usepackage{cleveref}
\usepackage{algorithm2e}      
\usepackage{multirow}
\usepackage{mathpartir}
\usepackage{amsthm}
\usepackage{ebproof}

\usepackage{mathtools} % Bonus


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\lstset{
    columns=fullflexible,
    showspaces=false,
    showtabs=false,
    breaklines=true,
    showstringspaces=false,
    breakatwhitespace=true,
    escapeinside={(*@}{@*)},
    commentstyle=\color{greencomments},
    keywordstyle=\color{bluekeywords},
    stringstyle=\color{redstrings},
    numberstyle=\color{graynumbers},
    basicstyle=\ttfamily\small,
    framesep=12pt,
    xleftmargin=12pt,
    tabsize=4,
    captionpos=b
}


\newif\ifcomments
\commentstrue
\newif\ifoutline
\outlinetrue

\newcommand{\contents}[1]{\ifoutline{\color{blue}
    \begin{itemize}
    #1
    \end{itemize}
  }\fi}

\allowdisplaybreaks[1]


\begin{document}

%% Title information
\title{An Efficient Calculus for Generalised Parikh Images of Regular Languages}
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}                   %% \country is recommended
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
  A common problem in string constraint solvers is computing the Parikh image, a
   set of linear equations that describe all possible combinations of character
   counts in strings of a given language. Automata-based string solvers
   frequently need to compute the Parikh image of large products (intersections)
   of automata, which in many operations is both prohibitively slow and
   memory-intensive. We contribute a novel understanding of how Parikh maps can
   be tackled as a constraint-solving problem to solve real-world constraints
   stemming from functions on regular languages, most notably the length
   constraint. Furthermore, we show how this formulation can be efficiently
   implemented as a calculus, \Calculus{}, in an automated theorem prover
   supporting Presburger logic. We also provide an efficient method for deriving
   the Presburger formula of a given Parikh image, \Fudge{which is good for
   something} based on techniques for quantifier elimination. We implement
   \Calculus{} in a tool called \Catra{}, and evaluate it on constraints
   produced by the \OstrichPlus{} string constraint solver when solving
   \Fudge{standard string constraint benchmarks involving string lengths}. We
   show that our solution strictly outperforms the standard eager approach
   described in~\citeauthor{generate-parikh-image} as well as the
   over-approximating method recently described
   by~\citeauthor{approximate-parikh}, and for realistic timeouts for constraint
   solving also the~\Nuxmv{} model checker.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{parikh images, string solvers, model checking}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}

The Parikh image is a characterisation of formal languages in terms of
their character counts. Given a language over an
alphabet~$\{a_1, \ldots, a_k\}$, the Parikh image is a set of
$k$-dimensional vectors that contains some
vector~$\VectorLiteral{m_1, m_2, \ldots, m_k}$ if and only if the
formal language contains a word in which each $a_i$ occurs $m_i$
times. It is a classical result that the Parikh image of every
context-free language (and, thus, also of every regular language) is a
semilinear set~\cite{parikh-theorem}, i.e., Presburger-definable.

Parikh images play a central role in many automata-based algorithms,
for instance and notably in today's string solvers, which often have
to process constraints that combine regular language membership with
word length. To decide whether a simple formula like
$x \in \Language_1 \wedge y \in \Language_2 \wedge |x| > |y|$,
with string variables~$x, y$ and regular
languages~$\Language_1, \Language_2$, is satisfiable, it is
necessary to reason about the sets of word lengths induced by
$\Language_1, \Language_2$, which is a special case of the Parikh
image.  This required combined reasoning about strings and string
length has long been identified as a major bottleneck in string
solvers
\cite{DBLP:conf/cav/AbdullaACHRRS15,length-aware-solver,approximate-parikh,more}.
Other string solvers make use of Parikh automata~\cite{X}, and thus
Parikh images in the general case, to handle operations including
\verb!str.substr!  and \verb!str.at!, which comes at an even higher
price in terms of computational complexity~\cite{ostrich-plus}.

\iffalse
It appears naturally as part of \Fudge{many operations} in model
checking and solving string constraints in automata-based solvers such
as \Ostrich{} \cite{ostrich}, notably in representing constraints on
string lengths. The enhanced \OstrichPlus{} solver~\cite{ostrich-plus}
makes even more extensive use of Parikh images.
\fi

It is possible to compute an existential Presburger formula describing
the Parikh image of any context-free language in linear
time~\cite{generate-parikh-image}; for the special case of regular
languages, this result was also stated in
\cite{muscholl-linear}. While theoretically elegant, this construction
has several disadvantages, often making it unpractical for integration
into algorithms. Firstly, the constructed Presburger formula contains
a linear number of existential quantifiers in the size of the
considered grammar, as well as complex Boolean structure, which is
needed to express the connectedness of sets of productions considered
in the construction. Eliminating those quantifiers to obtain a
quantifier-free representation of the Parikh image has exponential
complexity~\cite{X}, and is in practice often impossible in reasonable
time. Just solving the Parikh image membership problem is NP-complete,
as it corresponds to computing a satisfying assignment of the
existential Presburger formula, and taxing for solvers as
well~\cite{ostrich-plus}.

\iffalse
Later improvements have produced a construction taking at
most linear time to produce~\cite{muscholl-linear}. However, the
resulting existentially quantified clauses are costly to eliminate as
the number of variables increases, in practice making many real-world
problems intractable.
\fi

Secondly, in applications involving regular languages, it is typically
necessary to consider the Parikh image not only of a single automaton,
but of the intersection of multiple automata. This problem arises in
string solvers in particular, as conjunctions of string constraints
lead to the computation of length images of products (intersections)
of regular languages represented as finite automata. Applying the
approach in~\cite{generate-parikh-image} would in this case require
the eager computation of the product before its length image, and
result in an existential Presburger formula of exponential size (in
the number of automata). In several instances we have observed while
solving real-world string constraints, the computation of the product
of automata exhausts the memory of any machine available due to the
exponential blow-up in size of the product, quickly becoming
intractable as the number of automata in the product increases. The
current best published mitigation for this problem is an
over-approximation that works by approximating the Parikh image of a
product of automata to be the conjunction of the image of the
individual automata of the product \cite{approximate-parikh}. This
approach only works for unsatisfiable instances, and comes with a
harsh penalty for satisfiable instances.

Addressing these concerns, we have developed a calculus for Parikh images of
products of regular languages that we call \Calculus{}. It allows us to
interleave the computation of arbitrarily deep products of automata with the
product's Parikh image, and is generalised to an arbitrary homomorphism over
automata labels, including string lengths. This enables us to let both
calculations inform each other, eliminating unnecessary work, and pruning the
size of the partial products considered in the computation for a smaller memory
footprint. Moreover, the method can be used iteratively to tackle smaller chunks
of the product incrementally, thereby avoiding running out of memory at all.

The problem of computing constraints on Parikh images over products of regular
languages under a given commutative homomorphism amounts to solving products of
Parikh automata. Parikh automata are regular automata extended with integer
counters with given increments and decrements for each transition, where we
allow checking a set of linear constraints on the final values of the counters
(but not their intermittent values) \cite{parikh-automata}. Parikh automata
without constraints on the final values on their registers are also sometimes
called cost-enriched automata, weighted automata or counter automata, depending
on exact definitions and side-constraints. The decision problem tackled in this
paper, determining the emptiness of an intersection of Parikh automata, was
recently shown to be PSPACE-complete~\cite{graph-queries}.

In addition to the decision problem of determining whether an intersection of
Parikh automata is empty and the satisfaction problem of finding a value from
the intersection if it is not, \Catra{} is able to efficiently generate a
quantifier-free Presburger formula representing an intersection of Parikh
automata, something beyond the reach of general model checkers like~\Nuxmv.
\Fudge{This is useful because the moon is made of cheese.}

We implement \Calculus{} as a theory for the \Princess{} automated theorem
prover in a tool that we call \Catra{}. \Catra also supports using the
approximate method of \cite{approximate-parikh}, its fall-back variant adapted
from~\cite{generate-parikh-image}, and an adapter for the \Nuxmv{} model
checker~\cite{nuxmv}. Using \Catra{}, we compare \Calculus{} to the other two
back-ends on \NrBenchmarks{} Parikh automata satisfaction problems generated
\Fudge{by \OstrichPlus{}}.

In summary, we contribute:
\begin{itemize}
\item The \Calculus{} calculus to efficiently compute (a homomorphism on) the
Parikh image of products of Parikh automata.
\item A specialised method for quantifier elimination that blends well with \Calculus{} to efficiently compute quantifier-free Presburger representations of a given Parikh image. \Fudge{Why is this good?}
\item Experiments illustrating the performance of \Calculus{} on real-world examples from string solving, including \NrBenchmarks{} instances in a standardised format made available for future study.
\item The \Catra{} tool for solving such instances, containing an implementation of \Calculus{}, the over-approximation described in~\cite{approximate-parikh}, and an adapter for the~\Nuxmv{} model checker~\cite{nuxmv}.
\item Suggestions for how to efficiently implement \Calculus{} in a modern automated theorem prover, including strategies for case splitting, clause learning, and constraint propagation for connectedness.
\end{itemize}

\subsection{Related Work}

Parikh image computations as well as Parikh automata \cite{parikh-automata}
feature extensively in string solvers, including as mentioned above \Ostrich{}
and \OstrichPlus{} \cite{ostrich,ostrich-plus}, but also formes the basis of
Trau~\cite{trau-pldi}, and occurs in \textsc{Sloth}~\cite{sloth}. Parikh images
frequently appear when introducing cardinality constraints like length or string
indexing. The state-of-the-art approach to handling Parikh image computation is
to over-approximate the Parikh image of a product of $k$~automata
$\ParikhMap(\Automaton_1 \times \ldots \times \Automaton_k)$ with the
conjunction of the automata's parikh maps, $\AndComp{i=1}^{k}
\ParikhMap(\Automaton_i)$. This approach works only for unsatisfiable instances,
and will require falling back to computing the product of the automata before
using the standard approach for finding its image originally presented in~\cite{generate-parikh-image}.

Outside of string solvers, Parikh automata have been proposed as the basis of
queries~\cite{graph-queries}, and for solving cardinalities in model checking
problems involving epistemic logic~\cite{epistemic-logic}.

Many other generalisations of the Parikh than the ones we use here have been
studied. Prominent examples include generalising the Parikh map to segments of a
fixed length \cite{KARHUMAKI1980155} and the more general Parikh matrix, which
gives more information about a word than the standard Parikh image
\cite{parikh-matrix}. Another notable generalisation is the p-vector, introduced
in~\cite{infinite-words}, which denotes the position of each letter in the word
rather than the number of their occurrences and allows for generalisations into
infinite alphabets. All of these in some sense extend the Parikh map. By
contrast, the main utility of the formulation introduced here is that it allows
us to \emph{remove} something, thereby potentialy obtaining an easier problem.

\section{A demonstration of our approach}\label{sec:motivation}

To illustrate the key features of \Calculus, we will informally present our
reasoning. We begin by considering the following set of string and other
constraints, which we want to solve for all possible values of $\MonoidElement$,
a value in some commutative monoid and the string $s$, where
$\varphi(\MonoidElement)$ is some set of constraints on $\MonoidElement$:
\begin{equation*}
  \AndComp{}{\begin{aligned}
  & \mathtt{ac^*a|b(bb)^*c} \text{ accepts } s\\
  & \mathtt{.^*c.^*} \text{ accepts } s \\
  & \MonoidElement = f(s) \\
  & \varphi(\MonoidElement)
  \end{aligned}}
 \end{equation*}

We will represent the strings using the nondeterministic finite automata
 $\SomethingCSomething{}$ (\cref{fig:something-c-something}) and $\AcaOrBc{}$
 (\cref{fig:aca-or-bc}) respectively.

 Throughout these examples we will present our three key ideas:
 \emph{automata-aware splitting}, \emph{lazy enforcement of automata
 connectivity}, and \emph{incremental materialisation of products}.
 
\subsection{Counting string lengths}

To keep things simple, in this first example we let $f$ be the function that
gives the length of a given string, e.g. $f\left(\text{curious and curiouser
still}\right) = 27$, and $f\left(\text{homo}\right) +
f\left(\text{morphism}\right) = f\left(\text{homomorphism}\right) = 12$, and we
let $\varphi(\MonoidElement) = \exists k \HoldsThat m = 2k + 1 \land k \geq 0$,
i.e. require that the length of the string is odd.

\begin{figure}[t]
  \begin{minipage}[b]{0.50\linewidth}
  \centering 
    \includegraphics[width=\textwidth]{aca_or_bc}
    \caption{An automaton recognising the regular expression
    $\mathtt{ac^*a|b(bb)^*c}$.}\label{fig:aca-or-bc}

    \includegraphics[width=\textwidth]{something_c_something}
    \caption{An automaton recognising the regular expression
    $\mathtt{.^*c.^*}$.}\label{fig:something-c-something}
  \end{minipage}
  \Description[Two deterministic finite automata, one above the other]{NFAs 
    recognising the languages ac^*a|b(bb)^*c and ac^*a|b(bb)^*c respectively.
    The first one has 4 states and the second one 2.}
  \end{figure}

The na\"ive approach would be to first compute the product of the automata, then
use the approach from \cite{generate-parikh-image} to find its image under $f$,
and then verify the constraints. However, we can take advantage of both the
constraints and definitions of $f$ to prune the automata before computing the
product and help reduce the blow-up.

Similar to \cite{generate-parikh-image} we associate each transition
$\Transition$ with an integer variable $\TransitionVar_\Transition \geq 0$
representing how many times $\Transition$ is used that we leave implicitly
existentially quantified. These variables are shown as annotations under the
transition labels in \cref{fig:aca-or-bc,fig:something-c-something}. We then
apply graph-like flow reasoning by adding linear equations (e.g. $1 + {l_b}' =
l_a + l_b$, or $l_a + l_c' = 1$) that equate the incoming transitions into a
state to the ones going out. For the initial and accepting states we add an
incoming and outgoing flow of 1, respectively. We will use these linear
equations to perform reasoning to reduce the size of the product before we
compute it. Note that we have already performed some linear reasoning to
eliminate some of the variables in the graphs above, for example to introduce
the constant number $1$ in \cref{fig:something-c-something}, which comes from $1
+ r_\Sigma = x + r_\Sigma$ solved for $x$.

\begin{figure}[t]
  \centering 
    \includegraphics[width=0.50\linewidth]{aca_or_bc_simplified}
    \caption{$\AcaOrBc{}$ with its symbolic transition counters after
    simplification of their equations.}\label{fig:example-simplify-1}
    \Description[The automaton $\AcaOrBc{}$ labelled with variables counting the number of times each transition is taken, after having simplified the equations.]{The starting state shows an initial flow of 1, with two outgoing transitions to $A$ ($1-l_c'$ on letter a, and $l_b' + l_c'$ on b respectively). $A$ has a flow of $l_c$, and its outgoing transition into the final state, $F$ is the same as the incoming one from $S$, the initial state. The $B$ transition has an outgoing transition labeled $c/ l_c'$ to the final state $F$, and one backwards transition $b / l_b'$ to the starting state.}
  \end{figure}

We sum the individual transition variables to get the length $\MonoidElement$ we
are actually interested in. For example the image of $f$ on
$\SomethingCSomething$ is given by $\exists_{r_\Sigma, r_\Sigma'} \: 2k + 1 =
r_\Sigma + 1 + r_\Sigma' \land k \geq 0$. For $\SomethingCSomething{}$, this is
all we need to produce the image since all transitions are always accessible
from any path from the initial state to the accepting state. The difficult
connectivity constraint is not necessary to enforce here, since any path through
$\SomethingCSomething{}$ is connected, and therefore the flow equations
described above are sufficient to capture its image.

The story gets more complicated with $\AcaOrBc{}$, where we have a choice of two
paths, up or down, and where that choice affects the possibility of taking
either of the two loops. In that case, we would have the image $2l_a + l_c + l_c' + l_b + l_b'$.
If we apply standard linear reasoning on the equations (e.g. $1 + {l_b}' = l_a +
l_b$, or $l_a + l_c' = 1$), we can reduce the number of bound
($\exists$-quantified) variables somewhat, obtaining the relations in
\cref{fig:example-simplify-1}, and the corresponding over-approximation on
the $f$-image $2 + l_c + 2l_b'$. It is an over-approximation because it only
considers the number of times each transition is taken and the existence of a
path from an initial to an accepting state, but omits the connectivity of that
path in the presence of loops. For example, no valid path would have both $l_c >
0$ and $1 - l_c' = 0$, but no such restriction exists in the formula above.

\begin{figure}[t]
  \centering 
    \includegraphics[width=0.4\linewidth]{something_c_something_length_simplified}
    \caption{$\SomethingCSomething{}$ after using linear reasoning over lengths to replace a variable.}\label{fig:example-length-reduced}
    \Description[The automaton $\SomethingCSomething{}$ after reasoning over lengths.]{The same automaton as before for $\SomethingCSomething{}$, but with the transition from start to final annotated with a constant number 1, and the leftmost self-loop transition replaced with the equation $1 + l_c + 2l_b' -
    r_\Sigma'$.}
  \end{figure}

Still, this approximate view allows us to perform further reasoning. Since we
know that the length of a string accepted by the intersection of the two
automata's languages must be the same, we also know that their images must be
the same under $f$, e.g. that $r_\Sigma + 1 + r_\Sigma' = 2 + l_c + 2l_b' \iff
r_\Sigma + r_\Sigma' = 1 + l_c + 2l_b'$. This in turn allows us to replace one
additional variable, e.g. $r_\Sigma = 1 + l_c + 2l_b' - r_\Sigma'$ (see
\cref{fig:example-length-reduced}). Note that the two automata are now in
contact, allowing us to propagate reasoning on one to the other.

Since we cannot take a transition a negative number of times, we have an
implicit constraint on any transition variable that it must be at least $0$.
This means that we have $1 + l_c + 2l_b' - r_\Sigma'$, implying an upper bound
on $r_\Sigma'$: $r_\Sigma' \leq 1 + l_c + 2l_b'$.

This is as far as we can get with this reasoning. We must now choose: either we
compute (materialise) the product and continue our reasoning, or we perform a
case split. Let us try a case split in order to put off computing a potentially
large product.

We apply the principle of splitting and select an early transition in the
largest automaton with loops, in our case $\FromLabelTo{S}{a}{A}$ of $\AcaOrBc$.
This creates two cases: $1-l_c' = 0$ and $1-l_c' > 0$. We start with the first
case, and after propagating the now known values and removing any transition
that becomes zero, we get the automaton of \cref{fig:aca-or-bc-length-split-1}.


\begin{figure}[t]
  \centering 
    \includegraphics[width=0.75\linewidth]{aca_or_bc_length_split_1}
    \caption{$\AcaOrBc{}$ at the split where $\FromLabelTo{S}{a}{A}$ is not
    used.}\label{fig:aca-or-bc-length-split-1}
    \Description[The automaton \AcaOrBc{} where we have removed the transition
    from $S$ to $A$.]{ The state $A$ hangs free without incoming transitions,
    and only the lower part of the automaton remains.}
  \end{figure}

  We immediately notice that the self-loop of state $A$ is now disconnected from
  the initial state and must be removed. This condition can be detected
  efficiently using standard forwards/backwards reachability from the initial
  and accepting states respectively. In other words, $2k+1 = 0$ for this case
  and is implied by the splitting constraint. However, no choice of integer $k$
  will satisfy this equation. Therefore, we close this branch and proceed with
  the other one -- where $1-l_c' > 0$. Note that we do so without ever computing
  a potentially expensive product of automata.

  \begin{figure}[t]
    \centering 
      \includegraphics[width=0.5\linewidth]{aca_or_bc_length_split_2}
      \caption{$\AcaOrBc{}$ at the split where $\FromLabelTo{S}{a}{A}$ \emph{is} used.}\label{fig:aca-or-bc-length-split-2}
      \Description[$\AcaOrBc{}$ with the transition from $B$ to $F$ removed.]{$\AcaOrBc{}$ when we keep the transition from $S$ to $A$. This removes the transition between $B$ and $F$, but otherwise maintains the automaton.}
    \end{figure}
  
Plugging in the assumption $1-l_c' > 0$ (and its consequence that $l_c' = 0$),
we also get a reduced variant of $\AcaOrBc$, as seen in
\cref{fig:aca-or-bc-length-split-2}. Note that there is now no way to select any
transition variable to disconnect a loop from a path between $S$ and $F$,
and we can therefore stop worrying about enforcing connectivity at all.

\begin{figure}[t]
  \centering 
    \includegraphics[width=0.5\linewidth]{length_split_product}
    \caption{The product of $\AcaOrBc$ and $\SomethingCSomething$ after
    case splitting and removal of dead states.}\label{fig:length-split-product}
    \Description[$\AcaOrBc{}$ with the transition from $B$ to $F$ removed.]{$\AcaOrBc{}$ with the transition from $B$ to $F$ removed. There now remains only one path to an accepting state.}
  \end{figure}

Before continuing, we need to tie the inequalities for the transitions of the
automata of the product to the new transition counting variables we introduced
in the product automaton to ensure that they are consistent. In our case, we
obtain the following equations from the principle that a transition variable in
either automaton must be equal to the sum of variables in the resulting edges of
the product, which gives the following two interesting inequalities from
$\AcaOrBc{}$'s transition $\FromLabelTo{A}{c}{A}$: $2k + 1 = r_1 + r_2 + 1$

Note how this conserves the requirement that the number of loop iterations be
odd even when the same original loop appears three times in the product, twice
as a self-loop and once as a regular state transition. Using the same principle
as before for computing the length by adding up the transition counters, we
obtain the quantified formula $1 + 2k + 1 + 1 + r_b + r_b= 3 + 2k + 2r_b$ for
the length. Eliminating the final quantifier gives us the image of $f(\AcaOrBc
\times \SomethingCSomething)$ modulo odd lengths: $3 + 2k$, for some integer
$k$.

\subsection{Producing the Parikh image}\label{sec:introduction:parikh}

Now assume that rather than the length (e.g. the count of the characters) we
want to find the whole Parikh image of the intersection of $\AcaOrBc{}$ and
$\SomethingCSomething{}$. In other words, now $f$ is a mapping to a 3-vector
counting how often each character occurs in a string. Since extracting values
from a vector is tiresome, we will use the shorthand notation
$\TransitionCount{a}$ to refer to the number of letters a that appear. E.g.
$\TransitionCount{a}(\text{curious and curiouser still}) = 1$. Note that we
still have the property that $\TransitionCount{o}(\text{homomorphism}) =
\TransitionCount{o}(\text{homo}) + \TransitionCount{o}(\text{morphism}) = 2 + 1
= 3$. If you treat the addition as element-wise vector addition, this approach
works for the vector version as well.

For our constraint, we will use $\TransitionCount{a} > \TransitionCount{b}$,
that is finding a count of characters in a string accepted by both automata where
there are more a:s than b:s. Since the underlying calculus is the same, we start immediately from Figure~\ref{fig:example-simplify-1}.

  We now need to relate the number of times a transition was taken to the
  character counts we are really interested in, as opposed to just the length in
  the previous example. For the $\Sigma$ labels, since we do not know which
  letter to use, we introduce a sum $r_{a1} + r_{b1} + r_{c1}$ per $\Sigma$
  transition.

  As in the previous example, we sum the occurrences for each transition to
  obtain the counts, here with the element-wise vector addition unpacked:
  \[
\MonoidElement =   \begin{bmatrix}
  r_{a1} + r_{a2} \\
  r_{b1} + r_{b2} \\
  r_{c1} + r_{c2} + 1 \\
  \end{bmatrix} =
  \begin{bmatrix}
    2 -2l_c' \\
    2l_b' + l_c' \\
    l_c + l_c' \\
    \end{bmatrix} =
    \begin{bmatrix}
      \TransitionCount{a} \\
      \TransitionCount{b} \\
      \TransitionCount{c}
      \end{bmatrix}
  \]
  
These equalities can be combined with the constraint that $\TransitionCount{a} >
\TransitionCount{b}$ to obtain $2 - l_c' > l_b'$. Since we have $1-l_c' \geq 0$
from one of the transition labels, we know $l_c' \geq 0$, and therefore that $1
> l_b' \implies l_b' = 0$. Plugging in the same inequality again on the same
principle, we get $2 - 2l_c' > l_c' \iff l_c' = 0$. This gives us the much
smaller automaton of \cref{fig:example-simplify-3}.

% \begin{figure}[t]
%   \centering 
%     \includegraphics[width=0.70\linewidth]{aca_or_bc_simplified_2}
%     \caption{$\AcaOrBc{}$ after removing the now unused b-transition.}\label{fig:example-simplify-2}
%     \Description[$\AcaOrBc$ without the returning transition from $B$ to $S$]{$\AcaOrBc$ without the returning transition from $B$ to $S$, which now makes the transition from $S$ to $B$ also have the same associated variable $l_c'$.}
%   \end{figure}


  \begin{figure}[t]
    \centering 
      \includegraphics[width=0.7\linewidth]{aca_or_bc_simplified_3}
      \caption{$\AcaOrBc{}$ after further constraint propagation.}\label{fig:example-simplify-3}
      \Description[$\AcaOrBc$ without the lower section.]{$\AcaOrBc$ with only its upper section left and the $a$ transitions now set to static 1.}
    \end{figure}

This will produce a straightforward product since both automata now have the
same shape, where we essentially only need to expand the self-loops of
$\SomethingCSomething$ and reduce its range labels to only capture a's, as seen
in \cref{fig:final-example}. Note that we got this result by only performing
linear inequality reasoning on the automata of the product versus the number of
times each transition would be used.

\begin{figure}[t]
\centering
    \includegraphics[width=0.75\linewidth]{aca_or_bc_times_something_c_something}
    \caption{$\AcaOrBc{} \times \SomethingCSomething{}$ under the constraint
    $\TransitionCount{a} > \TransitionCount{b}$. Note the fresh variables
    introduced for the transitions (for most transitions the same one, since
    they are all linearly dependent).}\label{fig:final-example} \Description[The
    final product, still a boring stick automaton with only one loop for an
    arbitrary number of c's.]{A product of the intermittent automata, which
    accepts \lstinline{ac.*ca}}
  \end{figure}

%   \begin{figure}[t]
% \centering
%     \includegraphics[width=0.75\linewidth]{original_product}
%     \caption{$\AcaOrBc{} \times \SomethingCSomething{}$ as it would have
%     appeared if we had computed it from the initial
%     automata.}\label{fig:original-product}
%   \end{figure}

To obtain the final Parikh image, we again need to tie the inequalities for the
transitions of the automata of the product to the new transition counters we
introduced in the product automaton. The only interesting transition is the one
carrying the c, so we will choose that one for illustration. In that case, the
bridging equation is $r_2 = l_c \land r_2 = r_{\Sigma}$ (and for all others
$=1$), which gives the Parikh image $\exists_{r_2} \TransitionCount{a} = 1 \land
\TransitionCount{b} = 0 \land \TransitionCount{c} = r_2 + 1$, and after
existence-elimination using reasoning on lower bounds (e.g. $r_2 \geq 0$) we
arrive at $\TransitionCount{c} \geq 1$.

\section{Preliminaries}

\subsection{Monoids}

We write commutative monoids as $\Monoid =
\left(X;+_{\Monoid};0_{\Monoid}\right)$, where we mean $X$ to be the underlying
set of elements, $+_{\Monoid}$ to be the (commutative) monoid map, with
$0_{\Monoid}$ as its neutral element.

\subsection{Languages, Finite-state Automata and their Products}

We define an alphabet as a set of symbols $\Alphabet$ with words $\Strings$, and
the concatenation operation as $s_1 \Concat{} s_2$ over two strings $s_1, s_2$.

A non-deterministic automaton~$\Automaton$ with alphabet~$\Alphabet$ is
$\AutomatonTuple$ where $\Transitions = \States \times \Alphabet \times
\States$, $\States$ is its states, $\InitialState$ is w.l.o.g. assumed to be the
single initial state, and $\AcceptingStates$ is its set of accepting states.  We
write a transition $\Transition = \Tuple{\State, \Label, \State'} \in
\Transitions$ as $\Transition = \FromLabelTo{\State}{\Label}{\State'}$.
Similarly, we use the notation $\FromLabelTo{\State}{}{}$ to refer to the set of
transitions starting in $\State$, and $\FromLabelTo{}{}{\State}$ to refer to the
set of transitions coming into $\State$, whenever the automaton is clear from
the context.

We will let variables $\Transition, \Transition', \Transition_1, \ldots,
\Transition_n$ etc describe transitions, $\State, \ldots, \State_n$ states, and
$\Automaton, \ldots, \Automaton_n$ automata, and use subscript indexing
($\Transitions_\Automaton$) to refer to the transitions, states, etc of a given
automaton.

By a \emph{product} of two automata $\Automaton_1, \Automaton_2$, written
$\Automaton_1 \times \Automaton_2$, we mean an automaton constructed to run
$\Automaton_1$ and $\Automaton_2$ in parallel on an input and only accept the
input if both automata would do so.

We refer to the resulting product states as tuples, $\Tuple{\State, \State'}$,
which represent the state of the product automaton where $\Automaton_1$ would be
in $\State$ and $\Automaton_2$ would be in $\State'$. Note that since we use
ordered tuples the product is technically (but w.l.o.g) not commutative; the
left-hand-side must come from the left-hand term. The sole purpose of this
matching is to allow us to speak with precision about the origins of components
in a product.

\subsection{The Parikh Map and its Image}
Formally, the \textit{Parikh map} over an alphabet $\Alphabet=
\left\{a_1, \ldots, a_k \right\}$ is defined as in \cite{kozen}:
$$
\begin{aligned}
& \ParikhMap: \MapFromTo{\Strings}{\natural^k} \\
& \ParikhMap(s) = \VectorLiteral{\#a_1(s), \#a_2(s), \ldots, \#a_k(s)}
\end{aligned}
$$

That is, $\ParikhMap(s)$ is a vector of the number of occurrences of each
character in the language for a given string $s$. For example, for  $\Alphabet =
\Set{a, b}$, we would have $\ParikhMap(abb) = \VectorLiteral{1, 2}$.

We define the image of this map, the \textit{Parikh image}, of some subset of
the language $\Language \subseteq \Strings$ as:
\[
\ParikhMap(\Language) = \Set{\ParikhMap(x) \SuchThat x \in \Language}
\]

Thus, we would have $\ParikhMap(\left\{ab, abb\right\}) = \left\{\left[1,
1\right], \left[1, 2\right]\right\}$. We also sometimes use the standard
notation $\#l(w)$ to talk about an individual letter $l$ in a word $w$. For
example, for the Parikh vector above, we would have $\CountOf{a} = 1$. You have
already seen this in \cref{sec:introduction:parikh}.

Parikh's theorem states that any context-free language has a letter-equivalent
regular language (c.f.~\cite{construction} for a construction of such automata
from context-free grammars and~\cite{bounds} for bounds on its size). However,
there are languages that are not context-free that also have semilinear images
under~$\ParikhMap$ (e.g. $\ParikhMap(\Set{a^nb^nc^n \SuchThat n \geq 0}) =
\ParikhMap((abc)^*) = \CountOf{a} = \CountOf{b} = \CountOf{c} \land \CountOf{a}
\geq 0$). This means they can be represented as a quantifier-free Presburger
formula.

Note that while Parikh's theorem applies to context-free languages, in this
paper we focus only on regular languages.

\subsection{The Parikh image of a regular language expressed in Presburger arithmetic}

Since Parikh images are semilinear, any Parikh image can be written as a set of linear
equations. The following construction, here adapted to work on an NFA $\Automaton = \AutomatonTuple$, was presented in \cite{generate-parikh-image}:
\begin{equation}
\begin{aligned}
\ParikhMap(\Automaton) := 
& \AndComp{\Letter \in \Alphabet}{\LetterVar_{\Letter} = \sum_{\Transition = \FromLabelTo{}{\Letter}{} \in \Transitions} \TransitionVar_{\Transition}
}\\
& \AndComp{\State \in \AcceptingStates}{\FinalStateVar_{\State} > 0 \longrightarrow  \StateVar_{\State} > 0}\\
& \AndComp{\State \in \States}{
  \left(\FinalStateVar_{\State} \text{ if } \State \in \AcceptingStates \text{ otherwise } 0 \right) +
  \sum_{\Transition = \FromLabelTo{\State}{}{}} \Filter(\Transition) - \sum_{\Transition = \FromLabelTo{}{}{\State}} \Filter(\Transition)
= \begin{cases}
    1 \text{  if $\State = \InitialState$} \\
    0 \text{ otherwise}
  \end{cases}
}\\
& \AndComp{\Transition = \FromLabelTo{\State'}{}{\State} \in \Transitions}{
  \TransitionVar_{\Transition} > 0 \longrightarrow \StateVar_{\State} > 0
} \\
& \AndComp{\State \in F}{
  \StateVar_{\State} > 0 \longrightarrow \FinalStateVar_{\State} = 1
} \\
&\AndComp{\State \in \States}{
  \StateVar_{\State} > 0 \longrightarrow
  \left(\FinalStateVar_{\State} = 1 \text{ if } \State \in \AcceptingStates \land \right) \OrComp{\Transition = \FromLabelTo{\State}{}{\State'} \in \Transitions}{
    \StateVar_{\State} = \StateVar_{\State'} + 1 \land 
    \TransitionVar_{\Transition} \geq 1 \land
  \StateVar_{\State'} \geq 1
    }
}
\end{aligned}
\label{eq:generate-parikh}
\end{equation}

All variables $\TransitionVar_i, \StateVar_i, \FinalStateVar_i$ are
existentially quantified and the free variables $\LetterVar_\Letter$ make up the
image. $\StateVar_\State$ represents the distance of state $\State$ from
$\InitialState$ in a spanning tree, $\TransitionVar_\Transition$ how many times
a transition $\Transition$ is used, and $\FinalStateVar_\State$ whether a given
accepting state $\State \in \AcceptingStates$ is the actually used final state.

In this paper we refer to this model as the baseline approach, though we also
apply optimisations as described in \cref{sec:implementing-baseline}. The
calculus introduced in this paper, by contrast, lazily enforces the
connectedness constraint of this encoding (the final three clauses) while also
interleaving the computation of products of automata and propagating information
between the steps to reduce the amount of work that needs to be done.

\subsubsection{An Example}
We apply \eqref{eq:generate-parikh} to the automaton $\AcaOrBc{}$ of \cref{fig:aca-or-bc}. We then get the quantified Presburger formula seen in \eqref{eq:parikh-formula}. As before, we omit the implicit existential quantifiers for brevity, along with the constranints requiring all variables to be nonnegative. We also omit the $\land$-clauses between each line. In other words, in this example, only the variables $\LetterVar_a, \LetterVar_b, \LetterVar_c, \LetterVar_d, \LetterVar_e$, and $\LetterVar_f$ are free, and corresponds to the counts of the respective subscript letter.

\begin{equation}
  \begin{aligned}
  \ParikhMap(\Automaton):=\:
  &\LetterVar_{a} = \TransitionVar_{\FromLabelTo{0}{a}{1}} + \TransitionVar_{\FromLabelTo{1}{a}{3}} \land \LetterVar_{b} = \TransitionVar_{\FromLabelTo{0}{b}{2}} + \TransitionVar_{\FromLabelTo{2}{b}{0}} \land \LetterVar_{c} = \TransitionVar_{\FromLabelTo{1}{c}{1}} + \TransitionVar_{\FromLabelTo{2}{c}{3}}\\
  &\FinalStateVar_{3} > 0 \implies \StateVar_{3} > 0\\
  &\TransitionVar_{\FromLabelTo{0}{b}{2}} + \TransitionVar_{\FromLabelTo{0}{a}{1}} - \TransitionVar_{\FromLabelTo{2}{b}{0}} = 1 \land  \TransitionVar_{\FromLabelTo{1}{a}{3}} - \TransitionVar_{\FromLabelTo{0}{a}{1}} = 0\\
  & \TransitionVar_{\FromLabelTo{2}{b}{0}} + \TransitionVar_{\FromLabelTo{2}{c}{3}} - \TransitionVar_{\FromLabelTo{0}{b}{2}} = 0 \land \FinalStateVar_{3}  - \TransitionVar_{\FromLabelTo{1}{a}{3}} - \TransitionVar_{\FromLabelTo{2}{c}{3}} = 0\\
  &\TransitionVar_{\FromLabelTo{0}{a}{1}} > 0 \implies \StateVar_{1} > 0 \land \TransitionVar_{\FromLabelTo{0}{b}{2}} > 0 \implies \StateVar_{2} > 0\\
  &\TransitionVar_{\FromLabelTo{1}{c}{1}} > 0 \implies \StateVar_{1} > 0 \land \TransitionVar_{\FromLabelTo{1}{a}{3}} > 0 \implies \StateVar_{3} > 0\\
  &\TransitionVar_{\FromLabelTo{2}{b}{0}} > 0 \implies \StateVar_{0} > 0 \land \TransitionVar_{\FromLabelTo{2}{c}{3}} > 0 \implies \StateVar_{3} > 0\\
  &\StateVar_{0} > 0 \implies \left(\StateVar_{0} = \StateVar_{1} + 1 \land \TransitionVar_{\FromLabelTo{0}{a}{1}} \geq 1 \land \StateVar_{1} \geq 1\right) \lor \left(\StateVar_{0} = \StateVar_{2} + 1 \land \TransitionVar_{\FromLabelTo{0}{b}{2}} \geq 1 \land \StateVar_{2} \geq 1\right)\\
  &\StateVar_{1} > 0 \implies \left(\StateVar_{1} = \StateVar_{1} + 1 \land \TransitionVar_{\FromLabelTo{1}{c}{1}} \geq 1 \land \StateVar_{1} \geq 1\right) \lor \left(\StateVar_{1} = \StateVar_{3} + 1 \land \TransitionVar_{\FromLabelTo{1}{a}{3}} \geq 1 \land \StateVar_{3} \geq 1\right)\\
  &\StateVar_{2} > 0 \implies \left(\StateVar_{2} = \StateVar_{0} + 1 \land \TransitionVar_{\FromLabelTo{2}{b}{0}} \geq 1 \land \StateVar_{0} \geq 1\right) \lor \left(\StateVar_{2} = \StateVar_{3} + 1 \land \TransitionVar_{\FromLabelTo{2}{c}{3}} \geq 1 \land \StateVar_{3} \geq 1\right)\\
  &\StateVar_{3} > 0 \implies \FinalStateVar_{3} = 1
  \end{aligned}
  \label{eq:parikh-formula}
  \end{equation}
    
  We see here that the formula encodes the choice of path at the start of the
  automaton ($\TransitionVar_{\FromLabelTo{0}{b}{2}} +
  \TransitionVar_{\FromLabelTo{0}{a}{1}} -
  \TransitionVar_{\FromLabelTo{2}{b}{0}} = 1$), and the dependency between that
  choice and the following outgoing transition, e.g. in
  $\TransitionVar_{\FromLabelTo{1}{a}{3}} -
  \TransitionVar_{\FromLabelTo{0}{a}{1}} = 0$. Here we only have one choice of
  incoming flow; from $\FinalStateVar_3$, corresponding to the only accepting
  state.


Note that one of the $\lor$-clauses can be immediately removed since it contains
the contradiction $\StateVar_{1} = \StateVar_{1} + 1$, arising from cycles.The
unsatisfiability is by design, and ensures that two states in a cycle cannot
vouch for each other's connectedness.

Assume we are really interested in occurrences with the letter a, i.e. we solve
for $\LetterVar_a > 0$. This implies that $\StateVar_{1} > 0 \land \StateVar_{3}
> 0$ (essentially, that the state our transition of interest started in is
reachable), in turn implying that $\FinalStateVar_3 = 1$. Plugging in these
facts, we get:
\begin{equation}
  \begin{aligned}
  &\LetterVar_{a} = 2 \land \LetterVar_{b} = 0 \land \LetterVar_{c} = \TransitionVar_{\FromLabelTo{1}{c}{1}}\\
  & \StateVar_{3} > 0\\
  &\StateVar_{0} > 0 \implies \StateVar_{0} = \StateVar_{3} + 2\\
  \end{aligned}
  \label{eq:parikh-formula-calc}
  \end{equation}

We can then proceed to eliminate the remaining existentially quantified
variables to obtain the formula $\LetterVar_{a} = 2 \land \LetterVar_{b} = 0
\land \LetterVar_{c} \geq 0$ without too much effort.

  This example, in addition to showing how the encoding works, has hopefully
  also illustrated one of the key insights behind the design of \Calculus: the
  fact that even few additional constraints on the Parikh image can dramatically
  reduce the difficulty of computing it.

  
\section{Generalised Parikh Maps}\label{sec:generalised}

It is easy to see that the Parikh map~$\ParikhMap$ represents a
homomorphism from the (free) non-commutative monoid~$\Strings$ to the
(free) commutative monoid~$\Naturals^k$. As we are often not
interested in the full Parikh image, but only in some projections of
it, for the purposes of this paper we use a generalised version of the
Parikh map. We consider \emph{arbitrary} homomorphisms
$\Map:\: \Sigma^* \to \Monoid$, where
$\Monoid = \left(X;+_{\Monoid};0_{\Monoid}\right)$ is a commutative
monoid; this means that $\Map(\epsilon) = 0_{\Monoid}$ and
$\Map(u \Concat w) = \Map(u) +_{\Monoid} \Map(w)$. We give several
examples of such generalised Parikh image later in this section.

Observe that every homomorphism~$\Map:\: \Sigma^* \to \Monoid$ can be
represented as the composition~$h' \circ \ParikhMap$, for some
homomorphism~$h' : \Naturals^k \to M$. One of the insights underlying our
approach is that it is more efficient to directly compute a
generalised Parikh image~$\Map(\Language)$, than to first compute the
standard image~$\ParikhMap(\Language)$ followed by projection to
some property of interest.

%\subsection{Applications of generalised Parikh images}\label{sec:applications}

\subsection{Example~1: Reasoning about String Length}

A useful example of such a simplifying morphism can express string
length, the problem that originally motivated our study of the Parikh
map. This mapping is relevant when solving constraints that combine
language membership as well as word length, for instance the formula
\begin{equation}\label{eq:stringLength}
x \in \Language_1 \wedge y \in \Language_2 \wedge |x| > |y|
\end{equation}
mentioned in the introduction. To solve this formula, let
$\Monoid = \Naturals$, and define the homomorphism~$L$ by $L(a) = 1$
for all characters~$a \in \Alphabet$. The length of a string
$s = s_1 \RepeatSum{\Concat} s_n$ is given by
$L(s) = \sum L(s_i) = 1 \RepeatSum{+} 1 = n$, and to solve
\eqref{eq:stringLength} we can instead solve the equi-satisfiable
formula~$\alpha \in L(\Language_1) \wedge \beta \in L(\Language_2)
\wedge \alpha > \beta$. This paper proposes efficient native
procedures to reason about membership constraints
like~$\alpha \in L(\Language_1)$, avoiding the computation of
the complete image~$L(\Language_1)$.

Intuitively, this produces a simpler problem than computing the Parikh image, as
we now ignore not just the order of characters but also their differences.
This
means that the branching automaton of \cref{fig:len-branch} becomes
equivalent to the non-branching automaton of \cref{fig:len-branch-free}.
Note that the elimination of the choice of paths also effectively eliminates the
possibility of disconnecting one of the self-loops. This, in turn, means that we
can eliminate all of the connectedness implications of the quantified formula,
which would in turn simplify it (and the quantifier elimination of it)
considerably.

\subsection{Example~2: Generalised String Constraints with Integer
  Datatype}\label{sec:parikh-automata}
\contents{\item Ostrich+!}

Another application for this generalisation is to express Parikh automata, and
this is indeed what the implementation described in
Section~\ref{sec:implementation} does. Following~\cite{parikh-automata} as
filtered through~\cite{expressiveness} (which offers a better formalisation for
our use), we define a Parikh automaton as an automaton with integer counters
that are incremented at the transitions, and where we have linear
constraints on the final values of the counters after running the automaton, or
formally as in \cref{def:parikh-automata,def:projections}.

In the following definitions, we use (as in~\cite{expressiveness}) the following notation:
\begin{itemize}
  \item $\Alphabet$ is an alphabet
  \item $D$ is a finite subset of $\Naturals^d$, for some $d \in \Naturals^+$ (e.g. vectors of length $d$)
\end{itemize}

\begin{definition}\label{def:projections}
  We define two projections on the composite alphabet $\left(\Alphabet \times D\right)^*$:
  \begin{itemize}
    \item $\pi_{\Alphabet} := (a, \Vector{v}) \mapsto a$ or the projection on $\Alphabet$ is the monoid morphism from $\left(\Alphabet \times D\right)^*$ to $\Alphabet^*$, and the opposite projection
    \item $\pi_{v} := (a, \Vector{v}) \mapsto \Vector{v}$, called the
    \emph{extended Parikh image}.
  \end{itemize}
\end{definition}

\begin{definition}\label{def:parikh-automata} A Parikh automaton of dimension $d
  \geq 0$ is a pair $\Tuple{\Automaton, C}$, where $C \subseteq \Naturals^d$ is
  a semi-linear set (or, equivalently, a Presburger formula), and $\Automaton$
  is a finite automaton with the alphabet $\Alphabet \times D$, where $D
  \subseteq \Naturals^d$. We say that $\Tuple{\Automaton, \varphi}$
  \emph{recognises} the language, $\Language(\Automaton, C)$, that is it
  recognises the language created by is the $\pi_\Alphabet$-projection on
  $\Alphabet$ whose $\pi_{v} \subseteq C$.
\end{definition}

\begin{figure}[t]
  \centering
      \includegraphics[width=0.5\linewidth]{parikh_automaton}
      \caption{The automaton part of a Parikh automaton for $\AcaOrBc{}$ with
      $\Alphabet = \Set{\text{a, b, c}}, d = 3$. The semilinear set/Presburger
      formula containing the constraints on the final register values cannot be
      visualised.}\label{fig:parikh-automaton}
    \end{figure}
  

An example of a Parikh automata version of $\AcaOrBc{}$ can be seen in
\cref{fig:parikh-automaton}. A corresponding Presburger version of the
constraint seen in \cref{sec:motivation} is $\psi(\Vec{v}) = v_1 > v_2$.

Generally, we can encode the problem as follows:
\begin{enumerate}
  \item The morphism ($\Map$) we would use is $\pi_{v}$
  \item The monoid operation is element-wise vector addition, which is trivially commutative.
  \item \Fudge{We would just add the constraints on the final values!?}
\end{enumerate}

\section{A Calculus for Generalised Parikh Images}\label{sec:calculus}

We start by defining our calculus, \Calculus{}, for one automaton, and only
extend it to products of automata in \cref{sec:multiple}. Assume an NFA
$\Automaton = \AutomatonTuple$ with $\NrTransitions$ transitions $\Transitions =
\Set{\EllipsisSequence{\Transition}{\NrTransitions}}$. For convenience, we then
introduce the following supporting notations:

\begin{definition}
  A \textit{path} $\Path = \PathEnumeration$ of an automaton~$\Automaton$ with
  transitions $\Transitions$ represents a path through $\Automaton$ using
  transitions in $\Transitions$ (i.e. $\Transitions(\State_k, \Label_{k+1},
  \State_{k+1})$ holds), passing zero or more labels $\Label_1, \ldots,
  \Label_n$. The path must begin in the initial state, i.e.~$\State_0 =
  \InitialState$. The end state, $\State_n$, is not necessarily accepting. If it
  is, we say the path is also \textit{accepting}.
  \end{definition}

\begin{definition}
  Moreover, we talk about the \textit{set of paths} of an automaton,
  $\Paths(\Automaton)$, a possibly infinite (if $\Automaton$ has loops) set of
  valid paths through $\Automaton$. Additionally, we use the
  notation~$\Paths(\Automaton, \State)$ to mean all paths ending in
  state~$\State$.
\end{definition}

\begin{definition}
  For a path $\Path = \Tuple{\State_0 \Label_1 \State_1 \RepeatSum{,}
  \State_n}$, its \textit{word} $\WordOf(\Path) = \Label_1 \RepeatSum{\Concat}
  \Label_n$ is the word read out on its labels.
\end{definition}

\begin{definition}
  The \textit{states} of a path $\Path = \Tuple{\State_0 \Label_1 \State_1
  \RepeatSum{,} \State_n}$, $\StatesOf(\Path) = \Set{\State_0\RepeatSum{,}
  \State_n}$ are the states visited along $\Path$. Note that $\InitialState \in
  \StatesOf(\Path)$ for every path since all paths start in the initial state.
\end{definition}

\begin{definition}
  A \textit{cut}, $C$ of an automaton~$\Automaton = \AutomatonTuple$ to state
  $\State$, written $\SeparatingCut(C, \Automaton, \State)$, is a minimal set of
  transitions such that every accepting path $p$ where $\State \in
  \StatesOf(\Path)$ contains a transition $\Transition \in C$. A cut does not
  exist for every state; notably it never exists for $\InitialState$.
\end{definition}

\begin{definition}
 The \textit{transition count}, $\TransitionCount(\Transition, \Path)$ is the
 number of times a transition $\Transition =
 \FromLabelTo{\State_1}{\Label}{\State_2} \in \Transitions$ appears in a path
 $p$.
\end{definition}

We then introduce the two predicates into our calculus with the following
definitions:

\begin{definition}\label{def:single-image}
  The Parikh predicate, $\SinglePredicateInstance$, for some automaton
  $\Automaton = \AutomatonTuple$, modulo some map $\Map$ to a commutative monoid
  $\Monoid$ as described in \cref{sec:generalised} and with a transition
  selection function $\Filter:\: \Transitions \to \Naturals$ holds when
  $\MonoidElement \in \Monoid$ is the Parikh image of $\Automaton$ modulo
  $\Map$, or more formally when there is an accepting path $\Path =
  \PathEnumeration \in \Accepting{\Paths(\Automaton)}$ such that:
  \begin{itemize}
    \item $\Filter(\Transition) = \TransitionCount(\Transition, \Path)$
    \item $\MonoidElement = \Map(\WordOf(\Path))$
  \end{itemize}
\end{definition}

\begin{definition}
  $\Connected(\Automaton, \Filter)$ for some automaton $\Automaton =
  \AutomatonTuple$ holds when for every $\Transition =
  \FromLabelTo{\State}{}{\State'} \in \Transitions$, $\Filter(\Transition) > 0
  \implies \exists \Path \in \Paths(\Automaton)$ $\Filter(\Transition_i) > 0$
  for $\Transition_i \in p$, and $\State \in \StatesOf(p)$, or in words that
  there exists some $\Filter$-selected valid path that reaches $\Transition$'s
  starting state, $\State$. Intuitively, it represents the condition that
  $\Automaton$ is connected with respect to the selection function $\Filter$ for
  every transition. It is redundant to $\Image$ by design.
\end{definition}

We present the rules of \Calculus{} for one automaton in
\cref{tbl:rules:single}. Note that we operate on sets of symbols (terms,
clauses). Additionally, we also use the convention of splitting the formulas
into linear inequalities ($\SomeInequalities$) and any other clauses
($\SomeClause$).

Note that the filtering function~$\Filter$ is evaluated symbolically in these
rules, and can in practice be read as a symbolic function from transitions to
terms (e.g.~\texttt{t} or~\texttt{t+1}). In our implemendation \Catra{},
described in \cref{sec:implementation}, $\Filter$~is a vector of fresh variables
with the same size as~$\Transitions$.

We use the shorthand notation~$\Transitions_\Automaton$ to refer to the
transitions of an automaton~$\Automaton$. Additionally, for an
automaton~$\Automaton = \AutomatonTuple$ we allow mapping the selection function
like so: $\Filter(\Automaton) = \Tuple{\States, \InitialState, \AcceptingStates,
\Set{\Transition \in \Transitions \SuchThat \Filter(\Transition) > 0}}$, i.e.
$\Automaton$~with only the transitions for which~$\Filter$ is positive. In this
instance, the basis for the matched linear inequalities is
implicitly~$\SomeInequalities$. Similarly, for our commutative monoid $\Monoid =
\left(X;+_{\Monoid};0_{\Monoid}\right)$ and the map into it $\Map : \Strings
\rightarrow X$, we also allow mapping over transitions:
$\Map(\FromLabelTo{\State}{\Label}{\State'}) = \Map(\Label)$.

The rules of \cref{tbl:rules:single} are meant to be used in tableaux-style
proofs, and are read bottom-up, where the syntax at the bottom is replaced with
the one at the top until \Fudge{it is possible to close the proof goal}. Since
our rules operate by adding and matching linear (in)equalities in a proof goal,
we use the shorthand of listing the matched inequalities as antecedents. An
example of this can be seen in the~\Propagate{} rule. \Fudge{We pull solutions
out of a hat? I have no idea.}

We require that every rule only fires when it would add a new clause. For
example, this means that we cannot use~\Split{} to split on the same term twice.
This suggests a proof strategy where you \Propagate{} when you can, \Split{}
when you must, and \Subsume{} when neither is possible anymore.

\begin{table}
\begin{tabular}{@{}l>{$}c<{$}p{3cm}@{}}\toprule
  Name & Rule & Prerequisites\\
  \midrule

  % EXPAND
  \Expand & 
    \inferrule
  {\Connected(\Automaton, \Filter) \land \FlowEq(\Automaton, \Filter) \land \MonoidElement = \sum_{\Transition \in \Transitions_\Automaton} \Filter(\Transition) \cdot \Map(\Transition), \SomeInequalities, \SomeClause}
  {\SinglePredicateInstance, \SomeInequalities, \SomeClause} & 
  None \\[3ex]

  % SPLIT
  \Split & 
  \inferrule{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause, \Filter(\Transition) = 0 \mid \Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause, \Filter(\Transition) > 0}{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause} &
  if $\Transition \in \Transitions_\Automaton$ \\[3ex]

  % PROPAGATE
  \Propagate &
  \inferrule{\Connected(\Automaton, \Filter), \Set{\Filter(\Transition') = 0 \SuchThat \Transition' \in C}, \SomeInequalities, \SomeClause, \Filter(\Transition) = 0}{\Connected(\Automaton, \Filter), \Set{\Filter(\Transition') = 0 \SuchThat \Transition' \in C}, \SomeInequalities, \SomeClause} &
  if $t = \FromLabelTo{\State}{}{\State'} \in \Transitions_\Automaton, \SeparatingCut(C, \Automaton, \State)$ \\[3ex]

  % SUBSUME
  \Subsume &
  \inferrule{\SomeInequalities,\SomeClause}{\Connected(\Automaton, \Filter), \SomeInequalities, \SomeClause} &
  \Split{} and \Propagate{} cannot be applied \\
  \bottomrule
  \end{tabular}
  \caption{Derivation rules for one automaton.}\label{tbl:rules:single}
\end{table}
We use the symbolic function $\FlowEq(\Automaton, \Filter)$ that generates a set
of existentially quantified linear inequalities with the following definition,
where we assign fresh, existentially quantified variables to
$\FinalStateVar_\State, \Filter(\Transition)$ for every $\State \in
\AcceptingStates, \Transition \in \Transitions$:
\[
\begin{aligned}
  & \FlowEq(\Automaton, \Filter) = \sum\limits_{\State \in \AcceptingStates} \FinalStateVar_\State = 1 \land \AndComp{\State \in \States}{\In(\State, \Filter) - \Out(\State, \Filter)} = \Sink(\State) \land
  \AndComp{\State \in \AcceptingStates}{\FinalStateVar_\State \geq 0}\\
  & \Sink(\State) = 0 \text{ if } \State \not\in \AcceptingStates, \FinalStateVar_\State \text{ otherwise.} \\
  & \In(\State, \Filter) = \StartFlow(\State) + \sum_{\Transition \in \FromLabelTo{\State'}{}{\State}} \Filter(\Transition)\\
  & \StartFlow(\State)  = 1 \text{ if } \State = \InitialState, \text{ otherwise $0$.} \\
  & \Out(\State, \Filter) = \sum_{\Transition \in \FromLabelTo{\State}{}{\State'}} \Filter(\Transition)
\end{aligned}
\]

In addition to \cref{tbl:rules:single}, we assume the existence of a rule
\PresburgerClose{}, corresponding to a sound and complete solver for Presburger
formulae, and for the elements of~$\Monoid$.

The $\Propagate{}$ rule allows us to propagate (dis-)connectedness across
$\Automaton$. It states that we are only allowed to use transitions attached to
a reachable state, and is necessary to ensure connectedness in the presence of
cycles in~$\Automaton$.

\textsc{Expand} expands the predicate into its most basic rules; one set of
linear equations synchronising the transitions mentioned by~$\Filter$ to the
corresponding Monoid element~$\MonoidElement$, and the linear flow equations of
the standard Parikh image formulation, as described by~\FlowEq. Since
$\Connected$ and $\Image$ are partially redundant and the difference is covered
by~$\FlowEq$, we can remove the instance of~$\Image$ when applying~$\Expand$. In
this sense, we split the semantics of the $\Image$~predicate into its counting
aspect (covered by $\FlowEq$) and its connectedness aspect (covered by
$\Connected$).

Finally, $\Split{}$ allows us to branch the proof tree by trying to exclude a
contested transition from a potential solution before concluding that it must be
included. Intuitively, this is what guarantees our ability to make forward
progress by eliminating paths through~$\Automaton$.

A decision procedure for our predicate in a tableau-based automated theorem
prover would start by expanding the predicate using the $\Expand{}$~rule. A
theorem prover would perform algebraic substitution on the underlying constants
of~$\Filter$, boiling them down to choices of branches, which depend on one
single variable, and loop transitions. This logic corresponds to the placement
of counters for optimally edge-profiling the CFG of a program, making up a
minimum-spanning tree of the automaton~\cite{path-profiling}.

In order to make the examples below tractable, we will assume the existence of a
rule \EquationReasoning{} that allows us to perform standard algebraic reasoning
on linear inequalities. This rule is not necessary for correctness or
completeness, but shortens the examples considerably.

\subsection{An Example}

Starting with~$\AcaOrBc{}$, where $\Map$ is the length function, in effect
$\Transition \mapsto 1$ for transitions, and the constraints that the length is
odd using the same trick as in \cref{sec:motivation}, we have the
definitions in \cref{fig:example:single:equivalences} (omitting existential
quantifiers and $x \geq 0$ for every variable to avoid clutter).

\begin{figure}[ht]
  We define $\Filter$ and $\FlowEq(\AcaOrBc{}, \Filter)$ as in the following two
  equations, and then apply \EquationReasoning{} (under the implicit assumption
  that every RHS is $\geq 0$) to obtain the equivalent third definition :
  \begin{minipage}[b]{0.3\linewidth}
    \begin{equation*}
      \begin{aligned}
        & \Filter(\FromLabelTo{S}{a}{A}) & = \TransitionVar_1 \\
        & \Filter(\FromLabelTo{S}{b}{B}) & = \TransitionVar_2 \\
        & \Filter(\FromLabelTo{A}{c}{A})  & = \TransitionVar_3  \\
        & \Filter(\FromLabelTo{B}{b}{S}) & = \TransitionVar_4 \\
        & \Filter(\FromLabelTo{A}{a}{F}) & = \TransitionVar_5 \\
        & \Filter(\FromLabelTo{B}{c}{F}) & = \TransitionVar_6 \\
      \end{aligned}
    \end{equation*}    
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}[b]{0.3\linewidth}
    \begin{equation*}
      \begin{aligned}
        % S
        &  \TransitionVar_4 = \TransitionVar_5 + \TransitionVar_2 - 1 \\
        % A
        & \TransitionVar_1 = \TransitionVar_5 \\
        % B
        & \TransitionVar_2 = \TransitionVar_4 + \TransitionVar_6 \\
        % F
        & \TransitionVar_5 + \TransitionVar_6 = \FinalStateVar_1 \\
        & \FinalStateVar_1 = 1 \\
      \end{aligned}  
    \end{equation*}    
  \end{minipage}
  \begin{minipage}[b]{0.3\linewidth}
    \begin{equation*}
      \begin{aligned}
        & \Filter(\FromLabelTo{S}{a}{A}) & = 1 - \TransitionVar_6 \\
        & \Filter(\FromLabelTo{S}{b}{B}) & = \TransitionVar_4 + \TransitionVar_6 \\
        & \Filter(\FromLabelTo{A}{c}{A})  & = \TransitionVar_3  \\
        & \Filter(\FromLabelTo{B}{b}{S}) & = 2\TransitionVar_6 + \TransitionVar_4 \\
        & \Filter(\FromLabelTo{A}{a}{F}) & = 1 - \TransitionVar_6 \\
        & \Filter(\FromLabelTo{B}{c}{F}) & = \TransitionVar_6 \\
      \end{aligned}
    \end{equation*}    
  \end{minipage}
  \caption{Equivalences defining $\Filter$ and $\FlowEq(\AcaOrBc{}, \Filter)$
  respectively.}\label{fig:example:single:equivalences}
  \end{figure}

\begin{figure}
  \centering
\begin{prooftree}
  \infer0[\PresburgerClose]{
    \begin{matrix}
      1 - \TransitionVar_6 = 0 \land \\
      \TransitionVar_6 = 1 \land \\
      \TransitionVar_4 + 1 > 0 \land \\
      \TransitionVar_3 = 0 \land \\
      2 + \TransitionVar_4 > 0 \land \\
      k = 1 + \TransitionVar_4 
    \end{matrix}
  }
  \infer1[\Subsume{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 = 0 \land \\
      \TransitionVar_6 = 1 \land \\
      \TransitionVar_4 + 1 > 0 \land \\
      \TransitionVar_3 = 0 \land \\
      2 + \TransitionVar_4 > 0 \land \\
      k = 1 + \TransitionVar_4 
    \end{matrix}
  }
  \infer1[\Propagate]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 = 0 \land \\
      \TransitionVar_6 = 1 \land \\
      \TransitionVar_4 + 1 > 0 \land \\
      \TransitionVar_3 \geq 0 \land \\
      2 + \TransitionVar_4 \geq 0 \land \\
      2k = 2 + 2\TransitionVar_4 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\EquationReasoning{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 = 0 \land \\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3 + \TransitionVar_6
    \end{matrix}
  }
  % BRANCH:  = 0
  \infer0[\PresburgerClose{}]{
    \begin{matrix}
      \TransitionVar_3 > 0 \\
      \TransitionVar_6 = 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\Subsume{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \TransitionVar_3 > = 0 \\
      \TransitionVar_6 = 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\EquationReasoning{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \TransitionVar_6 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2\TransitionVar_6 + \TransitionVar_4 = 0\\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\Propagate{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      \TransitionVar_6 = 0 \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3
    \end{matrix}
  }
  \infer1[\EquationReasoning{}]{
    \begin{matrix}
      \Connected(\AcaOrBc{}, \Filter) \land \\
      1 - \TransitionVar_6 > 0 \land \\
      2k = 1 + 2\TransitionVar_4 + \TransitionVar_3 + \TransitionVar_6
    \end{matrix} % BRANCH: > 0
  } % SPLIT
  \infer2[\Split{} $1 - \TransitionVar_6$]{ \Connected(\AcaOrBc{}, \Filter) \land 2k = 1 + 2\TransitionVar_4 + \TransitionVar_3 + \TransitionVar_6 }
  \infer1[\EquationReasoning{}]{
    \Connected(\AcaOrBc{}, \Filter) \land
    2k + 1 =
    \sum\limits_{\Transition \in \Transitions} (\Transition \mapsto 1)(\Label) \cdot \Filter(\Transition)
  }
  \infer1[\Expand{}]{\Image{}_{\AcaOrBc{}, \Transition \mapsto 1}(\Filter, 2k + 1)}
\end{prooftree}
\caption{A derivation for \Calculus{} computing odd lengths in $\AcaOrBc{}$.}\label{fig:derivation:single}
\end{figure}

In \cref{fig:derivation:single}, we see how we start by expanding the predicate
using the (simplified) flow equations. We can also see how interleaving
reasoning on the corresponding linear equations helps the production. In both
branches the reasoning is similar: we conclude that when choosing either path at
the starting state we must avoid the other one, perform propagation based on
that fact, use some equational reasoning to derive the expected forms of the
upper and lower bounds to propagate the now disconnected transitions, and
finally subsume, removing the $\Connected{}$ predicate when we are unable to use
either of the rules, leaving only the satisfiable remnants of the flow
equations.

\subsection{\Calculus{} for one automaton is correct}\label{sec:single:correct}

Since correctness is defined by the formulation of the $\Image{}$~predicate, our
correctness proof amounts to proving inductively that no rule would invalidate
that definition.

\subsubsection{\Calculus{} terminates}
\begin{lemma}\label{lma:single-terminates} A solver implementing the calculus
  described in \cref{sec:calculus} starting with some decidable set of clauses
  $\SomeClause{}$ and an instance of our predicate $\SinglePredicateInstance$
  will eventually terminate.
\end{lemma}

\begin{proof}
The first remaining source of divergence is from leftover predicates. The
$\Image$ predicate can always be trivially removed by applying \Expand{}. This
means a leftover $\Image$ predicate can never cause divergence. The only
remaining predicate then is $\Connected$, which can always be removed by
applying $\Subsume$ when no other of the rules is applicable. Hence all
predicates introduced in the calculus can always be removed.

It remains to prove that each rule is only applied finitely many times. This is
 obvious since both \Propagate{} and \Split{} can only be applied to transition
 variables whose $\Filter$ value has no known lower bound. However, \Split{} can
 always be used to split the proof tree and add clauses in precisely the right
 shape, thereby guaranteeing the ability to make forward progress. Therefore,
 the number of transition variables whose bounds are unknown is decreasing
 monotonically for any given proof goal, and hence termination is guaranteed.
\end{proof}

\subsubsection{\Calculus{} computes the homomorphic image}
\Fudge{I am unsure of the definitions here}
\begin{lemma}\label{lma:single-correct}
  Starting with $\SinglePredicateInstance$, we will close the goal with $\top$
  if $\exists s \in \Language(\Automaton) \HoldsThat \Map(s) = \MonoidElement$
  and $\bot$ otherwise.
\end{lemma}
\begin{proof}
Since we are always guaranteed to terminate following
\cref{lma:single-terminates}, we know that we will close the goal eventually.
Additionally, the property of \cref{lma:single-terminates} is trivially obtained
from the semantics of the $\Image$ predicate as defined in
\cref{def:single-image}. This means that we only need to prove the preservation
of those properties to prove \cref{lma:single-correct}.

\Fudge{I don't understand.}
  % \[
  % \begin{aligned}
  %   & \inferrule
  %   {\Connected(\Automaton, \Filter) \land \FlowEq(\Automaton, \Filter) \land \MonoidElement = \sum_{\Transition \in \Transitions_\Automaton} \Filter(\Transition) \cdot \Map(\Transition), \SomeInequalities, \SomeClause}
  %   {\SinglePredicateInstance, \SomeInequalities, \SomeClause} &\beta \models R_0 \iff \exists i : \beta \models R_i
  % \end{aligned}
  % \]

\begin{lemma}\label{lma:expand-preserves}
  \Expand{} preserves the semantics of $\Image$.
\end{lemma}
\begin{proof}

  After application of \Expand{}, $\FlowEq$ guarantees that any
  $\Filter(\Transition)$  for every $\Transition \in \Automaton$ is consistent
  with the flow through the automaton. After its application, the proof goal
  either contains an unsatisfiable set of Presburger formulae if the flow is
  infeasible, otherwise a satisfiable set. An infesaible flow can happen in
  either of these conditions:
  \begin{itemize}
    \item if $\Filter$ demands that some transition $\Transition$ is visited more times than would be allowed by $\Automaton$, or
    \item if the path described by $\Filter$ is not consistent with a path from
    $\InitialState$ to some state $\State$ in $\AcceptingStates$.
  \end{itemize}

  Note that \Expand{} is taken directly from the existing formulation of
  \eqref{eq:generate-parikh}.
\end{proof}

\begin{lemma}\label{lma:split-preserves}
  \Split{} preserves the semantics of $\Image$.
\end{lemma}
\begin{proof}
  \Split{} allows us to case-split at most once on each transition $\Transition
  \in \Transitions$. This means that our proof tree at its leaves will be
  guaranteed to have all transitions of known bounds in the correct format. This
  means that once \Split{} is at fixpoint, the status (present or absent) of any
  transition is known. Therefore it preserves the semantics of the $\Image$
  predicate.
\end{proof}

\begin{lemma}\label{lma:propagate-preserves}
  \Propagate{} preserves the semantics of $\Image$.
\end{lemma}
\begin{proof}
  \Propagate{} will lazily enforce connectedness to any state. If it can be
  applied, there exists a cut $C$ in $\Automaton$ such that some state $\State$
  for any incoming transition $\Transition \in \Transitions_\Automaton$ has
  $\Filter(t) = 0$  as part of the equations of the proof goal, and such that
  $\State$ has an outgoing transition $\Transition'$ whose lower bound is not
  $0$. Adding this conclusion will make equations unsatisfiable precisely if
  there exists a transition $\Transition \in \Transitions_\Automaton$ such that
  $\Filter(\Transition)$ is inconsistent with the homomorphic image, since it
  implies that the selection of transitions is disconnected.
\end{proof}

\begin{lemma}\label{lma:subsume-preserves}
  \Subsume{} preserves the semantics of $\Image$.
\end{lemma}
\begin{proof}
  Since this rule can \emph{only} be applied if no other rule can, it means that
  the upper and lower bounds of any transition $\Transition \in \Transitions$ as
  defined by $\Filter(\Transition)$ is known, meaning clauses of the shape
  $\Filter(\Transition) > 0$ or $\Filter(\Transition) = 0$ are present in the
  proof goal for every transition and that the $\Image$ predicate has been
  consumed by \Expand{} to be replaced by $\Connected$. This in turns means that
  the goal can be closed only if the assignment of $\Filter$ is consistent with
  an accepting path through $\Automaton$.
  
  It also means that \Propagate{} must have been applied whenever possible, meaning that for any transition $\Transition$ starting in a state $\State$ that is disconnected the corresponding $\Filter$ values, we must have applied $\Propagate{}$ to derive $\Filter(\Transition) = 0$. Note that for an infeasible assignment (such as an unsatisfiable instance) this would contradict other decisions, but it would preserve the semantics of $\Image$.
\end{proof}

Since the final step we execute is \Subsume, which we know from
\cref{lma:subsume-preserves} preserves the semantics of $\Image$ and each step
along the way
(\cref{lma:expand-preserves,lma:split-preserves,lma:propagate-preserves}), it
holds that \Calculus{} for one automaton also preserves the semantics.
\end{proof}

\subsection{Finding the Presburger representation of a homomorphic image}\label{sec:finding-the-image}

To find the Presburger form of the homomorphic image efficiently, we adapt the
quantifier elimination approach of~\cite{qe} to our problem domain. The core
method is the same: we incrementally use \Calculus{} to find models,
\Generalise{} the models we find into a quantifier-free Presburger formula with
the $\MonoidElement$ as the only free variable (algorithm~\ref{alg:generalise}), add the
negated formula as a constraint, and continue enumerating models until none
remain. The disjunction of the generalised models we enumerated is now our
image.

\begin{algorithm}
  \caption{$\Generalise{}(\Automaton, \Filter, a)$ will generalise a final product $\Automaton$ and a model $a$ under the homomorphism $\Map$.}\label{alg:generalise}
  \KwData{$\Automaton$, a product from \Calculus{}, a model $a$ assigning counts to the terms that $\Filter$ associate with each transition of $\Automaton$, and our homomorphism $\Map$ that we want to compute the image modulo.}
  \KwResult{a quantifier-free Presburger formula $P$ representing a partial $\Map$-homomorphic image}
  \SetKwFunction{EliminateQuantifiers}{eliminateQuantifiers}

  $\Automaton' \gets \Tuple{\Automaton_\States, \Automaton_{\InitialState}, \Automaton_\AcceptingStates, \Set{\Transition \in \Automaton_\Transitions \SuchThat a(\Filter(\Transition)) \neq 0}}$

  \Fudge{// This notation might be too much but IT'S OBVIOUS DAMMIT}

  \KwRet{\EliminateQuantifiers{$\Map(\ParikhMap(\Automaton'))$}}
  \end{algorithm}


  \begin{algorithm}
    \DontPrintSemicolon
    \caption{$\FindImage{}(\Automaton_1 \times \ldots \times \Automaton_k, \Map)$ will find the Presburger form for the product $\Automaton_1 \times \ldots \times \Automaton_k$ modulo a homomorphism $\Map$ where the only free variable is/are the one(s) representing the monoid element of $\Map$.}\label{alg:find-image}
    \KwData{$\Automaton$, a product from \Calculus{}, a model $a$ assigning counts to the terms that $\Filter$ associate with each transition of $\Automaton$, and our homomorphism $\Map$ that we want to compute the image modulo.}
    \KwResult{a quantifier-free Presburger formula $P$ representing a partial $\Map$-homomorphic image}
    \SetKwFunction{NewTheoremProver}{newTheoremProver}
    \SetKwFunction{EliminateQuantifiers}{eliminateQuantifiers}
    \SetKwFunction{FreshVariable}{freshVariable}
    \SetKwFunction{Assert}{assert}
    \SetKwFunction{GetModel}{getModel}
    \SetKwData{ImageVar}{image}
  
$p \gets \NewTheoremProver{}$\;
$\Filter(\Transition) := \FreshVariable{p}$ for every $\Transition \in \Transitions_{\Automaton}$\;
$\MonoidElement \gets$ \FreshVariable{$p$}\;
\Assert{$p, \exists \MonoidElement, \Filter(\Transition) \text{ for every } \Transition \in \Transitions_\Automaton \HoldsThat \ImagePredicate{\Automaton}{\Map}{\Filter}{\MonoidElement} \land \AndComp{\Transition \in \Transitions}{\Filter(\Transition) \geq 0}$}\;
$\ImageVar \gets \bot$\;
\While{$p$ has more models}{
  $\Tuple{\Automaton, a} \gets \GetModel(p)$\;
  $G \gets \Generalise{}(\Automaton, \Filter, a)$\;
  $\ImageVar \gets G \lor \Image$\;
  \Assert{$p, \lnot G$}\;
  }
    \KwRet{\ImageVar}
    \end{algorithm}

    The attentive reader will notice that the \GetModel{} function is
    nonstandard in that it returns both the model and the automaton used to
    generate it. Since we implement algorithm~\ref{alg:find-image} inside a theorem
    prover under our control, we are able to glean such internal data structures. This should be considered fair game, since we are in practice implementing a specialised form of quantifier elimination, which is an internal affair.
    
    The performance of this approach is evaluated in
    \cref{sec:evaluation:finding-image}, where we benchmark our implementation
    of it.

\section{Parikh Images from Products of Automata}\label{sec:multiple}

To generalise the calculus to calculations on products of automata, we change the main predicate to take arbitrarily many automata:
\begin{definition}\label{def:multiple}
  $\ImagePredicate{\Automaton_1\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}$
  is true exactly when the single-automaton version of the predicate would hold
  for the automaton.
\end{definition}

  For the calculus, we first extend $\Expand$ to generate flow equations and instances of $\Connected$ for each automaton.

  \begin{table}[h]
    \begin{tabular}{@{}l>{$}c<{$}p{3cm}@{}}\toprule
      Name & Rule & Prerequisites\\
      \midrule
    
      % EXPAND
      \ExpandM & 
      \inferrule
      {
        {\begin{matrix}
          \Set{ 
            \MonoidElement = \sum_{\Transition \in \Transitions_{\Automaton_i}} \Filter(\Transition) \cdot \Map(\Transition)
          \SuchThat \Automaton_i \in \Automaton_1,\ldots,\Automaton_k}, \\
          \Set{\FlowEq(\Automaton_i), \Connected(\Automaton_i) \SuchThat \Automaton_i \in \Automaton_1,\ldots,\Automaton_k}, \\
          \ImagePredicate{\Automaton_1\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \\
          \SomeInequalities, \SomeClause
        \end{matrix}}
        }
      {\ImagePredicate{\Automaton_1\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause} & 
      None \\
      \Materialise &
      \inferrule
      {    \BindingSum(\Automaton', \Filter),\ImagePredicate{\Automaton'\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause}
      {\ImagePredicate{\Automaton_1\times\Automaton_2\times\ldots\times\Automaton_k}{\Map}{\Filter}{\MonoidElement}, \SomeInequalities, \SomeClause} &
      $\Automaton' = \Automaton_1\times\Automaton_2$ \\
      \bottomrule
      \end{tabular}
      \caption{Additional derivation rules for products of arbitrarily many automata.}\label{tbl:rules:multi}
    \end{table}

\ExpandM{} must be applied before any other rule, like \Expand{}, but unlike \Expand{}, \ExpandM{} does not remove the $\Image$~predicate since it is needed to keep track of the product.

Before we can begin to define our final rule, we need to talk about product states.

Then we introduce the rule $\Materialise$, used to compute a partial product between two terms:

  With the following helper symbolic function:

  $$
  \BindingSum(\Automaton_1 \times \Automaton_2, \Filter) = \bigcup
  \begin{aligned}
  & \Set{ 
    \left<\Filter(\Transition)  =  \sum\limits_{\Transition' = \FromLabelTo{\Tuple{\State, \State_R}}{\Label}{\Tuple{\State', \State_R}}} \Filter(\Transition')\right>
  \SuchThat \Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions_{\Automaton_1} } , \\ 
  & \Set{
    \left<\Filter(\Transition)  =  \sum\limits_{\Transition' = \FromLabelTo{\Tuple{\State_L, \State}}{\Label}{\Tuple{\State_L, \State'}}} \Filter(\Transition')\right> \SuchThat \Transition = \FromLabelTo{\State}{\Label}{\State'} \in \Transitions_{\Automaton_2}
  }
  \end{aligned}
$$

Note that this definition implies that $\Filter(\Transition) =
\Filter(\Transition')$ whenever two transitions $\Transition \in
\Transitions_{\Automaton_1}, \Transition' \in \Transitions_{\Automaton_2}$
produces a product transition $\Transition'' \in \Transitions_{\Automaton_1
\times \Automaton_2}$. This corresponds to our intuition that the terms of the
product must agree on the value they accept. As before, we implicitly map
$\Filter$ to fresh terms for each transition in the product.

Finally, for instances of precisely one automaton, neither rule applies and we
perform the calculus as before.

\subsection{An Example}\label{sec:multiple:example}

We return again to our example in \cref{sec:introduction:parikh}, where we
compute the whole Parikh image of the product of $\AcaOrBc{}$ and
$\SomethingCSomething$ under the constraint that there are more instances of
letters a than b. This time our monoid $\Monoid$ is 3-dimensional vectors with
element-wise addition, and $\Map$ that maps each transition to the corresponding
increment vector, e.g $\Map(\FromLabelTo{S}{a}{A}) = \VectorLiteral{1,0,0}$.


In the interest of space, we refer back to
\cref{fig:example:single:equivalences} for the definitions of $\Filter$ for
$\AcaOrBc$ and only define $\Filter$ for $\SomethingCSomething{}$ after
substitutions as follows. Note the expansion of the $\Sigma$ labels:
    \begin{equation*}
      \begin{aligned}
        % & \Filter(\FromLabelTo{S}{a}{A}) & = 1 - \TransitionVar_6 \\
        % & \Filter(\FromLabelTo{S}{b}{B}) & = \TransitionVar_4 + \TransitionVar_6 \\
        % & \Filter(\FromLabelTo{A}{c}{A})  & = \TransitionVar_3  \\
        % & \Filter(\FromLabelTo{B}{b}{S}) & = 2\TransitionVar_6 + \TransitionVar_4 \\
        % & \Filter(\FromLabelTo{A}{a}{F}) & = 1 - \TransitionVar_6 \\
        % & \Filter(\FromLabelTo{B}{c}{F}) & = \TransitionVar_6 \\
        % Other automaton
        & \Filter(\FromLabelTo{S}{\Sigma}{S}) & = \VectorLiteral{\TransitionVar_{7a}, \TransitionVar_{7b}, \TransitionVar_{7c}} \\
        & \Filter(\FromLabelTo{S}{c}{F}) & = 1 \\
        & \Filter(\FromLabelTo{F}{\Sigma}{F}) & = \VectorLiteral{\TransitionVar_{9a}, \TransitionVar_{9b}, \TransitionVar_{9c}} \\
      \end{aligned}
    \end{equation*}
    
\begin{prooftree}
  \infer0[\PresburgerClose{}]{
    \begin{matrix}
      1 = \TransitionVar_{10} \land 
      \TransitionVar_3 = \TransitionVar_{11} \land \\
      1 = \TransitionVar_{12} \land 
      1 = \TransitionVar_{11} \land 
      2 > 0
    \end{matrix}  
  }
  \infer1[\Subsume{}, \Expand{}, \Subsume{}]{
    \begin{matrix}
      1 = \TransitionVar_{10} \land 
      \TransitionVar_3 = \TransitionVar_{11} \land  \\
      1 = \TransitionVar_{12} \land 
      1 = \TransitionVar_{11} \land \\
      \Image{}_{\Automaton', \Map}(\Filter, 
      \VectorLiteral{2, 0, 1}) \land 
      \Connected(\SomethingCSomething{}, \Filter) \land
        2 > 0
    \end{matrix}  
  }
  \infer1[\EquationReasoning{}]{
    \begin{matrix}
      1 = \TransitionVar_{10} \land
      \TransitionVar_3 = \TransitionVar_{11} 
      1 = \TransitionVar_{12} \land
      1 = \TransitionVar_{11} \\
      \Image{}_{\Automaton', \Map}(\Filter, 
      \VectorLiteral{
        \TransitionVar_{7a} + \TransitionVar_{9a},
        0,
        \TransitionVar_{7c} + \TransitionVar_{9c} + 1}) \land \\
        \TransitionVar_6 + \TransitionVar_4 = 0 \land 
        1 - \TransitionVar_6 > 0 \land
        \TransitionVar_6 = 0 \land 
        \TransitionVar_4 = 0 \land
        \TransitionVar_3 > 0 \land
        2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
          \VectorLiteral{
        \TransitionVar_{7a} + \TransitionVar_{9a},
        0,
        \TransitionVar_{7c} + \TransitionVar_{9c} + 1}
        = \VectorLiteral{
          2,
          0,
          \TransitionVar_3} \land \\
      \Connected(\SomethingCSomething{}, \Filter) \land
      \land \TransitionVar_{7a} + \TransitionVar_{9a} > 0
    \end{matrix}  
  }
  \infer1[\Materialise]{
    \begin{matrix}
      \TransitionVar_6 + \TransitionVar_4 = 0 \land 
      1 - \TransitionVar_6 > 0 \land
      \TransitionVar_6 = 0 \land 
      \TransitionVar_4 = 0 \land
      \TransitionVar_3 > 0 \land
      2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
      \VectorLiteral{
        \TransitionVar_{7a} + \TransitionVar_{9a},
        0,
        \TransitionVar_{7c} + \TransitionVar_{9c} + 1}
        = \VectorLiteral{
          2,
          0,
          \TransitionVar_3} \land \\
      \Connected(\SomethingCSomething{}, \Filter) \land \\
      \Image{}_{\AcaOrBc{}\times\SomethingCSomething{}, \Map}(\Filter, 
      \VectorLiteral{\TransitionVar_{7a} + \TransitionVar_{9a}, 0, \TransitionVar_{7c} + \TransitionVar_{9c} + 1}) \land \TransitionVar_{7a} + \TransitionVar_{9a} > 0
    \end{matrix}
  }
  \infer1[\Subsume{}]{
  \begin{matrix}
    \TransitionVar_6 + \TransitionVar_4 = 0 \land 
    1 - \TransitionVar_6 > 0 \land
    \TransitionVar_6 = 0 \land 
    \TransitionVar_4 = 0 \land
    \TransitionVar_3 > 0 \land
    2\TransitionVar_6 + \TransitionVar_4 = 0 \land \\
    \VectorLiteral{
      \TransitionVar_{7a} + \TransitionVar_{9a},
      0,
      \TransitionVar_{7c} + \TransitionVar_{9c} + 1}
      = \VectorLiteral{
        2,
        0,
        \TransitionVar_3} \land \\
    \Connected(\AcaOrBc{}, \Filter) \land 
    \Connected(\SomethingCSomething{}, \Filter) \land \\
    \Image{}_{\AcaOrBc{}\times\SomethingCSomething{}, \Map}(\Filter, 
    \VectorLiteral{\TransitionVar_{7a} + \TransitionVar_{9a}, 0, \TransitionVar_{7c} + \TransitionVar_{9c} + 1}) \land \TransitionVar_{7a} + \TransitionVar_{9a} > 0
  \end{matrix}
  }
  \infer1[\EquationReasoning]{
    \begin{matrix}
      \VectorLiteral{a, b, c} = \VectorLiteral{
          2 - 2\TransitionVar_6,
          2\TransitionVar_4 + 3\TransitionVar_6,
          \TransitionVar_3 + \TransitionVar_6
        } \land
        \\
        \VectorLiteral{a, b, c} = \VectorLiteral{
            \TransitionVar_{7a} + \TransitionVar_{9a},
            \TransitionVar_{7b} + \TransitionVar_{9b},
            \TransitionVar_{7c} + \TransitionVar_{9c} + 1
          } \land \\
      \Connected(\AcaOrBc{}, \Filter) \land 
      \Connected(\SomethingCSomething{}, \Filter) \land \\
      \Image{}_{\AcaOrBc{}\times\SomethingCSomething{}, \Map}(\Filter, 
      \VectorLiteral{a, b, c}) \land a > b
    \end{matrix}
  }
  \infer1[\ExpandM]{\Image{}_{\AcaOrBc{}\times\SomethingCSomething{}, \Map}(\Filter, \VectorLiteral{a, b, c}) \land a > b}
\end{prooftree}

\subsection{The extended calculus is also correct}

Since the expanded calculus is in practice the addition of two rules with the
purpose of reducing an~$\Image$ predicate containing a product to the form
solved by the previous single-automaton rules of \cref{tbl:rules:single}, we
extend the reasoning from \cref{sec:single:correct} to the single-automaton
rules of \cref{tbl:rules:multi}.

\subsubsection{The calculus terminates}
\begin{lemma}
  The addition of the \ExpandM{} and \Materialise{} rules in \cref{tbl:rules:multi}
  maintains termination as shown in \cref{lma:single-terminates}.
\end{lemma}
\begin{proof}
  Similarly to how we previously showed that the number of possible executions
  of the \Split{} rule is bounded above by the number of transitions of an
  automaton, we can bound the number of applications of \Materialise{} by the
  (monotonically) decreasing number of automata in the product until we have
  approached the starting state for the single-automaton calculus. Trivially, we
  can eagerly apply \Materialise{} repeatedly to do so. Similarly, \ExpandM{} is
  just the generalisation of \Expand{} to multiple automata. In other words there are no possibilities of the calculus diverging.
\end{proof}

\subsubsection{The calculus implemements the homomorphic image of the product}
\begin{lemma}
  Using \Calculus{} to compute $\Image_{\Automaton_1\times\ldots\times\Automaton_k, \Map}(\Filter, \MonoidElement)$ is equivalent to using the single-automaton rules to compute $\Image_{\Automaton', \Map}(\Filter, \MonoidElement)$ where $\Automaton' = \Automaton_1\times\ldots\times\Automaton_k$.
\end{lemma}
\begin{proof}

  We perform this proof equivalent to set inclusion: by proving that neither predicate is stronger than the other, in the sense of rejecting an element that the other accepts and vice versa.

\begin{lemma}\label{lma:multi:rinclude}
  There exists no element $\MonoidElement$ such that $\Image_{\Automaton_1\times\ldots\times\Automaton_k, \Map}(\Filter, \MonoidElement) = \top$ but $\Image_{\Automaton', \Map}(\Filter, \MonoidElement) = \bot$.
\end{lemma}
\begin{proof}
  Assume that such an $\MonoidElement$ exists with the goal of deriving a
  contradiction. Under this assumption, would we be able to use the two new
  rules to close the proof goal with $\top$?

  The \ExpandM{} rule would allow us to close no goal since it only adds
  additional clauses. Our only hope is in using \Materialise{} to get rid of the
  product step by step without introducing an unsatisfiable clause. However, all
  \Materialise{} does is computing the product step by step, eventually arriving
  at $\Automaton'$ modulo permutations. Therefore, it can be no more satisfiable
  than the single-automata predicate.

  \contents{\item Do I need to worry about the fact that I have no extra
  clauses/$\SomeClause$ here? If present, I would need to apply the definition
  of \BindingSum{} since that is what transfers constraints on individual
  automata up and down the materialisation.}
\end{proof}

\begin{lemma}\label{lma:multi:linclude}
  There exists no element $\MonoidElement$ such that $\Image_{\Automaton_1\times\ldots\times\Automaton_k, \Map}(\Filter, \MonoidElement) = \top$ but $\Image_{\Automaton', \Map}(\Filter, \MonoidElement) = \bot$.
\end{lemma}
\begin{proof}
  Assume that such an element exists. Then $\MonoidElement =
  \Map(\WordOf(\Path))$ for some $\Path$ ending up in an accepting state in
  $\Automaton'$, and consistent with $\Filter$ along the transitions of $\Path$.
  Then a corresponding path must exist in each automata of $\Automaton'$,
  although possibly with different labels.

  For \ExpandM{} to introduce unsatisfiability, either of its automata must be
  disconnected (or its corresponding $\Connected{}$ predicate can be removed
  with \Propagate{}), or its flow must be infeasible. However, this contradicts
  the existence of some corresponding path $\Path$ in each automaton. Hence,
  \ExpandM{} cannot be the source of the unsatisfiability.

  The same applies for the second half of \Materialise{}, if the corresponding
  path exists in each individual automaton and in $\Automaton'$, it must also
  exist in every intermittent product on the way to $\Automaton'$, and if $\Map$
  is consistent for intermittent transition labels as it must be, then neither the intermittent $\Image$ predicates nor the final one can be the source of the unsatisfiability.

  This leaves one final potential source of unsatisfiability: the first half of
  \Materialise{}: \BindingSum{}. For that to be unsatisfiable, it requires
  intermittent automata $\Automaton_l, \Automaton_r$ such that adding
  $\BindingSum(\Automaton_l \times \Automaton_r)$ is unsatisfiable. The
  generated equations state that the transitions of $\Automaton_l$ and
  $\Automaton_r$ must be used precisely as often as their corresponding results
  in $\Automaton_l \times \Automaton_r$. For this set of equations to be
  unsatisfiable there must exist no path through $\Automaton_l \times
  \Automaton_r$ that preserves this. However, this is true from the definition
  of the product of automata: since we know the product to be nonempty, it must
  in some sense contain at least one path whose equivalent exists in both
  $\Automaton_l$ and $\Automaton_2$ and cannot use a corresponding transition
  more often, or it would recognise a different langauge. Hence no such case
  exists.
\end{proof}

Since it follows from \cref{lma:multi:linclude,lma:multi:rinclude} that both
variants of the predicate are precisely equally strong, it must mean that they
are equivalent, and we have proven the correctness as defined in
\cref{def:multiple}.
\end{proof}


\section{Extensions}\label{sec:extensions}

This section describes a number of possible extensions to the calculus. Note
that some are partially or fully implemented in \Catra{} already. In particular,
we have some level of support for symbolic transitions over Unicode alphabets to
keep practical automata under a reasonable size, though we do not allow a full Boolean algebra over symbols as described e.g. in~\cite{symbolic-automata}.

\subsection{Backjumping and Learning No-Goods}\label{sec:ext:backjumping}

\Calculus{} can be accelerated for some instances by adding rules for
backjumping. In particular, central connectedness constraints for an automaton
can be learnt. In that case, when discovering that a state $\State$ of an
automaton $\Automaton$ under a certain transition variable $\Filter$ has become
unreachable, we can learn the clause $\Connected(\Automaton, \Filter) \land
\sum_{\Transition \in \SeparatingCut(\Automaton, \State)} \Filter(\Transition) =
0 \implies \sum_{\Transition' = \FromLabelTo{\State}{}{} \in
\Transitions_{\Automaton}} = 0$. A similar rule can be used when a state becomes
backwards-unreachable (but with a corresponding backwards cut). At the moment,
only forward-cut learning is implemented in \Catra, and has provided a slight
improvement in performance on some instances.

The other source of clauses to learn is the \Materialise{} rule. Whenever an
attempt to materialise a product of two ($\Filter$-filtered) automata
$\Automaton_1 \times \Automaton_2$ produces an empty product, we can determine
the cause of the failure with respect to the automata and their respective
transition variables, and learn no-good combinations that can never be part of a
model. In order to be able to do this, we need a few semantic predicates to
record the state of the calculation, as well as a system of disambiguating
automata, since it is possible to arrive at the same automaton by multiple
combinations of decisions and products. In \Catra{}, we use a number of
additional predicates to record the status of the materialisation of products,
to separate instances of our main predicate, and to register the mapping between
automata and their respective transition $\Filter$ terms, described in
\cref{sec:implementation}. However, at this point only rudimentary no-good
learning is implemented.

\subsection{Symbolic Automata}\label{sec:ext:symbolic}

Extending \Calculus{} to support fully symbolic automata is possible within the
framework, depending on your choice of $\Map$. The difficulty consists in
handling the mapping of the homomorphism $\Map$ over symbolic labels, assuming
it maps to finitely many monoid elements. This is not always straightforward, as
seen in the example of \cref{sec:multiple:example}. This complexity is inherent
in computing the full Parikh image, and stems from the fact that we need to
differentiate between the possible interpretations of the $\Sigma$ transitions
without knowing ahead of times which ones will actually be materialied in the
product. If, on the other hand, if our $\Map$ had been length-counting, which
does not differentiate between values, it would have required no adaptation at
all. With a somewhat liberal interpretation of what we are allowed to map to
(e.g. fresh terms), it is possible to use $\Map$ to represent choice operations
like the ones in a range label. In \Catra{}, we frontload this problem by
requiring the user to encode their input as a Parikh automaton. \Fudge{How is
the encoding of Ostrich Plus automata done?}

\subsection{Transducers}\label{sec:ext:transducers}

Another interesting application is applying \Calculus{} to automata with
multiple tracks, e.g. transducers. One application of such a calculus would be
to represent replace operations and other functions on regular languages, and to
be able to answer questions like "does this operation change the length of the
string". The difficulty in implementing it for transducers is first to perform
the mapping on the labels, which for both the length and Parikh cases is
straightforward; just do the same thing in two dimensions. The more complicated
operation is defining the product of transducers. In some cases it would
probably be desirable to perform synchronisation (e.g. requiring overlapping
transitions) on only some transitions, for example the first track.

Implementing such a calculus is straightforward in \Calculus, since the
definition of products was left out of the definition. All that would be needed
is an appropriate update of the definitions of products. Simiarly, \Catra was
written with modularity in mind, and it should be straightforward to extend both
the input grammar and automata implementations to accommodate multi-track
automata with arbitrary synchronisation.

\section{Implementation}\label{sec:implementation}

We implement a calculus for Parikh automata as described in
\cref{sec:parikh-automata}. The artefact submitted along with this paper
is a program that reads one or more products of one or more DFA, the register
incrementations performed along the edges of their transitions, and the labels
of the transitions as ranges of Unicode characters, along with a set of
constraints on the final values of their registers expressed as Presburger
arithmetic. We call this program \Catra.\footnote{If you really must read it
as an acronym, please read it as CAtegory Theory on Register Automata, or if you
object to the somewhat nonstandard use of register automata and category theory,
as Check Assignments of The Registers Afterwards, or alternatively if you find it
all to be too much of a theoretical exercise as Can Anyone Think of A Real
Application.}

\Catra{} is written in Scala, with the calculus described in this paper
implemented as a theory plug-in for the \Princess{} automated theorem
prover~\cite{princess}, which also performs the Presburger reasoning. For
comparison, we also provide an implementation of the baseline method
from~\cite{generate-parikh-image}, a direct translation that uses the~\Nuxmv{}
symbolic model checker~\cite{nuxmv} to solve our constraints, and the
approximation described in~\cite{approximate-parikh} on top of the standard
baseline back-end. An example of an input file corresponding to our running
example introduced in \cref{sec:motivation} can be seen in
\cref{lst:input-example}.

Please note that \Catra{} uses symbolic labels for automata. A symbolic label is
defined as a range of matching single Unicode code points. This allows succinct
representation of many regular expression patterns such as \lstinline{(a-z).*}
which would have otherwise required $27$~nearly identical transitions. In this
instance, the transition would be defined as \lstinline{a -> b [97, 122]}, as the
interval is closed from both edges.

In satisfaction mode, supported by all back-ends, \Catra{} tries to satisfy the
constraints expressed by the input file, reporting satisfiable with register
assignments or unsatisfiable much like traditional~SAT- or SMT solvers would.
Additionally, the baseline and our own back-end also support efficient
generation of the Presburger formula describing the constraints of the input
file using the method described in \cref{sec:finding-the-image}.

\Catra{} can solve equations of Parikh images. An equation like
$\ParikhMap(\Automaton_1 \times \Automaton_2) = \ParikhMap(\Automaton_3 \times
\Automaton_4)$ would be expressed as one \lstinline{synchronised} block per side of
the equation containing their respective terms followed by a
\lstinline{constraint} requiring all their counters to agree (e.g
\lstinline|Aa = Ba && Ab = Bb ...|). Note that the product construction will implicitly require the counters of each term to agree with each other (and the product), so equalities between all counters of all automata are not necessary.

We implement \Calculus{} as a theory plug-in for the \Princess{} theorem prover.
Since \Princess{} does not support multiple-arity predicates like the ones we
use in \Calculus{}, we have implemented variable-length arguments using
additional helper predicates. These are $\Unused{}(\Automaton)$, which marks an
automaton as unused in any product, and $\TransitionMask{}(\Automaton,
\Transition, \Filter(\Transition))$ which associates a transition $\Transition$
and automaton $\Automaton$ with its corresponding transition variable.
Additionally, we associate each of our predicates with an instance variable in
order to differentiate instances of the predicates.

\subsection{Implementing the Baseline}\label{sec:implementing-baseline}

We implement the baseline approach using the same Presburger solver
(\Princess{}), input file parser, and automaton implementation as \Catra. We
do this in order to make the comparison between methods as fair as possible, and
give us the ability to compare the efficiency of the calculus rules themselves.
Using the formula of Equation~\ref{eq:generate-parikh}, we produce quantified
Presburger formulae for each successive term and add them to Princess. In this
fashion we compute the product incrementally term by term, checking
satisfiability in each step. We use a priority queue to select which
term to use for the next step of the product, and order it by the number
of transitions as a heuristic for the size of the automaton. We use this heuristic
to avoid computing large (and therefore slow) products until we have to, banking
our hopes on finding a source of unsatisfiability early. The pseudocode for
our implementation can be seen in Algorithm~\ref{alg:baseline}.

\begin{algorithm}
  \caption{How we implement the baseline approach}\label{alg:baseline}
  \KwData{$\Automaton_1, \ldots, \Automaton_n$ automata, other constraints $\SomeClause$}
  \KwResult{\textsc{Sat} or \textsc{Unsat}}
  \SetKwFunction{NewTheoremProver}{newTheoremProver}
  \SetKwFunction{NewPriorityQueue}{newPriorityQueue}
  \SetKwFunction{Dequeue}{dequeue}
  \SetKwFunction{Enqueue}{enqueue}
  \SetKwFunction{Assert}{assert}

  $p \gets \NewTheoremProver{}$

  \Assert{$p$, $\SomeClause$}

  \ForEach{$\Automaton_i$}{
    \Assert{$p, \ParikhMap(\Automaton_i)$}

    \If{$p$ is \textsc{Unsat}}{break}

  }

  $q \gets \NewPriorityQueue{}$


  \While{$p$ not \textsc{Unsat} and $|q| > 1$}{
    $\Automaton, \Automaton' \gets \Dequeue{q}$ 
    
    \Assert{$p, \ParikhMap(\Automaton \times \Automaton')$}

    \Enqueue{$q, \Automaton \times \Automaton'$}
  }
  
  \KwRet{$p$'s SAT status}

  \end{algorithm}

It should be noted that our automata (including the successive products) are
always by construction forward- and backward- reachable-minimal. We avoid
computing fully minimal automata since it is unclear when an automaton with
registers can be safely minimised. This guarantees that any automaton we produce
only contains states that are both reachable from the initial state and has a
path to an accepting state. We never perform any other minimisation on the
automata for either backend. More complex minimisation was left out since
performing minimisation on automata with counters is non-trivial, and indeed
already minimising symbolic automata is itself risks exponential blowup
\cite{minimising-symbolic}.

\subsection{Heuristics and search strategies}

There are a number of choices left unspecified in \Calculus{} as described in
Sections~\ref{sec:calculus} and~\ref{sec:multiple}. For example, the order of
materialisation of intermediate products and the order of splitting. In
addition, the splitting specified in the general calculus is correct but
inefficient. In this section we describe additional implementation details used
to enhance our solution.

\subsubsection{Splitting, Materialisation, and Propagation}

We order our rule applications so that we first propagate connectedness if
possible, then perform materialisation if tractable (see below!), then finally
resort to splitting if we must.

In addition to the splitting on individual transition variables described in Table~\ref{tbl:rules:single}, where we randomly select a variable whose bounds are not known, we also prefer splitting to sever/include a strongly connected component. We randomly select an automaton where we can compute a cut to at least one strongly connecting component that can be severed from the initial state, e.g. where the strongly connected component does not contain the initial state and where the sum of the transition variables we have associated with the transitions of a minimal cut between the initial state and an arbitrary state in the component is not known to be greater than 0, but also not equal to 0. If there are multiple such strongly connected components we choose one randomly. We then proceed to split on the cut as if it were a regular transition, e.g. its sum being zero or nonzero. In this way we use our splitting strategy to drive the connectedness constraint towards propagation by cutting off loops from the automata.

The implementation of the connectedness constraint is opportunistic and straightforward. We compute a set of dead states by performing forward and backwards reachability computations on an automaton, where we disregard any transition whose associated variable is known to be zero. After that we add clauses ensuring any transition variable associated with a transition starting in a dead state is zero.

Product materialisation, then, is the final piece of the puzzle. In the current implementation, we put off computing intermediate products until at least all but 2 transition variables of one of them have a known status, e.g. either is known to be zero or known to be positive. The number was chosen experimentally, and we observe a consistent trend that to have a low number and therefore a high threshold for materialisation is better throughout the implementation of other features. The other automaton for the product is selected randomly.

\subsubsection{Clause Learning}

\Catra{} enables clause learning by default when using our backend, as it has
been experimentally shown to increase the performance in aggregate (though not
strictly). We do not currently implement all the proposed features of
\cref{sec:ext:backjumping}, but we do implement forward-reachability cut
learning (which had a very modest improvement in performance). No sophisticated
clause learning for products has been implemented.

\subsubsection{Random Restarts}

Finally, we perform restarts scaled by the Luby series~\cite{luby}. Experimental
results have shown this to have a large improvement in performance, which is
unsurprising given how many random choices we make during solving and how
tail-heavy our problem is. It seems to be very easy to get stuck with a poor
choice of product to materialise or transitions to split on.

\section{Evaluation}\label{sec:experiments}

We evaluate the performance of \Catra{} on~\NrBenchmarks{} instances generated
by the \Ostrich{} string constraint solver when solving the \Fudge{pyex
benchmarks}. \Fudge{The instances are all enormous and have some on average 900
products of 400000 automata with 97 counters each and 9999 constraints}. After
generating an initial~\InitialNrBenchmarks{}, we remove~\NrBroken{} misgenerated
instances that did not parse, as well as~\NrTrivial{} instances solved in under
five seconds by the baseline backend. The benchmarks are run on
commit~\texttt{\commit}.

The benchmarks are executed in parallel using GNU Parallel~\cite{parallel},
since they are mostly single-threaded and initial results showed negligible
interference on performance, on \BenchmarkRig{}. We compiled the code using
Scala~\ScalaVersion{}, and executed the experiments on~\JvmVersion{} with a
maximum heap of~\MaxHeapSize{}. We used \Nuxmv{} version~\NuxmvVersion{} invoked
as a subprocess for each instance. Experiments were executed on a fresh JVM per
backend, but the JVM was kept active for each batch of \BatchSize{} instances,
meaning that JIT compilation would make subsequent executions faster, while
still periodically restarting the JVM. We believe this represents a realistic
use case where \Calculus{} is integrated into a string solver or other system
and is called repeatedly. To mitigate systemic impacts of this decision on our
results, experiments were executed in random order for all backends. We run all
benchmarks with a timeout of~\RuntimeTimeout{}.

As a way of gauging the differences in overhead between the native backends and
\Nuxmv, we executed a small experiment where we ran the same trivial instance 30
times and observed the runtimes for all solvers. All backends saw an improvement
from JIT compilation within the first three runs, but subsequent runs only
improved the native backends. The final measured overhead for the fully warm
JVMs was less than~\OverheadSeconds. From this we draw the conclusion that major
differences in runtime between the native backends and \Nuxmv{} is more likely
to stem from innate differences rather than unfair overhead resulting from our
particular configuration. Additionally, one of the advantages of \Calculus{} to
\Nuxmv{} is that the former can me integrated into an automated theorem prover
while the latter has to be called as a separate process. In this sense, our
benchmarking setup can be said to be realistic.

All runtimes are measured in wall-clock time as observed by the JVM when executing the instance, and exclude time spent parsing, which in all observed cases was minimal in comparison to the runtime, usually far below 0.1 second.

It should be noted that despite running in the standard, deterministic
configuration, we observed some degree of nondeterminism in \Nuxmv. Instances
that were solved or unsolved would sometimes be solved or timed out in
subsequent runs. This observation was made even without randomised execution
order, suggesting that the nondeterminism likely stemmed either from our
encoding of the instances into \Nuxmv's format, or the execution environment.
Similarly, \Calculus{} showed similar signs of nondeterminism, but only for
satisfiable instances. To address this, we increased the runtime from our
initial experiments to give us a wider marigin. By contrast, the baseline
implementation showed no signs of nondeterminism. It should be noted that for
both backends exhibiting nondeterminism, the nondeterminism did not change the
general trend of the experiments, even for experiments with few instances.
Therefore, we believe that our large set of benchmarks will protect us from bias
since any backend is unlikely to be neither consistently lucky nor consistently
unlucky.

\subsection{Execution Time and Ability to Solve Instances}\label{sec:runtime}

In Figures~\ref{fig:solve-division} we show how many of the~\NrBenchmarks{} the
respective back-ends could solve and with which status. A full summary of their
outcomes is also available in Table~\ref{tab:solve-status}. We see here that
\Calculus{} generally outperforms \Nuxmv{} on unsatisfiable instances, while
being slightly worse at satisfiable instances. Both \Nuxmv{} and \Calculus{}
massively and strictly outperform the baseline approach on every kind of
instance, but most of all on satisfiable instances. In fact, the baseline
approach was significantly harder to benchmark due to its much higher demand for
RAM. 

\begin{table}[ht]
  \centering
  \input{graphs/solved_pivot_table.tex}
  \caption{The result of running the respective back-ends by instance
  satisifiability (satisifiable or unsatisifiable) with a timeout of
  \RuntimeTimeout. Instances solved by no backend within the timeout are omitted
  from the table. }\label{tab:solve-status}
\end{table}

\begin{figure}[ht]
  \includegraphics[width=0.75\linewidth]{graphs/\commit-by-solver.pdf}
  \caption{The division of statuses per backend.}
  \label{fig:solve-division}
\end{figure}


\begin{figure}[ht]
  \includegraphics[width=0.75\linewidth]{graphs/\commit-time-boxplot.pdf}
  \caption{The distribution of runtimes for solved instances per backend. Note that the number of instances solved differs between backends.}
  \label{fig:runtime-boxplot}
\end{figure}


\begin{figure}[ht]
  \begin{minipage}[b]{0.75\linewidth}
    \centering 
      \includegraphics[width=\textwidth]{graphs/\commit-duels-lazy-baseline-scatter}
      \caption{Runtime duel: \Calculus{} versus baseline.}
      \includegraphics[width=\textwidth]{graphs/\commit-duels-lazy-nuxmv-scatter}
      \caption{Runtime duel: \Calculus{} versus \Nuxmv.}
    \end{minipage}  
  \caption{The pairwise runtime of each instance for instances solved by any backend. Timeouts are reported as a runtime of \RuntimeTimeout.}
  \label{fig:duels}
\end{figure}

\subsubsection{Scalability}\label{sec:scaling}

A cactus plot showing the number of instances solved within a given timeout for each backend can be seen in Figure~\ref{fig:cactus}. \Fudge{We see here that some solver is fast and some other is not}.

\begin{figure}[ht]
  \includegraphics[width=0.75\linewidth]{graphs/\commit-cactus.pdf}
  \caption{The number of instances solved as the time budget increases, simulated from one \RuntimeTimeout-timeout run.}
  \label{fig:cactus}
\end{figure}

\subsection{Finding a Presburger Formula}\label{sec:evaluation:finding-image}

For baseline and \Calculus{}, \Catra{} offers the ability to find the equivalent Presburger formula representing a given instance. For baseline, we use the built-in quantifier elimination facilities of the underlying \Princess{} theorem prover, while for \Catra{} we use the specially tailored approach described in \cref{sec:finding-the-image}. For this experiment, we use only the~\NrKnownSat{} instances known to be satisfiable from the previous experiment detailed in \cref{sec:scaling,sec:runtime}. 

To make sure baseline puts up as much competition as possible, we disable
checking intermittent satisfiability and configure \Catra{} to run in the
maximally eager mode where the product is first computed before any
satisifiability check is performed. We run the experiments with a timeout
of~\ImageTimeout{}. The results of the experiment is summarised in
\cref{fig:cactus:image} and \cref{tab:image-results}. \Fudge{We see here that
something happens}.

\begin{figure}[ht]
  \caption{The number of instances \Catra{} was able to find the Presburger form of the image for within a given number of seconds per backend.}
  \label{fig:cactus:image}
\end{figure}

\subsection{Threats to Validity}

The most obvious threats to validity would be poor benchmarking or poor
implementation, e.g. if the method described in \cref{sec:calculus,sec:multiple}
deviate from what is actually benchmarked in section \cref{sec:experiments}, or
if the methods used for benchmarking would be unsound. The first problem cannot
easily be addressed, except by stating that we have been careful in our
implementation and descriptions alike. The second is more practical. To increase
the probability that our results are indeed representative both in the sense of
representing runs on expected, real world input and in the statistical sense for
a given run of \Catra, despite the fact that we use randomness in our
implementation, we execute many experiments. During the development of \Catra{},
we have also executed experimeints on different hardware and software
configurations on smaller samples of instances to ensure that the general trend
holds. The logs from these executions are distributed along with notes as part
of the artefact in the interest of transparency. Moreover, we have validated
\Fudge{all reported satisfiable instances and their witnesses} with \Nuxmv{} to
ensure that \Calculus{} is indeed sound and does not cheat.

Another threat to validity would be if our implementations of the competition
(baseline and \Nuxmv) would deviate significantly from the expected
implementations. For \Nuxmv{} we use the default configuration which we believe
should be performant (or it should not be the default). Additionally, tweaking our invocation of \Nuxmv{} is explicitly made easy for artefact reviewers.

The most important and most probable threat to validity is our choice of
implementation platform and our choice of automata library, that is our choice
not to use one. Since our automata are more advanced than anything supported by
e.g. the BRICS library, we opted to use our own implementation. There are some
signs that in particular our product computation is inefficient, notably the
good performance of low-threshold automata materialisation that prioritises
computing smaller products. This means that our implementation of \Calculus{}
makes much less heavy use of our product construction library than the baseline
implementation does, instead putting its weight into \Princess{}' theorem
proving abilities. This situation would unfairly advantage \Calculus{} if our
automata library was indeed the bottleneck. We believe this is unlikely since
similar performance issues as the ones in baseline in our string solver was the
instigating reason for this research. Additionally, while we may have a
constant-factor performance impact from bad implementation, it is less likely
for our potentially poor implementation to be asymptotically worse than e.g.
BRICS. However, the performance improvement of \Calculus{} over baseline seen in
\cref{sec:experiments} is generally not constant.

\section{Conclusion}

In this paper we have introduced an automated theorem prover-friendly calculus
to compute commutating operations on intersections of regular languages that we
call \Calculus{}. We have evaluated it on \NrBenchmarks{} Parikh automata
intersection problems generated \Fudge{by the Ostrich+} string solver
\cite{ostrich} solving \Fudge{standard string constraint benchmarks} using our
Parikh automata solver \Catra{}.

Within \Catra{}, \Calculus{} shows astonishing performance in terms of memory
usage and solve-time compared to the baseline approach laid out in
\cite{generate-parikh-image} when implemented on the same underlying automated
theorem prover (\Princess{}, \cite{princess}). It is also competitive with the
\Nuxmv{} model checker \cite{nuxmv}, outperforming it on unsatisfiable instances
and generally outperforming it for timeouts under 30 seconds with its advantage
increasing drastically for even shorter timeouts. 30 seconds would generally be
considered a long timeout for our intended use as supporting infrastructure to a
string constraint solver.

Future investigations involve two tracks. The first one is integration into
existing string solvers (wich \Ostrich{} being a particularly promising
candidate due to its shared use of \Princess{}), and further adaptation to that
use case. Closer inspection of the instances where we currently time out should
be useful to further improve our heuristics.

The second track for future improvements is the extension into other problem
domains, including other logics, model checking problems, as well as to more
powerful automata such as transducers. In principle, we are also already able to
express stronger constraints than Parikh automata, due to our use of a full
automated theorem prover which allows adding arbitrary constraints in addition
to the expected Presburger formulae.

%% Acknowledgments
% \begin{acks}                            %% acks environment is optional
%                                         %% contents suppressed with 'anonymous'
%   %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%   %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%   %% acknowledge financial support and will be used by metadata
%   %% extraction tools.
%   This material is based upon work supported by the
%   \grantsponsor{GS100000001}{National Science
%     Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%   No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%   No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%   conclusions or recommendations expressed in this material are those
%   of the author and do not necessarily reflect the views of the
%   National Science Foundation.
% \end{acks}


%% Bibliography
\bibliography{bibliography}


%% Appendix
\appendix
\section{Appendix}

\begin{lstlisting}[caption={An example input file for \Catra{} for the problem introduced in \cref{sec:introduction:motivation}, illustrating every major syntax element. From beginning to end: synchronised (product) automata using the keyword \texttt{synchronised} (automata A and B), labels (except those with ranges), register increments, and constraints on the final values of their counters.}, label=lst:input-example]
  // So far we only support integer counters. Note that we have individual
  // counters for each automaton to avoid surprises for product construction.
  counter int l_a, l_b, l_c, r_c;

  synchronised {
  automaton aca_or_bc {
    init S;
  
    // We use ASCII values here, this is for lowercase a
    S -> A [97] { l_a += 1 };
    S -> B [98] { l_b += 1 };
  
    B -> S  [98] { l_b += 1 };
    B -> final [99] { l_c += 1 };
  
    A -> A [99] { l_c += 1 };
    A -> final [97] { l_a += 1 };
  
    accepting final;
  };
  
  automaton something_c_something {
      init S;
  
      // Special short-hand value for the whole Unicode alphabet.
      S -> S [any] ;
      S -> F [99] { r_c += 1 };
  
      F -> F [any];
  
      accepting F;
  };
  };
  
  // Constrain the number of a:s to be larger than the number of c:s. Since the
  // automata are synchronised on every transition and counters are guaranteed
  // to be consistent these constraints are sufficient without also constraining
  // r_c.
  constraint l_a > l_c;
\end{lstlisting}

% Text of appendix \ldots

\end{document}
