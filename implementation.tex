
We implement \Calculus{} for Parikh automata as described in
\cref{sec:parikh-automata}. The artefact submitted along with this paper is a
program that reads an instance file with one or more products of one or more
Parikh automaton with transition labels defined as ranges of Unicode characters,
along with a set of constraints on the final values of their registers expressed
as Presburger arithmetic in a C-like syntax. We call this program
\Catra.

\Catra{} is written in Scala, with the calculus described in this paper
implemented as a theory plug-in for the \Princess{} automated theorem
prover~\cite{princess}, which also performs the Presburger reasoning. For
comparison, we also provide an implementation of the baseline method
from~\cite{generate-parikh-image}, a direct translation that uses the~\Nuxmv{}
symbolic model checker~\cite{nuxmv} to solve our constraints, and the
approximation described in~\cite{approximate-parikh} on top of the standard
baseline back-end. An example of an input file corresponding to our running
example introduced in \cref{sec:intuition} can be found in the root directory
of the artefact~\cref{artefact}.

\Catra{} uses symbolic labels for automata. A symbolic label is defined as a
finite range of Unicode code points. This allows representing regular expression
patterns like \lstinline{(a-z)} as \lstinline{a -> b [a, z]} when it would have
otherwise required $27$~non-symbolic transitions. 

In satisfaction mode, supported by all backends, \Catra{} tries to satisfy the
constraints expressed by the input file, reporting \Sat{} with register
assignments or \Unsat{}. Additionally, baseline and \Calculus{} also support
generating the Presburger formula describing the constraints of the input file,
i.e., computing a closed-form representation of the complete Parikh image.
% Baseline uses standard
%quantifier elimination, and \Calculus{} uses the method described in
%\cref{sec:finding-the-image}.

% Since \Princess{} does not support the multiple-arity predicates used
% in \Calculus{}, we have implemented variable-length arguments using
% additional helper predicates. These are $\Unused{}(\Automaton)$, which
% marks an automaton as unused in any product, and
% $\TransitionMask{}(\Automaton, \Transition, \Filter(\Transition))$
% which associates a transition $\Transition$ and automaton $\Automaton$
% with its corresponding transition
% variable~$\Filter(\Transition)$. Additionally, we associate each
% predicate with an instance variable to tie helper predicates to the
% $\Image$ predicate they originated from.

\subsection{Implementing the Baseline}\label{sec:implementing-baseline}

As a baseline, we use the same Presburger solver (\Princess{}), input file parser,
and automaton implementation as \Catra{}. We do this to better analyse the impact
of the calculus rules themselves. Adapting~\cite{generate-parikh-image}, we
produce quantified Presburger formulae for each successive term and add them to
\Princess{}. We compute the product incrementally term by term, checking
satisfiability at each step. We use a priority queue to select automata for each
step and order them by their number of transitions. We use this heuristic to put
off computing large (and therefore slow) products until we have to, hoping to
find an empty intermittent product. This is roughly similar to the approach
taken in~\cite{approximate-parikh}.

\begin{algorithm}
  \KwData{$\Automaton_1, \ldots, \Automaton_n$ automata, other constraints $\SomeClause$}
  \KwResult{\textsc{Sat} or \textsc{Unsat}}
  \SetKwFunction{NewTheoremProver}{newTheoremProver}
  \SetKwFunction{NewPriorityQueue}{newPriorityQueue}
  \SetKwFunction{Dequeue}{dequeue}
  \SetKwFunction{Enqueue}{enqueue}
  \SetKwFunction{Assert}{assert}

  $p \gets \NewTheoremProver{}$

  \Assert{$p$, $\SomeClause$}

  \ForEach{$\Automaton_i$}{
    \Assert{$p, \ParikhMap(\Automaton_i)$}

    \If{$p$ is \textsc{Unsat}}{break}

  }

  $q \gets \NewPriorityQueue{$\mathcal{A}_1, \ldots, \mathcal{A}_n$}$


  \While{$p$ not \textsc{Unsat} and $|q| > 1$}{
    $\Automaton, \Automaton' \gets \Dequeue{q}$ 
    
    \Assert{$p, \ParikhMap(\Automaton \times \Automaton')$}

    \Enqueue{$q, \Automaton \times \Automaton'$}
  }
  
  \KwRet{$p$'s SAT status}
  \caption{How we implement the baseline approach}\label{alg:baseline}
  \end{algorithm}

As an optimisation, our automata (including intermittent products) have dead
states eliminated during construction. Any automaton we produce contains only
states that are both reachable from the initial state and have a path to an
accepting state. We never perform any other minimisation on the automata for
either backend. More complex minimisation was left out since performing
minimisation on automata with counters is non-trivial.

% @ AS: I did the simple thing and dropped the claim about symbolic automata
% here: ours are less symbolic (i.e. typically finite) than the ones in the
% cited paper, and so are technically no worse than normal non-symbolic ones.

\subsection{Heuristics and Search Strategies}

\Calculus{} as described in \cref{sec:calculus,sec:multiple} leaves some choices
unspecified, including the priority of rules and the order of their arguments. In
this section, we address these and describe additional implementation details
and techniques used to enhance \Catra{}.

\subsubsection{Splitting, Materialisation, and Propagation}

We order our rule applications as follows: first, propagate connectedness if
possible, then perform materialisation if tractable as defined below, then
finally resort to splitting as a last resort.

In addition to applying \Split{} as described in \cref{tbl:rules:single} to
randomly selected transitions, we prefer splitting to sever a strongly connected
component from the initial state. We randomly select an automaton where we can
compute a cut between an SCC and the initial state, that is, where the SCC does
not contain the initial state and where the sum of the transition variables of
the transitions in the cut is not known to be positive. If there are multiple
such strongly connected components, we choose one randomly. We then proceed to
split on the sum of the transition variables of the cut as if it were a regular
transition, e.g., its sum being zero or nonzero. In this way, we drive \Calculus{}
towards applying \Propagate{}.

The implementation of the connectedness constraint is opportunistic and
straightforward. We compute a set of dead states by performing forward and
backward reachability computations on an automaton, where we disregard any
transition whose associated variable is known to be zero. After that we add
clauses ensuring any transition variable associated with a transition starting
in a dead state is zero.

Product materialisation is the final piece of the puzzle. In the current
implementation we put off computing intermediate products until at most six transition
variables of one of the automata are not known to always be used ($> 0$) or always unused ($\leq 0$). The
number was chosen experimentally. The other automaton for the product is
selected randomly.

\subsubsection{Clause Learning}\label{sec:clause-learning}

\Catra{} enables clause learning by default when using our backend, as it has
been experimentally shown to increase the performance in aggregate (though not
strictly). We currently only implement minimal clause learning based on
forward-reachability cuts. No sophisticated clause learning for products has
been implemented.

\subsubsection{Random Restarts}\label{sec:random-restarts}

Finally, we perform restarts scaled by the Luby series~\cite{luby}. Experimental
results have shown this to have a large improvement in performance, which is
unsurprising given how many random choices we make during solving and how
tail-heavy our problem is.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
